<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>【TVM模型编译】2. relay算子构造 | 一只特立独行的猪</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script src="https://www.googletagmanager.com/gtag/js?id=G-D0GZY9PECP" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D0GZY9PECP');
</script><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【TVM模型编译】2. relay算子构造</h1><a id="logo" href="/.">一只特立独行的猪</a><p class="description">既然我存在，就不能装作不存在</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags"><i class="fa fa-tag"> 标签</i></a><a href="/categories"><i class="fa fa-tag"> 分类</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">【TVM模型编译】2. relay算子构造</h1><div class="post-meta">2023-08-09<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><p>从TVM的官方<a target="_blank" rel="noopener" href="https://www.cnblogs.com/wanger-sjtu/p/15046641.html">Tutorial</a>里面，介绍了如何新增自定义算子。(这是我翻译的)</p>
<p>之前的文章讲到了<a href="../tvm-onnx-to-relay">onnx 算子转换到Relay IR</a>的过程<br>下面以Conv2d算子介绍，编译过程中 Relay IR是如何被调用的。</p>
<h2 id="relay-算子调用"><a href="#relay-算子调用" class="headerlink" title="relay 算子调用"></a>relay 算子调用</h2><p>上面的<code>get_relay_op</code>实际上是查找所有 relay ir算子，其代码在<code>python/tvm/relay/frontend/common.py</code>中的<code>get_relay_op</code>。继续以conv卷积算子为例介绍。上文所述的转换算子中，有下面的语句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate <span class="keyword">in</span> (_op, _op.nn, _op.image, _op.vision, _op.contrib):</span><br><span class="line">    op = <span class="built_in">getattr</span>(candidate, op_name, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> op <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>对于<code>conv2d</code>算子，在<code>_op.nn</code>中，找到conv2d实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data,</span></span><br><span class="line"><span class="params">    weight,</span></span><br><span class="line"><span class="params">    strides=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    padding=(<span class="params"><span class="number">0</span>, <span class="number">0</span></span>),</span></span><br><span class="line"><span class="params">    dilation=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    kernel_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data_layout=<span class="string">&quot;NCHW&quot;</span>,</span></span><br><span class="line"><span class="params">    kernel_layout=<span class="string">&quot;OIHW&quot;</span>,</span></span><br><span class="line"><span class="params">    out_layout=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params">    out_dtype=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(kernel_size, <span class="built_in">int</span>):</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(strides, <span class="built_in">int</span>):</span><br><span class="line">        strides = (strides, strides)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dilation, <span class="built_in">int</span>):</span><br><span class="line">        dilation = (dilation, dilation)</span><br><span class="line">    padding = get_pad_tuple2d(padding)</span><br><span class="line">    <span class="keyword">return</span> _make.conv2d( data, weight, strides, padding, dilation, groups, channels, kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>这里的<code>_make.conv2d</code>是通过下面的PackFunc注册得到的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tvm._ffi._init_api(<span class="string">&quot;relay.op.nn._make&quot;</span>, __name__)</span><br></pre></td></tr></table></figure>
<p>在<code>src/relay/op/nn/convolution.cc</code>找到conv2d的注册函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op.nn._make.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span><br><span class="line">                       Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span><br><span class="line">                       Array&lt;IndexExpr&gt; kernel_size, String data_layout, String kernel_layout,</span><br><span class="line">                       String out_layout, DataType out_dtype) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">MakeConv</span>&lt;Conv2DAttrs&gt;(data, weight, strides, padding, dilation, groups, channels,</span><br><span class="line">                                   kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">                                   <span class="string">&quot;nn.conv2d&quot;</span>);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>

<p>MakeConv 是对所有卷积的模板，根据参数实例化相应的函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> Expr <span class="title">MakeConv</span><span class="params">(Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; kernel_size, std::string data_layout,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string kernel_layout, std::string out_layout, DataType out_dtype,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string op_name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;T&gt;();</span><br><span class="line">  attrs-&gt;strides = std::<span class="built_in">move</span>(strides);</span><br><span class="line">  attrs-&gt;padding = std::<span class="built_in">move</span>(padding);</span><br><span class="line">  attrs-&gt;dilation = std::<span class="built_in">move</span>(dilation);</span><br><span class="line">  attrs-&gt;groups = groups;</span><br><span class="line">  attrs-&gt;channels = std::<span class="built_in">move</span>(channels);</span><br><span class="line">  attrs-&gt;kernel_size = std::<span class="built_in">move</span>(kernel_size);</span><br><span class="line">  attrs-&gt;data_layout = std::<span class="built_in">move</span>(data_layout);</span><br><span class="line">  attrs-&gt;kernel_layout = std::<span class="built_in">move</span>(kernel_layout);</span><br><span class="line">  attrs-&gt;out_layout = std::<span class="built_in">move</span>(out_layout);</span><br><span class="line">  attrs-&gt;out_dtype = std::<span class="built_in">move</span>(out_dtype);</span><br><span class="line">  <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(op_name);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data, weight&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里通过<code>Op::Get(op_name);</code> 获取对应relay算子，在<code>Op::Get</code>函数中发现是通过查表得到。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// find operator by name</span></span><br><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注册是通过C++的<code>RELAY_REGISTER_OP(&quot;nn.conv2d&quot;)</code>宏注册到<code>OpRegistry::Global()</code>中。宏展开为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> __attribute__((unused))::tvm::OpRegEntry&amp; __make_Op230 =</span><br><span class="line">    ::tvm::OpRegEntry::<span class="built_in">RegisterOrGet</span>(<span class="string">&quot;nn.conv2d&quot;</span>).<span class="built_in">set_name</span>()</span><br></pre></td></tr></table></figure>

<p>注册过程：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;nn.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(<span class="string">R&quot;code(2D convolution layer (e.g. spatial convolution over images).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This layer creates a convolution kernel that is convolved</span></span><br><span class="line"><span class="string">with the layer input to produce a tensor of outputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- **data**: This depends on the `layout` parameter. Input is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, in_channels, height, width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string">- **weight**: (channels, in_channels, kernel_size[0], kernel_size[1])</span></span><br><span class="line"><span class="string">- **out**:  This depends on the `layout` parameter. Output is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">)code&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_attrs_type</span>&lt;Conv2DAttrs&gt;()</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;weight&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The weight tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Conv2D&quot;</span>, Conv2DRel&lt;Conv2DAttrs&gt;)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;FInferCorrectLayout&gt;(<span class="string">&quot;FInferCorrectLayout&quot;</span>, ConvInferCorrectLayout&lt;Conv2DAttrs&gt;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>返回的是<code>OpRegEntry</code>，后续的<code>set_name</code>等，则是通过<code>OpRegEntry</code>的get接口（返回的是OpNode），构造对应的Relay op</p>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>【TVM模型编译】2. relay算子构造</p><p><span>文章作者：</span>王二</p><p><span>发布时间：</span>2023-08-09</p><p><span>最后更新：</span>2023-12-13</p><p><span>原始链接：</span><a href="/tvm-relay-op-construct/">https://wanger-sjtu.github.io/tvm-relay-op-construct/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://wanger-sjtu.github.io/tvm-relay-op-construct/"></i></span></p><p><span>版权声明：</span>本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</p></div><br><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TVM/" rel="tag">TVM</a></li></ul></div><div class="post-nav"><a class="pre" href="/tvm-custom-op/">【TVM教程】 自定义relay算子</a><a class="next" href="/tvm-op-strategy/">【tvm解析】3. Operator Strategy 机制</a></div><div id="container"></div><link rel="stylesheet" type="text/css" href="//unpkg.com/gitalk/dist/gitalk.css"><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script><script type="text/javascript" src="//unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
  clientID: '25c52264eb62b88a09d0',
  clientSecret: '08dd790dcadb5826a40004b55e56414d2f477734',
  repo: 'wanger-sjtu.github.io',
  owner: 'wanger-sjtu',
  admin: ['wanger-sjtu'],
  id: md5(location.pathname),
  distractionFreeMode: false
})
gitalk.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://wanger-sjtu.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.jpg"/></a><p>除了这只猪，还没见过谁敢于如此无视对生活的设置。</p><a class="info-icon" href="https://github.com/Wanger-SJTU" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%9E%E5%88%86/">竞分</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" style="font-size: 15px;">体系结构</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/ANN/" style="font-size: 15px;">ANN</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/CPU/" style="font-size: 15px;">CPU</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/SD/" style="font-size: 15px;">SD</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/CI/" style="font-size: 15px;">CI</a> <a href="/tags/%E7%AB%9E%E5%88%86/" style="font-size: 15px;">竞分</a> <a href="/tags/NDK/" style="font-size: 15px;">NDK</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/mlx-apple/">MLX 框架浅析</a></li><li class="post-list-item"><a class="post-list-link" href="/ANN-algo/">从向量数据库到 ANN search</a></li><li class="post-list-item"><a class="post-list-link" href="/L1-cache-size/">L1 data 缓存为什么一般只有32K或者64K</a></li><li class="post-list-item"><a class="post-list-link" href="/ndk-pid/">ndk std_thread 获取pid</a></li><li class="post-list-item"><a class="post-list-link" href="/LLM_SD_Basic/">了解LLM——LLM&& SD 基本概念</a></li><li class="post-list-item"><a class="post-list-link" href="/LoRA/">了解LLM —— LoRA</a></li><li class="post-list-item"><a class="post-list-link" href="/mlc-llm/">TVM－MLC LLM 调优方案</a></li><li class="post-list-item"><a class="post-list-link" href="/VectorizeLoop/">TVM 源码阅读PASS — VectorizeLoop</a></li><li class="post-list-item"><a class="post-list-link" href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/">SVE特性以及寄存器</a></li><li class="post-list-item"><a class="post-list-link" href="/tir-to-llvm-ir/">tir_to_llvm_ir</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">一只特立独行的猪.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>