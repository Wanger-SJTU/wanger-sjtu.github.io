<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>一只特立独行的猪 | 既然我存在，就不能装作不存在</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script src="https://www.googletagmanager.com/gtag/js?id=G-D0GZY9PECP" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D0GZY9PECP');
</script><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">一只特立独行的猪</h1><a id="logo" href="/.">一只特立独行的猪</a><p class="description">既然我存在，就不能装作不存在</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags"><i class="fa fa-tag"> 标签</i></a><a href="/categories"><i class="fa fa-tag"> 分类</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/mlx-apple/">MLX 框架浅析</a></h1><div class="post-meta">2023-12-13</div><div class="post-content">最近Apple 新发布了一个MLX的DL框架，这是继 ML Compute 可用于在 Mac 上进行 TensorFlow 模型的训练，PyTorch 在 M1芯片后可使用 Metal Performance Shaders (MPS) 作为GPU 加速的 PyTorch 机器学习模型训练之后，进一步的尝试。
与MLX同时开源的还有数据读取的框架MLX-da...</div><p class="readmore"><a href="/mlx-apple/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/ANN-algo/">从向量数据库到 ANN search</a></h1><div class="post-meta">2023-09-10</div><div class="post-content">LLM的模型的爆火，意外带动了向量数据库的热度。之前名不见经传的一些初创公司也突然备受追捧。最近在分析端侧LLM场景的时候也分析了相关的一些向量数据库的相关知识。
GPT的缺陷chatgpt在对话过程中表现出的能力包括了一定的上下文检索能力。但这个能力是基于LLM本身的上下文理解能力完成的，但受限于多数模型是基于kv cache结构的记忆历史对话信息的，kv...</div><p class="readmore"><a href="/ANN-algo/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/L1-cache-size/">L1 data 缓存为什么一般只有32K或者64K</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">L1 data缓存为什么一般只有32K或者64K？为什么不能更大一点？更大不是更好吗？
至少有这么两个原因。L1缓存因为会频繁被访问，所以优化目标是hit time，缓存size越大，hit time越长。另外现代CPU普遍采用virtually index physically tagged（VIPT）的L1缓存，所以L1数据缓存的大小实际上就是page ...</div><p class="readmore"><a href="/L1-cache-size/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/ndk-pid/">ndk std_thread 获取pid</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">最近在解决tvm绑核问题时，发现android下绑核只有sched_setaffinity函数，这导致无法使用标准库中的td::thread::native_handle_type thread 进行绑核操作。虽然在ndk 21以上的版本提供了pthread_gettid_np函数获取线程相应的pid，但在较低版本中，还是没办法直接使用。
看下ndk 中 s...</div><p class="readmore"><a href="/ndk-pid/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/LLM_SD_Basic/">了解LLM——LLM&amp;&amp; SD 基本概念</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">Causual LM这里以llama模型为例，通常在执行用户输入之前会有一个[[文章&#x2F;LM basic知识#Prefill]]的过程。然后根据用户promts 得到输出。



Perfix LM这里以GLM为例介绍，展示了基本的流程。



prefix LM和causal LM的区别attention mask不同，prefix LM的pref...</div><p class="readmore"><a href="/LLM_SD_Basic/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/LoRA/">了解LLM —— LoRA</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">
论文链接：link
code: github

什么是LoRALoRA，英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT（参数高效性微调方法），这是微软的研究人员为了解决大语言模型微调而开发的一项技术。当然除了LoRA，参数高效性微调方法中实现最简单的方法还是Pro...</div><p class="readmore"><a href="/LoRA/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/mlc-llm/">TVM－MLC LLM 调优方案</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">LLM 等GPT大模型大火以后,TVM社区推出了自己的部署方案，支持Llama，Vicuna，Dolly等模型在iOS、Android、GPU、浏览器等平台上部署运行。
https://github.com/mlc-ai/mlc-llm
本文在之前作者介绍的基础上,简要介绍一下mlc的调优部署方案。
pipeline在正式介绍TVM mlc.ai部署LLM方...</div><p class="readmore"><a href="/mlc-llm/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/VectorizeLoop/">TVM 源码阅读PASS — VectorizeLoop</a></h1><div class="post-meta">2023-09-09</div><div class="post-content">VectorizeLoop这个PASS就是对标记为ForKind::kVectorized的For循环做向量化处理，并对For循环中的语句涉及到的变量，替换为Ramp，以便于在Codegen的过程中生成相关的向量化运算的指令。
VectorizeLoop这个PASS的入口函数如下，只有在打开enable_vectorize=true的情况下载才会被启用，否则...</div><p class="readmore"><a href="/VectorizeLoop/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/">SVE特性以及寄存器</a></h1><div class="post-meta">2023-09-05</div><div class="post-content">SVE对比NEON有几个新增的地方。

变长的向量
支持Gather-load &amp;&amp; Scatter-store



可以由P寄存器控制向量通道的计算


由软件控制的向量切分。

基于First Fault 寄存器完成的，加载不合法内存页的时候，会有记录 


扩展浮点和位运算的水平缩减


SVE 寄存器
Scalable vector...</div><p class="readmore"><a href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/tir-to-llvm-ir/">tir_to_llvm_ir</a></h1><div class="post-meta">2023-09-03</div><div class="post-content">TVM在编译过程中，经历了
graph LR
  A[3rd IR] --> B[Relay IR]
  B --> C[TIR]
  C --> D[LLVM IR]
  C -->E[Source]

这一系列的过程。其中在生成cpu、rocm、nvptx、hexagon等平台的相关代码的时候，会先由TVM的TIR转换为LLVM IR,在后续由LLVM生...</div><p class="readmore"><a href="/tir-to-llvm-ir/">阅读全文</a></p></div><nav class="page-navigator"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页</a></nav><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://wanger-sjtu.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.jpg"/></a><p>除了这只猪，还没见过谁敢于如此无视对生活的设置。</p><a class="info-icon" href="https://github.com/Wanger-SJTU" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%9E%E5%88%86/">竞分</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" style="font-size: 15px;">体系结构</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/ANN/" style="font-size: 15px;">ANN</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/CPU/" style="font-size: 15px;">CPU</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/SD/" style="font-size: 15px;">SD</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/CI/" style="font-size: 15px;">CI</a> <a href="/tags/%E7%AB%9E%E5%88%86/" style="font-size: 15px;">竞分</a> <a href="/tags/NDK/" style="font-size: 15px;">NDK</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/mlx-apple/">MLX 框架浅析</a></li><li class="post-list-item"><a class="post-list-link" href="/ANN-algo/">从向量数据库到 ANN search</a></li><li class="post-list-item"><a class="post-list-link" href="/L1-cache-size/">L1 data 缓存为什么一般只有32K或者64K</a></li><li class="post-list-item"><a class="post-list-link" href="/ndk-pid/">ndk std_thread 获取pid</a></li><li class="post-list-item"><a class="post-list-link" href="/LLM_SD_Basic/">了解LLM——LLM&& SD 基本概念</a></li><li class="post-list-item"><a class="post-list-link" href="/LoRA/">了解LLM —— LoRA</a></li><li class="post-list-item"><a class="post-list-link" href="/mlc-llm/">TVM－MLC LLM 调优方案</a></li><li class="post-list-item"><a class="post-list-link" href="/VectorizeLoop/">TVM 源码阅读PASS — VectorizeLoop</a></li><li class="post-list-item"><a class="post-list-link" href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/">SVE特性以及寄存器</a></li><li class="post-list-item"><a class="post-list-link" href="/tir-to-llvm-ir/">tir_to_llvm_ir</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">一只特立独行的猪.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>