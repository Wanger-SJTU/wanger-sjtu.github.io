<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MLX 框架浅析</title>
      <link href="/mlx-apple/"/>
      <url>/mlx-apple/</url>
      
        <content type="html"><![CDATA[<p>最近Apple 新发布了一个MLX的DL框架，这是继 ML Compute 可用于在 Mac 上进行 TensorFlow 模型的训练，PyTorch 在 M1芯片后可使用 Metal Performance Shaders (MPS) 作为GPU 加速的 PyTorch 机器学习模型训练之后，进一步的尝试。</p><p>与MLX同时开源的还有数据读取的框架MLX-data，以及最近大模型相关的一些Example代码，MLX-examples</p><h2 id="MLX特性"><a href="#MLX特性" class="headerlink" title="MLX特性"></a>MLX特性</h2><ul><li><p><strong>Familiar APIs</strong>: MLX的python API 设计基本上与numpy和Pytorch对齐，基础的数据结构array设置可以隐式的转换为numpy的 Array。高层次的API mlx.nn 和mlx.optimizers则基本与pytorch对齐，使用方式也基本一致。C++ 的API与python基本一致。 这对算法开发人员来说上手的成本较低，历史代码也比较好迁移和继承。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlx.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> mlx.core <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure></li><li><p><strong>Composable function transformations</strong>:  JAX </p></li><li><p>MLX has composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.</p></li><li><p><strong>Lazy computation</strong>: 延迟计算，每部分的计算结果都是按需求值，也包括内存申请。</p></li><li><p><strong>Dynamic graph construction</strong>: 动态图构建，函数输入shape发生变化时，并不会触发编译，debug也很简单符合直觉。</p></li><li><p><strong>Multi-device</strong>: 多IP计算支持。当前支持CPU、GPU。因为ANE在Apple内部处于闭源的工具链，在这一现状没变化时，不会支持ANE。ref：<a href="https://github.com/ml-explore/mlx/issues/18#issuecomment-1846492294">link</a></p></li><li><p><strong>Unified memory</strong>: 最大的特点在于统一的内存模型，MLX 中的在共享内存上分配，跨IP计算时无需拷贝移动数据。</p></li></ul><h2 id="为什么还需要一个MLX"><a href="#为什么还需要一个MLX" class="headerlink" title="为什么还需要一个MLX"></a>为什么还需要一个MLX</h2><ul><li>Apple silicon first</li><li>Alternative Design and API</li><li>Simple, Flexible, Baggage-Free</li><li>More Exploration, More Diversity</li></ul><p><a href="https://github.com/ml-explore/mlx/issues/12">https://github.com/ml-explore/mlx/issues/12</a></p><h2 id="缺少哪些部分"><a href="#缺少哪些部分" class="headerlink" title="缺少哪些部分"></a>缺少哪些部分</h2><ol><li>图优化几近于无</li><li>序列化、反序列化功能</li><li>JIT 编译</li><li>INT量化</li></ol><h2 id="性能以及现状"><a href="#性能以及现状" class="headerlink" title="性能以及现状"></a>性能以及现状</h2><p>M2 Max</p><p>FP16 10 token&#x2F;s</p><img src="/mlx-apple/20231213220124.png" class=""><img src="/mlx-apple/20231213220841.png" class=""><h2 id="可能的-Roadmap"><a href="#可能的-Roadmap" class="headerlink" title="可能的 Roadmap"></a>可能的 Roadmap</h2><ol><li>有微调模型诉求的非算法人员。 比如lora、大模型的</li><li>基础模型开源、lora微调以后，通过core ml导出</li></ol>]]></content>
      
      
      <categories>
          
          <category> 竞分 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 竞分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从向量数据库到 ANN search</title>
      <link href="/ANN-algo/"/>
      <url>/ANN-algo/</url>
      
        <content type="html"><![CDATA[<p>LLM的模型的爆火，意外带动了向量数据库的热度。之前名不见经传的一些初创公司也突然备受追捧。最近在分析端侧LLM场景的时候也分析了相关的一些向量数据库的相关知识。</p><h1 id="GPT的缺陷"><a href="#GPT的缺陷" class="headerlink" title="GPT的缺陷"></a>GPT的缺陷</h1><p>chatgpt在对话过程中表现出的能力包括了一定的上下文检索能力。但这个能力是基于LLM本身的上下文理解能力完成的，但受限于多数模型是基于kv cache结构的记忆历史对话信息的，kv cache size是有限的，在长程记忆上就天然存在一些缺陷。另一方面，在跨对话的场景下，这些上下文信息也不能使用。如果在端侧作为一个数字助理的场景来看，这显然是不合格的。</p><p>不同模型对于 token 的限制也不同，gpt-4 是 32K tokens 的限制，而目前最大的 token 限制是 Claude 模型的 100K，这意味可以输入大约 75000 字的上下文给 GPT，这也意味着 GPT 直接理解一部《哈利波特》的所有内容并回答相关问题。</p><p>这时候就可能觉得，那我把上下文信息一起发给LLM模型不就可以了。这就到了向量数据库的场景范畴了。在处理用户输入的时候，先去通过向量查找得到一些相关信息，一起输入给LLM模型，这样就可以正确回答相关信息了。</p><img src="/ANN-algo/Embedding.png" class=""><h1 id="ANN-Search"><a href="#ANN-Search" class="headerlink" title="ANN Search"></a>ANN Search</h1><p>向量数据库说起来并不是一个新鲜的技术了，在统计机器学习时代，做KNN算法的时候就已经在研究相关的技术了。这里就简要的介绍一下原理和算法。</p><p>ANN搜索（Approximate nearest neighbor）, 本质上是在很多稠密向量中，迅速找到目标点的临近点，并认为这认为是相似的节点，主要用于图像检索、高维检索。这里隐含了一个假设，映射在同一向量空间且距离相近的点，具有相似的语义特征，距离越近越相关，反之关系越远。</p><p>当前 ANN 搜索的方法大都是对空间进行切分，可以迅速找到子空间，并与子空间的数据进行计算。方法主要有基于树的方法、哈希方法、矢量量化、基于图的方法。</p><h2 id="基于树的方法"><a href="#基于树的方法" class="headerlink" title="基于树的方法"></a>基于树的方法</h2><p>基于树的方法最经典的就是KD树了。</p><img src="/ANN-algo/kd-tree.png" class=""><p><strong>构建</strong><br>KD树构建的过程就是迭代二分空间的过程<br>经典算法：<br>选择方差最大的维度,计算中位数点，作为划分点，分为左右子树，迭代上述过程, 直到空间上的点小于阈值</p><p><strong>检索</strong><br>因为ANN这个任务并不像关系数据库中那样需要精准的结果，而是得到其中Top-K的候选结果返回。<br>KD树的检索过程其实就是一个二叉树的回溯搜索过程：</p><ol><li>根据目标p的坐标和kd树的结点向下进行搜索，如果树的结点root是以数据集的维度d以来切分的，那么如果p的维度d坐标值小于root，则走左子结点，否则走右子结点。</li><li>到达叶子结点时，将其标记为已访问。如果S中不足k个点，则将该结点加入到S中；否则如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点。</li><li>如果当前结点不是根节点，执行（a）；否则，结束算法。<br>  a.  回退到当前结点的父结点，此时的结点为当前结点（回退之后的结点）。将当前结点标记为已访问，执行（b）和（c）；如果当前结点已经被访过，再次执行（a）。<br>  b. 如果此时S中不足k个点，则将当前结点加入到S中；如果S中已有k个点，且当前结点与p点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。<br>  c. 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已有k个点，执行步骤3；如果该距离小于S中最远的距离或S中没有k个点，从当前结点的另一子节点开始执行步骤1；如果当前结点没有另一子结点，执行步骤3。</li></ol><h2 id="LSH"><a href="#LSH" class="headerlink" title="LSH"></a>LSH</h2><p>LSH即 local sensitive hash，局部敏感哈希。不同于sha256、MD5这种避免碰撞的函数，这里我们选取hash函数的时候希望语义相近的向量可以映射到同一个桶里。这里有一个前提在的：</p><blockquote><p>原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。</p></blockquote><img src="/ANN-algo/lsh.png" class=""><p><strong>构建</strong></p><ol><li>选取一组的LSH hash functions；</li><li>将所有数据经过 LSH hash function 哈希到相应的hash码，所有hash数据构成了一个hash table；</li></ol><p><strong>检索</strong></p><ol><li>将查询数据经过LSH hash function哈希得到相应的编码；</li><li>通过hamming 距离计算query数据与底库数据的距离，返回最近邻的数据</li></ol><p>当然也有其他的实现方案，这里不一一列举了。</p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>LSH这一类算法给了一个很好的加速方案，既然在原始向量空间内存在计算慢的问题，那么把向量数据映射到一个新的空间是不是就可以加速了。量化的算法就是这么想的，float型数据内存占用大，计算慢，那映射到整型数据就快了。</p><h3 id="PQ量化"><a href="#PQ量化" class="headerlink" title="PQ量化"></a>PQ量化</h3><p>PQ量化，即乘积量化，这里的乘积指的是笛卡尔积。<br>如图所示。我们有一个向量库，里面有N个向量，每个向量D维。简要介绍一下算法原理：</p><img src="/ANN-algo/PQ.png" class=""><p>PQ 量化一般分为三个步骤：</p><p><strong>Train</strong></p><ol><li>向量切分：将D维向量切分成M组子向量，每个子向量 $\frac{D}{M}$ 维。</li><li>聚类：分别在每一组子向量集合内，做Kmeans聚类，在每个子向量空间中，产生K个聚类中心。<ul><li>每个聚类中心就是一个 $\frac{D}{M}$ 维子向量，由一个id来表示，叫做clusterid。</li><li>一个子空间中所有的clusterid，构造了一个属于当前子空间的codebook。对于当前向量库，就有M个codebook。</li><li>这M个codebook所能表示的样本量级就是 $K^M$，也就是 M个codebook的笛卡尔积。</li></ul></li></ol><p><strong>建库</strong><br>对于子向量空间中的N个子向量样本，在完成Kmeans聚类之后，用这个聚类中心的clusterid来代表这个子向量。这就是构建底库的过程。</p><p>原本我们的向量库的大小为 $N\times D\times 32bit$，压缩后，clusterid按照8bit来算的话，那就是 $N\times M * 8bit $，相比压缩前少了很多。</p><p><strong>查找</strong><br>这里查找的过程存在两种方式：SDC和ADC</p><img src="/ANN-algo/SDC_ADC.png" class=""><p><strong>SDC</strong><br>S&#x3D;symmetric，对称的。如图symmetric case。图中x就是query检索向量，y就是向量库里面的向量(注意，y已经是量化过了的，就是上文中说的那个用数字id替代向量)。那么如何计算x与y的距离呢？</p><ul><li>首先，计算q(x)，拿到x对应的聚类中心；同样的，计算q(y)，拿到y对应的聚类中心。</li><li>q(x)和q(y)就是两个完整的子向量，我们计算这两个向量的距离，便是当前子空间下的距离。</li></ul><p>为什么名字叫symmetric呢？因为他俩都是用对应的聚类中心来计算距离，所以是对称的。<br>优点:</p><ul><li>两两聚类中心之间的距离，可以离线就计算好，在线直接查表，提升了在线query的效率。</li></ul><p>缺点：</p><ul><li>误差也比ADC来的大，因为有x和q(x)，y和q(y)两个量化误差。</li></ul><p><strong>ADC</strong><br>A&#x3D;asymmetric，不对称的。上文中讲了对称是因为SDC都用了对应的聚类中心。那么ADC，就只有向量库中的y使用了聚类中心，而query向量x没有。那么，计算距离的时候，计算的就是x和q(y)的距离了。ADC的精确度更高，因为只有y和q(y)这一个量化误差；当然必须要在线计算(x是用户请求带过来的)，计算速度不如SDC。</p><p><strong>计算过程</strong></p><p>将每一个子空间下的所有距离的平方相加再开根号，就是最终的X跟Y的距离了(就是使用每个子空间的向量距离进行了一次欧氏距离计算)。</p><h3 id="SQ量化"><a href="#SQ量化" class="headerlink" title="SQ量化"></a>SQ量化</h3><p>SQ量化，又叫标量量化。是按照向量维度统计min-max最值，然后将每一维向量归一化指定bit数整数的量化方式。</p><img src="/ANN-algo/SQ.jpg" class=""><p>基本原理如上图所示。</p><h2 id="IVF类方法"><a href="#IVF类方法" class="headerlink" title="IVF类方法"></a>IVF类方法</h2><p>上面讲的量化算法，仅仅并没有解决全库计算的问题，虽然数据上做了压缩，如果数据量一大，计算量还是很大。如果可以只计算最相关的一部分，是不是就可以进一步减少了呢。这就是IVF算法的思路。</p><img src="/ANN-algo/IVF.jpg" class=""><p>概括一下：<br>IVF主要利用倒排的思想保存每个聚类中心下的向量(id，vector)，每次查询向量的时候找到最近的几个中心，分别搜索这几个中心下的向量。通过减小搜索范围，大大提升搜索效率。</p><p>这里额外补充一点：</p><ul><li>IVF跟PQ结合的时候，IVF的聚类中心里面向量按照PQ量化的聚类时，我们将不会在样本上直接做PQ量化，而是对样本Y和聚类中心q(Y)的残差向量(向量减法，Y-q(Y))做PQ量化。</li></ul><h2 id="基于图的方法"><a href="#基于图的方法" class="headerlink" title="基于图的方法"></a>基于图的方法</h2><p>让我们重新回顾一下ANN这个任务：</p><img src="/ANN-algo/points.png" class=""><p>已有的向量数据库内容就是图中的点，ANN的任务就是对给定一个点找到距离最近的点。那么如果每个点都知道离自己近的点，那么是不是就可以沿着这个连接线找到相近的点了。这样就避免了与所有数据计算距离。这就是基于图算法出发点。</p><h3 id="NSW"><a href="#NSW" class="headerlink" title="NSW"></a>NSW</h3><p>NSW（navigate small world）,漫游小世界算法。对于每个新的传入元素，我们从结构中找到其最近邻居的集合（近似的 Delaunay 图， 就是上面的右图）。该集合连接到元素。随着越来越多的元素被插入到结构中，以前用作短距离边现在变成长距离边，形成可导航的小世界。</p><img src="/ANN-algo/NSW.png" class=""><p>圆（顶点）是度量空间中的数据，黑边是近似的 Delaunay 图，红边是用于对数缩放的长距离边。箭头显示从入口点到查询的贪心算法的示例路径（显示为绿色）。</p><p>图中的边有两个不同的目的：</p><ul><li>Short-range edges，用作贪婪搜索算法所需的近似 Delaunay 图。</li><li>Long-range edges，用于贪婪搜索的对数缩放。负责构造图形的可导航小世界（NSW）属性。</li></ul><p><strong>NSW查找步骤</strong></p><ol><li>随机选一个点作为初始进入点，建立空废弃表g和动态列表c，g是变长的列表，c是定长为s的列表（s&gt;m）,将初始点放入动态列表c（附上初始点和待查找q的距离信息），制作动态列表的影子列表c’。</li><li>对动态列表c中的所有点并行找出其“友点”，查看这些“友点”是否存储在废弃表g中，如果存在，则丢弃，如不存在，将这些 剩余“友点”记录在废弃列表g中（以免后续重复查找，走冤枉路）。</li><li>并行计算这些剩余“友点”距离待查找点q的距离，将这些点及其各自的距离信息放入c。</li><li>对动态列表c去重，然后按距离排序（升序），储存前s个点及其距离信息。</li><li>查看动态列表c和c’是否一样，如果一样，结束本次查找，返回动态列表中前m个结果。如果不一样，将c’的内容更新为c的 内容，执行第2步。</li></ol><p>NSW有什么问题呢：</p><ul><li>先插入的点构建的边，大都是长边；后插入的大都是短边。边的的连接关系不是很均衡。实际搜索的时候优化空间还比较大。</li></ul><h3 id="HNSW"><a href="#HNSW" class="headerlink" title="HNSW"></a>HNSW</h3><p>HNSW（Hierarchical Navigable Small World）是对 NSW 的一种改进。HNSW 借鉴了跳表的思想，根据连接的长度（距离）将连接划分为不同的层，然后就可以在多层图中进行搜索。在这种结构中，搜索从较长的连接（上层）开始，贪婪地遍历所有元素直到达到局部最小值，之后再切换到较短的连接（下层），然后重复该过程，如下图所示：</p><img src="/ANN-algo/HNSW.jpg" class=""><p>利用这种结构可以将原来 NSW 的多重对数（Polylogarithmic）计算复杂度降低至对数（Logarithmic）复杂度。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>L1 data 缓存为什么一般只有32K或者64K</title>
      <link href="/L1-cache-size/"/>
      <url>/L1-cache-size/</url>
      
        <content type="html"><![CDATA[<p>L1 data缓存为什么一般只有32K或者64K？为什么不能更大一点？更大不是更好吗？</p><p>至少有这么两个原因。L1缓存因为会频繁被访问，所以优化目标是hit time，缓存size越大，hit time越长。另外现代CPU普遍采用virtually index physically tagged（VIPT）的L1缓存，所以L1数据缓存的大小实际上就是page size * associativity。譬如linux-x86上page size一般是4K，那L1d缓存每一个way就只能放4K大小的数据，想缓存总大小大一点就得增加associativity，譬如如果associativity是8，L1d就能是32K。但是associativity太大也会导致hit time上去。再譬如像Mac OS上page size是16K，L1d缓存就能做得更大一点。</p><p>注：实际VIPT做缓存查找时，虚拟地址的部分就是页表项，所以实际上虚拟地址部分就对应了page size。为什么跟associativity有关，因为associativity决定了一个页表项的虚拟地址可以映射到几个cacheline，为了最大化利用associativity也就是page size*associativity</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 体系结构 </tag>
            
            <tag> CPU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ndk std_thread 获取pid</title>
      <link href="/ndk-pid/"/>
      <url>/ndk-pid/</url>
      
        <content type="html"><![CDATA[<p>最近在解决tvm绑核问题时，发现android下绑核只有<code>sched_setaffinity</code>函数，这导致无法使用标准库中的<code>td::thread::native_handle_type thread</code> 进行绑核操作。虽然在ndk 21以上的版本提供了<code>pthread_gettid_np</code>函数获取线程相应的pid，但在较低版本中，还是没办法直接使用。</p><p>看下ndk 中 std 标准库上thread 的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_LIBCPP_TYPE_VIS</span> thread</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">__libcpp_thread_t</span> __t_;</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">typedef</span> __thread_id id;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="type">__libcpp_thread_t</span> native_handle_type;</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">pthread_t</span> <span class="type">__libcpp_thread_t</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">pthread_t</span>;</span><br></pre></td></tr></table></figure><p>上面可以看出，在ndk的实现中<code>native_handle_type</code> 等价于<code>pthread_t</code>, 再根据<code>pthread_gettid_np</code>的实现，可以发现 ，<code>pthread_t</code> 其实就是<code>pthread_internal_t</code>的地址。在<code>pthread_internal_t</code>中保存了线程的<code>tid</code> </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">pid_t</span> <span class="title">pthread_gettid_np</span><span class="params">(<span class="type">pthread_t</span> t)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> __pthread_internal_gettid(t, <span class="string">&quot;pthread_gettid_np&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> __pthread_internal_gettid(<span class="type">pthread_t</span> thread_id, <span class="type">const</span> <span class="type">char</span>* caller) &#123;</span><br><span class="line"><span class="type">pthread_internal_t</span>* thread = __pthread_internal_find(thread_id, caller);</span><br><span class="line"><span class="keyword">return</span> thread ? thread-&gt;tid : <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* prev;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_attr_t</span> attr;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> tid;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> allocated_on_heap;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_cond_t</span> join_cond;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> join_count;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>* return_value;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> internal_flags;</span><br><span class="line"></span><br><span class="line"><span class="type">__pthread_cleanup_t</span>* cleanup_stack;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>** tls; <span class="comment">/* thread-local storage area */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* The dynamic linker implements dlerror(3), which makes it hard for us to implement this</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* per-thread buffer by simply using malloc(3) and free(3).</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __BIONIC_DLERROR_BUFFER_SIZE 512</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> dlerror_buffer[__BIONIC_DLERROR_BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">&#125; <span class="type">pthread_internal_t</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
            <tag> NDK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解LLM——LLM&amp;&amp; SD 基本概念</title>
      <link href="/LLM_SD_Basic/"/>
      <url>/LLM_SD_Basic/</url>
      
        <content type="html"><![CDATA[<h2 id="Causual-LM"><a href="#Causual-LM" class="headerlink" title="Causual LM"></a>Causual LM</h2><p>这里以llama模型为例，通常在执行用户输入之前会有一个[[文章&#x2F;LM basic知识#Prefill]]的过程。然后根据用户promts 得到输出。</p><img src="/LLM_SD_Basic/2462804-20230609220042409-2086756901.png" class=""><h3 id="Perfix-LM"><a href="#Perfix-LM" class="headerlink" title="Perfix LM"></a>Perfix LM</h3><p>这里以GLM为例介绍，展示了基本的流程。</p><img src="/LLM_SD_Basic/2462804-20230609220056534-615175021.png" class=""><h2 id="prefix-LM和causal-LM的区别"><a href="#prefix-LM和causal-LM的区别" class="headerlink" title="prefix LM和causal LM的区别"></a>prefix LM和causal LM的区别</h2><p>attention mask不同，prefix LM的prefix部分的token互相能看到，causal LM严格遵守只有后面的token才能看到前面的token的规则。</p><h2 id="Prefill"><a href="#Prefill" class="headerlink" title="Prefill"></a>Prefill</h2><p>对于causual LM，在正式推理前，需要一部分前置输入，这个过程就是Prefill。主要目的是产生 kv cache</p><blockquote><p>the prefill stage which takes a prompt sequence to generate the key-value cache (KV cache) for each transformer layer of the LLM</p></blockquote><p><strong>prefill phase</strong></p><p>$$<br>x^i_K &#x3D; x^i · w^i_K; x^i_V &#x3D; x^i · w^i_V<br>$$</p><p>$$<br>x^i_Q &#x3D; x^i · w^i_Q<br>$$<br>$$<br>x^i_{Out} &#x3D; fSoftmax(\frac{x^i_Q (x^i_K)^T}{\sqrt{h}}) · x^i_V · w^i_O + x^i \</p><p>$$</p><p>$$<br>x^(i+1) &#x3D; frelu(x^i_{out} ·w_1)·w_2+x^i_{out}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> SD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解LLM —— LoRA</title>
      <link href="/LoRA/"/>
      <url>/LoRA/</url>
      
        <content type="html"><![CDATA[<ul><li>论文链接：<a href="https://arxiv.org/abs/2106.09685">link</a></li><li>code: <a href="https://github.com/microsoft/LoRA">github</a></li></ul><h2 id="什么是LoRA"><a href="#什么是LoRA" class="headerlink" title="什么是LoRA"></a>什么是LoRA</h2><p>LoRA，英文全称<strong>L</strong>ow-<strong>R</strong>ank <strong>A</strong>daptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT（参数高效性微调方法），这是微软的研究人员为了解决大语言模型微调而开发的一项技术。当然除了LoRA，参数高效性微调方法中实现最简单的方法还是Prompt tuning，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型，建议可从Prompt tuning开始学起。</p><p>LoRA的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果</p><img src="/LoRA/2462804-20230609214112382-1836386385.png" class=""><h2 id="why-works"><a href="#why-works" class="headerlink" title="why works"></a>why works</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>给定一个预训练模型$P_{\Phi}(y|x)$ , fine tuning 的过程可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi}(y_t|x,y&lt;t))}<br>$$<br>对于fine tuning前后参数变化，其实就是<br>$$<br>\Phi &#x3D; \Phi_0+\Delta \Phi<br>$$<br>这种方案有一个缺点，对不同的下游任务，$\Delta \Phi$ 需要训练，而且$\Delta \Phi$ 的参数维度跟$\Phi$一样大，如果是GPT-3的话参数量要175B了。<br>如果$\Delta \Phi$ 够小，只调整$\Delta \Phi$ 这部分参数是不是就可以减少资源使用了。所以问题可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi_0+\Delta \Phi(\Theta)}(y_t|x,y&lt;t))}<br>$$</p><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>对于NN模型来说，权重都是满秩的。但是对于特定任务来说，</p><blockquote><p>预训练的语言模型具有较低的“固有维度”，尽管随机投影到较小的子空间，但仍然可以有效地学习<br>the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace</p></blockquote><p>基于此，假设与训练的LLM也具有这个性质，finetuning 的过程中也有一个低秩的性质。</p><p>对于权重  $W_0 \in \mathbb{R}^{d\times k}$ ,权重更新可以表示为 $W_0+\Delta W$ ,考虑低秩分解，即为$W_0+\Delta W &#x3D; W_0+BA$ , 其中$B \in \mathbb{R}^{d\times r}$, $A\in \mathbb{R}^{r\times k}$ , $r &lt;&lt; \min(d,k)$<br>则：<br>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h3><ul><li>code <a href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py">link</a></li></ul><p><a href="https://spaces.ac.cn/archives/9590">梯度视角下的lora</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TVM－MLC LLM 调优方案</title>
      <link href="/mlc-llm/"/>
      <url>/mlc-llm/</url>
      
        <content type="html"><![CDATA[<p>LLM 等GPT大模型大火以后,TVM社区推出了自己的部署方案，支持Llama，Vicuna，Dolly等模型在iOS、Android、GPU、浏览器等平台上部署运行。</p><p><a href="https://github.com/mlc-ai/mlc-llm">https://github.com/mlc-ai/mlc-llm</a></p><p>本文在之前作者介绍的基础上,简要介绍一下mlc的调优部署方案。</p><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><p>在正式介绍TVM mlc.ai部署LLM方案之前，首先简要介绍一下当前主流LLM的一个工作流程。</p><img src="/mlc-llm/2462804-20230621222850510-751335110.png" class=""><blockquote><p>需要说明一点的是，上图中的prefill跟Decode指的的同一个模型，只是输入的shape存在差异。</p></blockquote><p>这里的示意图省略了很多，只是大致描述一下pipeline。<br>在处理用户输入时，此时长度大小是不能确定的，这时候是完全的是一个完全的动态shape的。但在decode过程中由于是token by token的，这时候网络中的中除了kv cache相关几个部分，其他大多数的操作都是固定shape的，就可以用已有的算法调优了。</p><h2 id="MLC-AI-部署调优方案"><a href="#MLC-AI-部署调优方案" class="headerlink" title="MLC.AI 部署调优方案"></a>MLC.AI 部署调优方案</h2><p>以下以RedPajama3B模型的tuning跟build过程介绍一下mlc的方案。</p><h3 id="pipeline-组成"><a href="#pipeline-组成" class="headerlink" title="pipeline 组成"></a>pipeline 组成</h3><p>在已经支持的几个模型里面均有<code>get_model</code> 这个函数，在这个函数里面会创建下面4个IRModel。</p><ul><li>encoding_func</li><li>decoding_func</li><li>create_kv_cache_func</li><li>create_softmax_func</li><li>create_metadata_func</li></ul><p><strong>encoding_func</strong><br>这对应了上图中的prefill过程，在每次用户输入后调用。由于用户输入的不确定性，所以这个过程基本上都是动态shape的，很难确定到底输入是多大，也不适合搜索调优。</p><p><strong>decoding_func</strong><br>这是上图中decode过程的一部分，因为这个过程是token by token的，在计算过程中大部分的计算是固定shape的。</p><p><strong>create kv cache func</strong><br>这里是直接调用的<code>relax.vm</code>中的函数，创建的是kv cache的存储相关。</p><p><strong>create softmax func</strong><br>这个也是解码过程的一部分，确切的说是采样过程中计算的一部分</p><p>** create_metadata_func **<br>模型的meta信息，比如<code>model_name</code>、<code>stop_tokens</code>等</p><h3 id="部署优化"><a href="#部署优化" class="headerlink" title="部署优化"></a>部署优化</h3><p>构建完以后，就进入到优化的阶段了。下面根据build.py过程描述一下过程。</p><ol><li><p>API构图构建了相关的模型，读取权重</p></li><li><p>量化</p></li><li><p>优化PASS</p><ol><li>FuseTransposeMatmul</li><li>FuseDecodeMatmulEwise</li><li>DeadCodeElimination</li><li>LiftTransformParams</li><li>split_transform_deploy_mod</li></ol></li><li><p>Codegen 生成代码</p><ol><li>DispatchTIROperatorAdreno&#x2F;DispatchTIROperator&#x2F;DefaultGPUSchedule 手动优化的sch</li><li>MetaScheduleApplyDatabase搜索的log生成固定shape的sch</li></ol></li></ol><h3 id="Tuning"><a href="#Tuning" class="headerlink" title="Tuning"></a>Tuning</h3><p>在MLC-LLM的代码仓里面已经提供了tuning的脚本，有一点需要先做一下，先调用build.py的文件，把静态shape的相关的函数分离出来。就得到了tuning文件中需要的<code>mod_tir_static.py</code></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TVM 源码阅读PASS — VectorizeLoop</title>
      <link href="/VectorizeLoop/"/>
      <url>/VectorizeLoop/</url>
      
        <content type="html"><![CDATA[<p>VectorizeLoop这个PASS就是对标记为<code>ForKind::kVectorized</code>的<code>For</code>循环做向量化处理，并对For循环中的语句涉及到的变量，替换为<code>Ramp</code>，以便于在Codegen的过程中生成相关的向量化运算的指令。</p><p>VectorizeLoop这个PASS的入口函数如下，只有在打开<code>enable_vectorize=true</code>的情况下载才会被启用，否则<code>VectorizeSkipper</code>会把<code>ForKind::kVectorized</code>的<code>For</code>循环替换为普通循环。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Pass <span class="title">VectorizeLoop</span><span class="params">(<span class="type">bool</span> enable_vectorize)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) &#123;</span><br><span class="line">    <span class="keyword">auto</span>* n = f.<span class="built_in">CopyOnWrite</span>();</span><br><span class="line">    <span class="keyword">if</span> (enable_vectorize) &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">LoopVectorizer</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">VectorizeSkipper</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> f;</span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">CreatePrimFuncPass</span>(pass_func, <span class="number">0</span>, <span class="string">&quot;tir.VectorizeLoop&quot;</span>, &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面就以UT中的几个例子，介绍一下源码实现。</p><h2 id="vectorize-loop"><a href="#vectorize-loop" class="headerlink" title="vectorize_loop"></a>vectorize_loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dtype = <span class="string">&quot;int64&quot;</span></span><br><span class="line">n = te.var(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">ib = tvm.tir.ir_builder.create()</span><br><span class="line">A = ib.pointer(<span class="string">&quot;float32&quot;</span>, name=<span class="string">&quot;A&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ib.for_range(<span class="number">0</span>, n) <span class="keyword">as</span> i:</span><br><span class="line"> <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, <span class="number">4</span>, kind=<span class="string">&quot;vectorize&quot;</span>) <span class="keyword">as</span> j:</span><br><span class="line">     A[i*<span class="number">4</span>+j] += tvm.tir.const(<span class="number">1</span>, A.dtype)</span><br><span class="line">stmt = ib.get()</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(stmt.body, tvm.tir.For)</span><br><span class="line">mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([A, n], stmt))</span><br><span class="line">stmt = tvm.tir.transform.VectorizeLoop()(mod)[<span class="string">&quot;main&quot;</span>].body</span><br></pre></td></tr></table></figure><p>上面的这个代码完成的是，向量加法，长度为4n的向量A，对每个元素+1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  vectorized (j, <span class="number">0</span>, <span class="number">4</span>) &#123;</span><br><span class="line">    A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + 1f)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># after</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] = (A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] + x4(1f))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到在经过<code>VectorizeLoop</code>的PASS以后，内层的循环消掉了，替换成为了一个Ramp的向量指令，这个在CPU中会被替换为SIMD指令（neon，AVX等）</p><h4 id="PASS流程"><a href="#PASS流程" class="headerlink" title="PASS流程"></a>PASS流程</h4><p>在向量化的处理的PASS中是在LoopVectorizer中处理的，处理For循环部分。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoopVectorizer</span> : <span class="keyword">public</span> StmtMutator &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">Stmt <span class="title">VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> <span class="keyword">final</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (op-&gt;kind == ForKind::kVectorized) &#123;</span><br><span class="line">      <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">      <span class="keyword">auto</span>* extent_as_int = op-&gt;extent.<span class="built_in">as</span>&lt;IntImmNode&gt;();</span><br><span class="line">      <span class="keyword">if</span> (!extent_as_int || extent_as_int-&gt;value &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Failed to vectorize loop with extent &quot;</span> &lt;&lt; op-&gt;extent;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">Vectorizer</span>(op-&gt;loop_var, <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(extent_as_int-&gt;value))(op-&gt;body);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> StmtMutator::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当遇到需要向量化的节点时，首先记录循环变量和范围，这个在后续替换相应的Load和Store操作为Ramp时用到。然后就到了Vectorizer部分，遍历For循环体，修改相应的stmt。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Vectorizer</span>(Var var, <span class="type">int</span> var_lanes) : <span class="built_in">var_</span>(var), <span class="built_in">var_lanes_</span>(var_lanes) &#123;</span><br><span class="line">    ramp_ = <span class="built_in">Ramp</span>(<span class="number">0</span>, <span class="number">1</span>, var_lanes);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Vectorizer中对不同的<code>PrimExpr</code>、<code>Stmt</code>做了重载。这里不逐一介绍，就以上面的向量加计算，介绍一下用到的函数以及流程。</p><p>首先看一下这里的上面sch的For的循环内的计算逻辑：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + <span class="number">1f</span>)</span><br></pre></td></tr></table></figure><p>因为TVM中，Stmt的表达可以视为一个DSL的语言，访问的时候也是按照深度优先的策略遍历的AST，这里把上面的计算过程简单表示为一个AST的语法树，然后再分析一下流程中调用的各个函数是如何处理的。</p><img src="/VectorizeLoop/2462804-20230624144328795-2055285024.png" class=""><p>从上面的AST的示意图可以看出来，对于上面的sch，依次访问了<code>BufferStoreNode</code>、<code>Add</code> <code>Mul</code>、<code>BufferLoadNode</code> 等。这里就以这几个Node的处理介绍一下向量化的过程。</p><p>所谓向量化的过程就是把这个标记为<code>kVectorized</code>的标量循环操作映射到向量化的操作，对于上面的例子来说就是把所有关于<code>j</code>的访问映射为RampNode，以便于后续处理可以正确生成相应的指令。</p><h5 id="BufferStoreNode"><a href="#BufferStoreNode" class="headerlink" title="BufferStoreNode"></a>BufferStoreNode</h5><p><code>BufferStoreNode</code>中有三部分：</p><ul><li>buffer——写入的buffer</li><li>value——待写入的值或者表达式</li><li>indices——写入buffer的坐标<br>这里的目的就是修改<code>value</code>和<code>indices</code>中的内容。<br>对于<code>indices</code>，是在这里完成的。最终通过<code>MapHelper</code>依次访问了<code>indices</code>的表达式。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> fmutate = [<span class="keyword">this</span>](<span class="type">const</span> PrimExpr&amp; index) &#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(index); &#125;;</span><br><span class="line">Array&lt;PrimExpr&gt; indices = op-&gt;indices.<span class="built_in">Map</span>(fmutate);</span><br></pre></td></tr></table></figure><p>对于<code>value</code> 则是直接遍历。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr value = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;value);</span><br></pre></td></tr></table></figure><h5 id="AddNode"><a href="#AddNode" class="headerlink" title="AddNode"></a>AddNode</h5><p>对于<code>AddNode</code>和<code>SubNode</code> 都会走到<code>AddSubVec</code>这个模板函数。<br>这个函数里面首先会遍历左右表达式，</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"> <span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; b_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a, b_ramp-&gt;base),</span><br><span class="line"> <span class="built_in">fcompute</span>(<span class="built_in">make_zero</span>(b_ramp-&gt;stride.<span class="built_in">dtype</span>()), b_ramp-&gt;stride), b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; a_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a_ramp-&gt;base, b), a_ramp-&gt;stride, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">fcompute</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br></pre></td></tr></table></figure><p>如果遍历之后没有变化，就直接返回了。而对于这里的我们需要计算的是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((i*<span class="number">4</span>) + j)</span><br></pre></td></tr></table></figure><p><code>j</code> 是需要向量化的坐标。<code>i*4</code> 是没有变化的。遍历以后<code>a</code>没变化，<code>b</code>变成了<code>T.Ramp(0, 1, 4)</code> 这时候<code>lanes=4</code>，会走到第一个<code>if</code>分支，返回的是新构造的<code>RampNode</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T.Ramp(i * 4, 1, 4)</span><br></pre></td></tr></table></figure><p>其他的分支也类似。比如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A[i * <span class="number">4</span> + j] + T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// --- after ---</span></span><br><span class="line">A[i * <span class="number">4</span>:i * <span class="number">4</span> + <span class="number">4</span>]   T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里会把a、b broadcast为一个向量再做计算。</p><h5 id="VarNode"><a href="#VarNode" class="headerlink" title="VarNode"></a>VarNode</h5><p>对于这里的VarNode判断就比较简单了，如果匹配到的是需要向量化的变量，就返回构造函数中构造的<code>RampNode</code>，否则就返回。其他的操作，暂时略过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Var var = <span class="built_in">GetRef</span>&lt;Var&gt;(op);</span><br><span class="line"><span class="keyword">if</span> (var.<span class="built_in">same_as</span>(var_)) &#123;</span><br><span class="line"> <span class="keyword">return</span> ramp_;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"> <span class="keyword">return</span> std::<span class="built_in">move</span>(var);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="MulNode"><a href="#MulNode" class="headerlink" title="MulNode"></a>MulNode</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a_ramp &amp;&amp; b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(b &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(a_ramp-&gt;base * b, a_ramp-&gt;stride * b, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b_ramp &amp;&amp; a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(a &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(b_ramp-&gt;base * a, b_ramp-&gt;stride * a, b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">Mul</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">BinaryVec</span>&lt;Mul&gt;(op);</span><br></pre></td></tr></table></figure><p>这里的处理逻辑与Add基本一致。只是在计算RampNode的时候有点区别。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVE特性以及寄存器</title>
      <link href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/"/>
      <url>/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>SVE对比NEON有几个新增的地方。</p><ol><li>变长的向量</li><li>支持Gather-load &amp;&amp; Scatter-store</li></ol><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/gather.png" class=""><ol start="3"><li><p>可以由P寄存器控制向量通道的计算</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/Pvector.png" class=""></li><li><p>由软件控制的向量切分。</p><ol><li>基于First Fault 寄存器完成的，加载不合法内存页的时候，会有记录 <img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/20230905222847.png" class=""></li></ol></li><li><p>扩展浮点和位运算的水平缩减</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/140339805.png" class=""></li></ol><h2 id="SVE-寄存器"><a href="#SVE-寄存器" class="headerlink" title="SVE 寄存器"></a>SVE 寄存器</h2><ul><li>Scalable vector registers<br><code>Z0-Z15</code>, 支持double、float、float16，int64、int32、int16、int8<br>向量寄存器长度128-2048bit可变，具体取决于SoC厂商确定，当前手机上上商用的由联发科的天玑9200，长度是128bit，这部分与NEON共用。</li><li>Scalable predicate registers<br>谓词寄存器，<ul><li>P0-P7 控制的数据加载、存取、计算</li><li>P8-P15做循环控制</li><li>FFR ： 用来软件推测的FFR寄存器<img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/SVE.png" class=""></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 体系结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tir_to_llvm_ir</title>
      <link href="/tir-to-llvm-ir/"/>
      <url>/tir-to-llvm-ir/</url>
      
        <content type="html"><![CDATA[<p>TVM在编译过程中，经历了</p><pre class="mermaid">graph LR  A[3rd IR] --> B[Relay IR]  B --> C[TIR]  C --> D[LLVM IR]  C -->E[Source]</pre><p>这一系列的过程。其中在生成cpu、rocm、nvptx、hexagon等平台的相关代码的时候，会先由TVM的<code>TIR</code>转换为<code>LLVM IR</code>,在后续由LLVM生成相关的机器码。</p><p>这一步是由<code>tvm::codegen::Build</code>调用转换的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">runtime::Module <span class="title">Build</span><span class="params">(IRModule mod, Target target)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (transform::PassContext::<span class="built_in">Current</span>()</span><br><span class="line">          -&gt;<span class="built_in">GetConfig</span>&lt;Bool&gt;(<span class="string">&quot;tir.disable_assert&quot;</span>, <span class="built_in">Bool</span>(<span class="literal">false</span>))</span><br><span class="line">          .<span class="built_in">value</span>()) &#123;</span><br><span class="line">    mod = tir::transform::<span class="built_in">SkipAssert</span>()(mod);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> target_attr_map = tvm::TargetKind::<span class="built_in">GetAttrMap</span>&lt;FTVMTIRToRuntime&gt;(<span class="string">&quot;TIRToRuntime&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (target_attr_map.<span class="built_in">count</span>(target-&gt;kind)) &#123;</span><br><span class="line">    <span class="keyword">return</span> target_attr_map[target-&gt;kind](mod, target);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the build function.</span></span><br><span class="line">  std::string build_f_name = <span class="string">&quot;target.build.&quot;</span> + target-&gt;kind-&gt;name;</span><br><span class="line">  <span class="type">const</span> PackedFunc* bf = runtime::Registry::<span class="built_in">Get</span>(build_f_name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(bf != <span class="literal">nullptr</span>) &lt;&lt; build_f_name &lt;&lt; <span class="string">&quot; is not enabled&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> (*bf)(mod, target);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在LLVM相关的target时候，这里的<code>build_f_name</code>就是<code>target.build.llvm</code></p><p>这时候会走到</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;target.build.llvm&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](IRModule mod, Target target) -&gt; runtime::Module &#123;</span><br><span class="line">      <span class="keyword">auto</span> n = <span class="built_in">make_object</span>&lt;LLVMModuleNode&gt;();</span><br><span class="line">      n-&gt;<span class="built_in">Init</span>(mod, target);</span><br><span class="line">      <span class="keyword">return</span> runtime::<span class="built_in">Module</span>(n);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>在<code>Init</code>函数中创建codegen的具体类：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LLVMModuleNode::Init</span><span class="params">(<span class="type">const</span> IRModule&amp; mod, <span class="type">const</span> Target&amp; target)</span> </span>&#123;</span><br><span class="line">  llvm_instance_ = std::<span class="built_in">make_unique</span>&lt;LLVMInstance&gt;();</span><br><span class="line">  <span class="function">With&lt;LLVMTarget&gt; <span class="title">llvm_target</span><span class="params">(*llvm_instance_, target)</span></span>;</span><br><span class="line">  llvm::TargetMachine* tm = llvm_target-&gt;<span class="built_in">GetOrCreateTargetMachine</span>();</span><br><span class="line">  <span class="comment">// 这里会根据target得到不同的codegen的实现类</span></span><br><span class="line">  std::unique_ptr&lt;CodeGenLLVM&gt; cg = CodeGenLLVM::<span class="built_in">Create</span>(llvm_target.<span class="built_in">get</span>());</span><br><span class="line"></span><br><span class="line">  std::string entry_func;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  skip crt/cpp systemlib options</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> kv : mod-&gt;functions) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!kv.second-&gt;<span class="built_in">IsInstance</span>&lt;PrimFuncNode&gt;()) &#123;</span><br><span class="line">      <span class="comment">// (@jroesch): we relax constraints here, Relay functions will just be ignored.</span></span><br><span class="line">      <span class="built_in">DLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Can only lower IR Module with PrimFuncs, but got &quot;</span> &lt;&lt; kv.second-&gt;<span class="built_in">GetTypeKey</span>();</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> f = <span class="built_in">Downcast</span>&lt;PrimFunc&gt;(kv.second);</span><br><span class="line">    <span class="keyword">auto</span> global_symbol = f-&gt;<span class="built_in">GetAttr</span>&lt;String&gt;(tvm::attr::kGlobalSymbol);</span><br><span class="line">    <span class="type">bool</span> is_entry_func = f-&gt;<span class="built_in">HasNonzeroAttr</span>(tir::attr::kIsEntryFunc);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (global_symbol) &#123;</span><br><span class="line">      function_names_.<span class="built_in">push_back</span>(global_symbol.<span class="built_in">value</span>());</span><br><span class="line">      <span class="keyword">if</span> (is_entry_func) &#123;</span><br><span class="line">        entry_func = global_symbol.<span class="built_in">value</span>();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 初始化CodeGenLLVM, 会产生builder_, module_等llvm 中codegen需要的基础数据结构</span></span><br><span class="line">  cg-&gt;<span class="built_in">Init</span>(<span class="string">&quot;TVMMod&quot;</span>, llvm_target.<span class="built_in">get</span>(), system_lib_prefix, </span><br><span class="line">             system_lib_prefix.<span class="built_in">defined</span>(),</span><br><span class="line">           target_c_runtime);</span><br><span class="line">  cg-&gt;<span class="built_in">SetFastMathFlags</span>(llvm_target-&gt;<span class="built_in">GetFastMathFlags</span>());</span><br><span class="line">    <span class="comment">// 核心功能,tir 转化为llvm ir就在此</span></span><br><span class="line">  cg-&gt;<span class="built_in">AddFunctionsOrdered</span>(mod-&gt;functions.<span class="built_in">begin</span>(), mod-&gt;functions.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">if</span> (entry_func.<span class="built_in">length</span>() != <span class="number">0</span>) &#123;</span><br><span class="line">    cg-&gt;<span class="built_in">AddMainFunction</span>(entry_func);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  module_owning_ptr_ = cg-&gt;<span class="built_in">Finish</span>();</span><br><span class="line">  module_ = module_owning_ptr_.<span class="built_in">get</span>();</span><br><span class="line">  llvm_target-&gt;<span class="built_in">SetTargetMetadata</span>(module_);</span><br><span class="line">  module_-&gt;<span class="built_in">addModuleFlag</span>(llvm::Module::Override, <span class="string">&quot;Debug Info Version&quot;</span>,</span><br><span class="line">                         llvm::DEBUG_METADATA_VERSION);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux_shell中提取文件名和路径</title>
      <link href="/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/"/>
      <url>/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/</url>
      
        <content type="html"><![CDATA[<p>首先假设我的文件全称：&#x2F;home&#x2F;luna&#x2F;Desktop&#x2F;Software&#x2F;softHLA&#x2F;HLAreporter.v103&#x2F;HLAreporter.sh.</p><h1 id="获取文件名"><a href="#获取文件名" class="headerlink" title="获取文件名"></a>获取文件名</h1><h1 id="使用-，-str"><a href="#使用-，-str" class="headerlink" title="使用${}，${str##*/}"></a>使用<code>$&#123;&#125;，$&#123;str##*/&#125;</code></h1><p>这个命令的作用就是去掉变量str从左边算起的最后一个&#x2F;字符及其左边的内容，返回的值是从左边算起的最后一个&#x2F;（不含该字符）的右边的所有内容，例子很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=<span class="variable">$&#123;str##*/&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$file</span></span><br><span class="line">HLAreporter.sh  <span class="comment">## 运行结果</span></span><br></pre></td></tr></table></figure><h1 id="使用awk语句"><a href="#使用awk语句" class="headerlink" title="使用awk语句"></a>使用awk语句</h1><p>因为在ubuntu下面，路径都是以&#x2F;为隔开的，那么我们就以&#x2F;为分隔符，然后把最后部分打印，赋值，例子如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=`echo $str | awk -F &quot;/&quot; &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="使用官方函数basename"><a href="#使用官方函数basename" class="headerlink" title="使用官方函数basename"></a>使用官方函数basename</h1><p>bash shell本身提供了basename命令，可以直接获取路径名最后的文件名，实现代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$(basename $str)</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="后缀和文件名分开"><a href="#后缀和文件名分开" class="headerlink" title="后缀和文件名分开"></a>后缀和文件名分开</h1><p>使用${}<br>在这里分别使用&#x2F;和.作为分隔符来进行处理，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%.*&#125;</span><br><span class="line">suffix=$&#123;file##*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.sh, HLAreporter, sh</span><br><span class="line"></span><br><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103.tar.gz</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%%.*&#125;</span><br><span class="line">suffix=$&#123;file#*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.v103.tar.gz, HLAreporter, v103.tar.gz</span><br></pre></td></tr></table></figure><p>用的是Shell的参数扩展(Parameter Extension)功能，解释如下：</p><p><code>$&#123;str##*/&#125;</code>: 从左边开始删除str中最大匹配(longest matching pattern) <em>&#x2F; 的字符串<br><code>$&#123;str%/*&#125;</code>：从右边开始删除str中最小匹配(shortest matching pattern) &#x2F;</em> 的字符串<br><code>$&#123;file##*.&#125;</code>：从左边开始删除file中最大匹配(longest matching pattern) <em>. 的字符串<br><code>$&#123;file%.*&#125;</code>：从右边开始删除file中最小匹配(shortest matching pattern) .</em> 的字符串<br><code>$&#123;file%%.*&#125;</code>：从右边开始删除file中最大匹配(longest matching pattern) .* 的字符串<br><code>$&#123;file#*.&#125;</code>：从左边开始删除file中小匹配(shortest matching pattern) *. 的字符串<br><code>#</code>：表示从左边算起第一个<br><code>%</code>：表示从右边算起第一个<br><code>##</code>：表示从左边算起最后一个<br><code>%%</code>：表示从右边算起最后一个<br>换句话来说，<code>＃</code>总是表示左边算起，<code>％</code>总是表示右边算起。<br>参数扩展有多种形式，在shell编程中可以用作参数的拼接，字符串的替换，参数列表截取，变量初值等操作，这里不再详述，请参考右边的功能列表和官方文档.</p><h1 id="参数扩展功能列表"><a href="#参数扩展功能列表" class="headerlink" title="参数扩展功能列表"></a>参数扩展功能列表</h1><p>参数形式扩展后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x&#123;y,z&#125;xy xz</span><br><span class="line">$&#123;x&#125;&#123;y, z&#125;$&#123;x&#125;y $&#123;x&#125;z</span><br><span class="line">$&#123;x&#125;&#123;y, $z&#125;$&#123;x&#125;y $&#123;x&#125;$&#123;z&#125;</span><br><span class="line">$&#123;param#pattern&#125;从param前面删除pattern的最小匹配</span><br><span class="line">$&#123;param##pattern&#125;从param前面删除pattern的最大匹配</span><br><span class="line">$&#123;param%pattern&#125;从param后面删除pattern的最小匹配</span><br><span class="line">$&#123;param%%pattern&#125;从param后面删除pattern的最大匹配</span><br><span class="line">$&#123;param/pattern/string&#125;从param中用string替换pattern的第一次匹配，string可为空</span><br><span class="line">$&#123;param//pattern/string&#125;从param中用string替换pattern的所有匹配，string可为空</span><br><span class="line">$&#123;param:3:2&#125;截取$param中索引3开始的2个字符</span><br><span class="line">$&#123;param:3&#125;截取$param中索引3至末尾的字符</span><br><span class="line">$&#123;@:3:2&#125;截取参数列表$@中第3个开始的2个参数</span><br><span class="line">$&#123;param:-word&#125;若$param为空或未设置，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:+word&#125;若$param为非空，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:=word&#125;若$param为空或为设置，则参数式返回word，同时$param设置为word</span><br><span class="line">$&#123;param:?message&#125;若$param为空或为设置，则输出错误信息message，若包含空白符，则需引号</span><br></pre></td></tr></table></figure><p>获取路径名<br>使用${}，${str%&#x2F;*}<br>去掉变量var从右边算起的第一个’&#x2F;’字符及其右边的内容，返回从右边算起的第一个’&#x2F;’（不含该字符）的左边的内容。使用例子及结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">path=$&#123;str%/*&#125;</span><br><span class="line">echo $path</span><br><span class="line">/home/luna/Desktop/Software/softHLA/HLAreporter.v103</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>packfunc</title>
      <link href="/packfunc/"/>
      <url>/packfunc/</url>
      
        <content type="html"><![CDATA[<p>为实现多种语言支持，需要满足以下几点：</p><ul><li>部署：编译结果可以从<code>python/javascript/c++</code>调用。</li><li>Debug: 在python中定义一个函数，在编译函数中调用。</li><li>链接：编写驱动程序以调用设备特定代码（如CUDA），可以在编译的host侧调用</li><li>原型：python侧定义IR PASS，并从C++后端调用该代码</li><li>接口暴露：c++后端代码暴露到python侧</li><li>实验：将编译的函数运送到嵌入式设备，可以直接在嵌入式设备上运行</li></ul><p>tvm希望在任何一个语言中定义的函数，可以在其他的语言中都可以调用。同样希望runtime尽可能的轻量化，以方便在嵌入式设备上部署。</p><h1 id="PackedFunc"><a href="#PackedFunc" class="headerlink" title="PackedFunc"></a>PackedFunc</h1><p><code>PackedFunc</code>是解决上述问题的一个优雅的方案。一个<code>PackedFunc</code>对象对应着一个函数调用，即使定义与调用分散在不同语言之间也可以满足。下面展示一个C++的例子。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tvm/runtime/packed_func.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MyAdd</span><span class="params">(TVMArgs args, TVMRetValue* rv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// automatically convert arguments to desired type.</span></span><br><span class="line">  <span class="type">int</span> a = args[<span class="number">0</span>];</span><br><span class="line">  <span class="type">int</span> b = args[<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// automatically assign value return to rv</span></span><br><span class="line">  *rv = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CallPacked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  PackedFunc myadd = <span class="built_in">PackedFunc</span>(MyAdd);</span><br><span class="line">  <span class="comment">// get back 3</span></span><br><span class="line">  <span class="type">int</span> c = <span class="built_in">myadd</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的例子中，定义了一个<code>MyAdd</code>的<code>PackedFunc</code>，接受两个参数，<code>args</code>表示输入参数， <code>rv</code>表示返回值。这个参数是类型无关的(type-erased)，这意味着函数签名中对输入输出参数的类型没有限制。这样，当调用这个函数的时候， 从栈上获取输入参数（TVMArgs），通过TVMRetValue返回函数返回值。</p><p>通过C++的模板技巧，可以像正常函数一样调用<code>PackedFunc</code>。由于类型无关的特性，可以在像python这样的动态类型的语言中调用<code>PackedFunc</code>，而无需插入额外其他的胶水代码。下面展示了<code>PackedFunc</code> 的注册及其在python端的调用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// register a global packed function in c++</span></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>(MyAdd);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line">myadd = tvm.get_global_func(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line"><span class="comment"># prints 3</span></span><br><span class="line"><span class="built_in">print</span>(myadd(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>多数的<code>PackedFunc</code>技巧依赖于<code>TVMArgs</code>和<code>TVMRetValue</code>，我们限制其中的参数类型，下面是主要用的类型：</p><ul><li>int, float and string</li><li>PackedFunc itself</li><li>Module for compiled modules</li><li>DLTensor* for tensor object exchange</li><li>TVM Object to represent any object in IR</li></ul><p>这个限制，使得实现及其简单而且无需序列化操作。虽然增加了限制，但对于DL开发来说，大多数场景下仅仅需要传递<code>DLTensor</code>和数字就够了。</p><p>既然<code>PackedFunc</code>可以将另外的PackedFunc作为函数参数，那就可以在python与c++之间传递函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">callback</span>(<span class="params">msg</span>):</span><br><span class="line">  <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PackedFunc</span></span><br><span class="line">f = tvm.convert(callback)</span><br><span class="line">callhello = tvm.get_global_func(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line"><span class="comment"># prints hello world</span></span><br><span class="line">callhello(f)</span><br></pre></td></tr></table></figure><p>TVM 提供了极简的C API，使得将PackedFunc可以方便地嵌入到其他的语言中。除python外，还支持java、JavaScript。</p><p>PackFunction不仅用于tvm编译器中，同样也用于开发的技术栈中。在tvm中所有的PASS函数都通过PackedFunc暴露给前端的。编译结果同样是通过PackedFunc打包的。</p><p>为了保证runtime尽可能的小，runtime中隔离了IR对象的支持。这使得runtime大小只有200~600k，具体的大小取决于平台驱动部分。</p><p>PackedFunc带来的调用开销很小，仅仅是通过栈传递了一些参数对象，只要不通过它包装较小的函数，就是OK的。总之，PackedFunc是tvm中通用的胶水代码，支持了tvm的编译部署。</p><p>额外的部分：</p><h2 id="c-注册，python调用"><a href="#c-注册，python调用" class="headerlink" title="c++ 注册，python调用"></a>c++ 注册，python调用</h2><p>上文中介绍注册时，使用到了一个C++宏<code>TVM_REGISTER_GLOBAL</code>，这里介绍中间是如何链接起来的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//展开就是</span></span><br><span class="line"><span class="built_in">TVM_STR_CONCAT</span>(TVM_FUNC_REG_VAR_DEF, __COUNTER__) = ::tvm::runtime::Registry::<span class="built_in">Register</span>(<span class="string">&quot;callhello&quot;</span>).<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>这里的<code>::tvm::runtime::Registry::Register</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Registry&amp; <span class="title">Registry::Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> can_override)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  Manager* m = Manager::<span class="built_in">Global</span>();<span class="comment">//这是个静态对象，Manager持有一个map来记录注册对象</span></span><br><span class="line">  <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(m-&gt;mutex)</span></span>;</span><br><span class="line">  <span class="keyword">if</span> (m-&gt;fmap.<span class="built_in">count</span>(name)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(can_override) &lt;&lt; <span class="string">&quot;Global PackedFunc &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is already registered&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Registry* r = <span class="keyword">new</span> <span class="built_in">Registry</span>();</span><br><span class="line">  r-&gt;name_ = name;</span><br><span class="line">  m-&gt;fmap[name] = r;</span><br><span class="line">  <span class="keyword">return</span> *r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面看下Registry的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Registry for global function */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Registry</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">//设置函数体</span></span><br><span class="line">  <span class="function">TVM_DLL Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc f)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc::FType f)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">PackedFunc</span>(f));</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//给一个任意函数，萃取函数签名</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> FLambda&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_typed</span><span class="params">(FLambda f)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> FType = <span class="keyword">typename</span> detail::function_signature&lt;FLambda&gt;::FType;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;FType&gt;(std::<span class="built_in">move</span>(f), name_).<span class="built_in">packed</span>());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//给一个类成员函数、返回值、参数，使用lambda包装</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...))</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...) <span class="type">const</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](<span class="type">const</span> T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(<span class="type">const</span> T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...) <span class="type">const</span>) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      <span class="type">const</span> TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> Registry&amp; <span class="title">Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> <span class="keyword">override</span> = <span class="literal">false</span>)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">bool</span> <span class="title">Remove</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">const</span> PackedFunc* <span class="title">Get</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>; </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> std::vector&lt;std::string&gt; <span class="title">ListNames</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  std::string name_;</span><br><span class="line">  PackedFunc func_;</span><br><span class="line">  <span class="keyword">friend</span> <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上面注册以后是在一个全局对象中，下一部就看python侧如何调用的。</p><p>python端最终会调用到 <code>_get_global_func</code>函数，具体实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_func</span>(<span class="params">name, allow_missing=<span class="literal">False</span></span>):</span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    check_call(_LIB.TVMFuncGetGlobal(c_str(name), ctypes.byref(handle)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> handle.value:</span><br><span class="line">        <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> allow_missing:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot find global function %s&quot;</span> % name)</span><br></pre></td></tr></table></figure><p>进而会调用到<code>TVMFuncGetGlobal</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncGetGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle* out)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  <span class="type">const</span> tvm::runtime::PackedFunc* fp = tvm::runtime::Registry::<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="keyword">if</span> (fp != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    *out = <span class="keyword">new</span> tvm::runtime::<span class="built_in">PackedFunc</span>(*fp);  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *out = <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里既可以发现<code>tvm::runtime::Registry::Get(name)</code>来查找相关注册函数的。</p><h2 id="python注册，c-调用"><a href="#python注册，c-调用" class="headerlink" title="python注册，c++ 调用"></a>python注册，c++ 调用</h2><p>如下面的函数，通过装饰器注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_func(<span class="params"><span class="string">&quot;relay.backend.lower_call&quot;</span></span>)</span></span><br></pre></td></tr></table></figure><p>在c++中调用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">auto</span> flower_call = tvm::runtime::Registry::<span class="built_in">Get</span>(<span class="string">&quot;relay.backend.lower_call&quot;</span>);</span><br></pre></td></tr></table></figure><p>下面介绍以下python的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_func</span>(<span class="params">func_name, f=<span class="literal">None</span>, override=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">callable</span>(func_name):</span><br><span class="line">        f = func_name</span><br><span class="line">        func_name = f.__name__</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(func_name, <span class="built_in">str</span>):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;expect string function name&quot;</span>)</span><br><span class="line"></span><br><span class="line">    ioverride = ctypes.c_int(override)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">myf</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;internal register function&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(myf, PackedFuncBase):</span><br><span class="line">            myf = convert_to_tvm_func(myf) <span class="comment">#转化为packfunc</span></span><br><span class="line">        <span class="comment">#注册</span></span><br><span class="line">        check_call(_LIB.TVMFuncRegisterGlobal(c_str(func_name), myf.handle, ioverride))</span><br><span class="line">        <span class="keyword">return</span> myf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        <span class="keyword">return</span> register(f)</span><br><span class="line">    <span class="keyword">return</span> register</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to_tvm_func</span>(<span class="params">pyfunc</span>):</span><br><span class="line">    local_pyfunc = pyfunc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cfun</span>(<span class="params">args, type_codes, num_args, ret, _</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; ctypes function &quot;&quot;&quot;</span></span><br><span class="line">        num_args = num_args.value <span class="keyword">if</span> <span class="built_in">isinstance</span>(num_args, ctypes.c_int) <span class="keyword">else</span> num_args</span><br><span class="line">        pyargs = (C_TO_PY_ARG_SWITCH[type_codes[i]](args[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_args))</span><br><span class="line">        <span class="comment"># pylint: disable=broad-except</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rv = local_pyfunc(*pyargs)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            msg = traceback.format_exc()</span><br><span class="line">            msg = py2cerror(msg)</span><br><span class="line">            _LIB.TVMAPISetLastError(c_str(msg))</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rv <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(rv, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;PackedFunction can only support one return value&quot;</span>)</span><br><span class="line">            temp_args = []</span><br><span class="line">            values, tcodes, _ = _make_tvm_args((rv,), temp_args)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(ret, TVMRetValueHandle):</span><br><span class="line">                ret = TVMRetValueHandle(ret)</span><br><span class="line">            <span class="keyword">if</span> _LIB.TVMCFuncSetReturn(ret, values, tcodes, ctypes.c_int(<span class="number">1</span>)) != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">            _ = temp_args</span><br><span class="line">            _ = rv</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    f = TVMPackedCFunc(cfun)</span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> We will need to use python-api to increase ref count of the f</span></span><br><span class="line">    <span class="comment"># TVM_FREE_PYOBJ will be called after it is no longer needed.</span></span><br><span class="line">    pyobj = ctypes.py_object(f)</span><br><span class="line">    ctypes.pythonapi.Py_IncRef(pyobj)</span><br><span class="line">    <span class="keyword">if</span> _LIB.TVMFuncCreateFromCFunc(f, pyobj, TVM_FREE_PYOBJ, ctypes.byref(handle)) != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">    <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncRegisterGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle f, <span class="type">int</span> <span class="keyword">override</span>)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  tvm::runtime::Registry::<span class="built_in">Register</span>(name, <span class="keyword">override</span> != <span class="number">0</span>)</span><br><span class="line">      .<span class="built_in">set_body</span>(*<span class="built_in">static_cast</span>&lt;tvm::runtime::PackedFunc*&gt;(f));</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM教程】 自定义relay算子</title>
      <link href="/tvm-custom-op/"/>
      <url>/tvm-custom-op/</url>
      
        <content type="html"><![CDATA[<p>本文为tvm 教程的翻译版。这部分介绍了如何在tvm中添加新的relay算子，具体的是以一个累乘（cumprod）算子为例进行介绍。</p><p>新增relay算子基本是下面几个步骤：</p><ol><li>定义新增算子的属性节点（Attribute Node），声明在编译时已知的固定参数</li><li>为新增算子编写类型关系，以集成到relay的类型系统中</li><li>使用C++ <code>RELAY_REGISTER_OP</code> 宏，为新增算子注册生命参数数量、类型、提示信息</li><li>算子的compute</li><li>注册算子的compute、schedule</li><li>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</li><li>将上面的 Python API hook 封装成简洁的调用方式</li><li>为新的relay 算子编写测试</li></ol><h2 id="新增算子的属性节点"><a href="#新增算子的属性节点" class="headerlink" title="新增算子的属性节点"></a>新增算子的属性节点</h2><p>算子属性是编译期已知的参数。以卷积算子为例，strid、dilation就属于卷积算子的属性。这部分算子属性定义在<code>include/tvm/relay/attrs/</code>下。<br>最终来说，我们期望定义有如下属性说明的算子，其python侧的接口如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Numpy style cumprod op. Return the cumulative inclusive product of the elements along</span></span><br><span class="line"><span class="string">    a given axis.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    data : relay.Expr</span></span><br><span class="line"><span class="string">        The input data to the operator.</span></span><br><span class="line"><span class="string">    axis : int, optional</span></span><br><span class="line"><span class="string">        Axis along which the cumulative product is computed. The default (None) is to compute</span></span><br><span class="line"><span class="string">        the cumprod over the flattened array.</span></span><br><span class="line"><span class="string">    dtype : string, optional</span></span><br><span class="line"><span class="string">        Type of the returned array and of the accumulator in which the elements are multiplied.</span></span><br><span class="line"><span class="string">        If dtype is not specified, it defaults to the dtype of data.</span></span><br><span class="line"><span class="string">    exclusive : bool, optional</span></span><br><span class="line"><span class="string">        If true will return exclusive product in which the first element is not</span></span><br><span class="line"><span class="string">        included. In other terms, if true, the j-th output element would be</span></span><br><span class="line"><span class="string">        the product of the first (j-1) elements. Otherwise, it would be the product of</span></span><br><span class="line"><span class="string">        the first j elements. The product of zero elements will be 1.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    result : relay.Expr</span></span><br><span class="line"><span class="string">        The result has the same size as data, and the same shape as data if axis is not None.</span></span><br><span class="line"><span class="string">        If axis is None, the result is a 1-d array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><code>.cumsum()</code>有类似的接口。</p><p>因此，在定义我们新增算子（cumprod）属性时，需要选择操作的轴、数据类型和排他性作为属性字段。<code>include/tvm/relay/attrs/transform.h</code></p><p>ScanopAttrs 这里定义了对累加、累乘等操作的属性定义。对累乘来说就不需要额外定义了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Attributes used in cumsum and cumprod operator */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ScanopAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;ScanopAttrs&gt; &#123;</span><br><span class="line">  Integer axis;</span><br><span class="line">  DataType dtype;</span><br><span class="line">  Bool exclusive = <span class="built_in">Bool</span>(<span class="literal">false</span>);</span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(ScanopAttrs, <span class="string">&quot;relay.attrs.ScanopAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to operate over&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;Integer&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(dtype).<span class="built_in">describe</span>(<span class="string">&quot;Output data type&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;DataType&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(exclusive)</span><br><span class="line">        .<span class="built_in">describe</span>(<span class="string">&quot;The first element is not included&quot;</span>)</span><br><span class="line">        .<span class="built_in">set_default</span>(<span class="built_in">Bool</span>(<span class="literal">false</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>但是如果是其他的算子，需要自己定义相应的属性节点。如<code>BiasAdd</code>就需要单独定义</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">BiasAddAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;BiasAddAttrs&gt; &#123;</span><br><span class="line">  <span class="type">int</span> axis;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(BiasAddAttrs, <span class="string">&quot;relay.attrs.BiasAddAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to add the bias&quot;</span>).<span class="built_in">set_default</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="类型推导-Type-Relation"><a href="#类型推导-Type-Relation" class="headerlink" title="类型推导 Type Relation"></a>类型推导 Type Relation</h2><p>为了算子注册的灵活性以及relay算子有更好的泛化能力，relay算子通过输入输出之间的类型关系来实例化。<br>这些关系通过一系列的函数进行表示（这些函数是以算子输入输出类型为参数，返回满足类型关系的输入输出列表）， 、、？<br>这包括编译期已知的输入输出的shape 信息<br>本质上，算子relation除了推到输出类型外，还能够强制指定类型规则（检查输入类型）。</p><p>然后就是官网教程的给的例子<code>src/relay/op/tensor/transform.cc</code>。这里依旧是<code>ScanopAttrs</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_NODE_TYPE</span>(ScanopAttrs);</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ScanopRel</span><span class="params">(<span class="type">const</span> Array&lt;Type&gt;&amp; types, <span class="type">int</span> num_inputs, <span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> TypeReporter&amp; reporter)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// types: [data, output]</span></span><br><span class="line">    <span class="built_in">ICHECK_EQ</span>(types.<span class="built_in">size</span>(), <span class="number">2</span>) &lt;&lt; <span class="string">&quot;Expects two types, one for the input and another for the output&quot;</span>;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* data = types[<span class="number">0</span>].<span class="built_in">as</span>&lt;TensorTypeNode&gt;(); <span class="comment">//输入的tensor信息</span></span><br><span class="line">    <span class="keyword">if</span> (data == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="built_in">ICHECK</span>(types[<span class="number">0</span>].<span class="built_in">as</span>&lt;IncompleteTypeNode&gt;())</span><br><span class="line">        &lt;&lt; <span class="string">&quot;Scanop: expect input type to be TensorType but get &quot;</span> &lt;&lt; types[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* param = attrs.<span class="built_in">as</span>&lt;ScanopAttrs&gt;(); <span class="comment">//算子属性</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> dtype = param-&gt;dtype;</span><br><span class="line">    <span class="keyword">if</span> (dtype.<span class="built_in">is_void</span>()) &#123;</span><br><span class="line">        dtype = data-&gt;dtype;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//设置输出tensor属性</span></span><br><span class="line">    <span class="keyword">if</span> (param-&gt;axis.<span class="built_in">defined</span>()) &#123;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(data-&gt;shape, dtype));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">auto</span> prod = data-&gt;shape[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">1</span>; i &lt; data-&gt;shape.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            prod = prod * data-&gt;shape[i];</span><br><span class="line">        &#125;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(&#123;prod&#125;, dtype));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的例子可以看出 XXXOpRel 的主要功能是根据输入类型确定输出类型。特别的， <code>TensorType</code>的构造函数可以看出，需要指定输出的shape信息，这部分主要目的就是infershape和infertype。</p><h2 id="关联算子的参数数目、属性"><a href="#关联算子的参数数目、属性" class="headerlink" title="关联算子的参数数目、属性"></a>关联算子的参数数目、属性</h2><p>这一步的操作，为自定义算子注册算子名称，通过调用接口增加算子注释。这里需要用到C++的宏<code>RELAY_REGISTER_OP</code><br>涉及的参数含义如下：</p><ul><li>Arity（参数数量）</li><li>位置参数的名称和描述</li><li>支持级别（1 表示内部实现;较高的数字表示较少的内部支持或外部支持的算子）</li><li>算子的类型关系</li><li>优化算子时有用的其他注释。<br><code>src/relay/op/tensor/transform.cc</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumsum&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative sum of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumsum&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);</span><br><span class="line"></span><br><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumprod&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative product of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumprod&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);<span class="comment">// 不融合</span></span><br></pre></td></tr></table></figure><p>注：<code>set_attr&lt;TOpPattern&gt;(&quot;TOpPattern&quot;, );</code>此处表示融合算子是，跳过此算子。</p><h2 id="编写的算子compute"><a href="#编写的算子compute" class="headerlink" title="编写的算子compute"></a>编写的算子compute</h2><p>到现在，我们已经实现了算子的接口，但是还缺少算子的compute逻辑。这部分内容超出了这个教程的范围。<br>对于<code>cumprod</code>和<code>cumsum</code>，CPU实现可以参考<code>python/tvm/topi/scan.py</code>，GPU实现可以参考<code>python/tvm/topi/cuda/scan.py</code>。<br>这里这两个的实现，直接在TIR基础上实现得到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scanop</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    binop: <span class="type">Callable</span>[[<span class="string">&quot;tvm.Expr&quot;</span>, <span class="string">&quot;tvm.Expr&quot;</span>], <span class="string">&quot;tvm.Expr&quot;</span>],</span></span><br><span class="line"><span class="params">    identity_value: <span class="string">&quot;tvm.Expr&quot;</span>,</span></span><br><span class="line"><span class="params">    op_name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> dtype == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        dtype = data.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exclusive <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        exclusive = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maybe_cast</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">if</span> dtype != data.dtype:</span><br><span class="line">            <span class="keyword">return</span> cast(x, dtype)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    axis_mul_before = <span class="number">1</span></span><br><span class="line">    axis_mul_after = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> axis <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        axis = <span class="number">0</span></span><br><span class="line">        cumsum_axis_len = prod(data.shape)</span><br><span class="line">        shape = (cumsum_axis_len,)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(axis, <span class="built_in">int</span>):</span><br><span class="line">            axis = get_const_int(axis)</span><br><span class="line"></span><br><span class="line">        shape = data.shape</span><br><span class="line">        cumsum_axis_len = shape[axis]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> axis &lt; <span class="number">0</span>:</span><br><span class="line">            axis = <span class="built_in">len</span>(shape) + axis</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(shape, <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; axis:</span><br><span class="line">                axis_mul_before *= value</span><br><span class="line">            <span class="keyword">elif</span> i &gt; axis:</span><br><span class="line">                axis_mul_after *= value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_ir</span>(<span class="params">data_buf, out_buf</span>):</span><br><span class="line">        ib = ir_builder.create()</span><br><span class="line">        data_buf = ib.buffer_ptr(data_buf)</span><br><span class="line">        out_buf = ib.buffer_ptr(out_buf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, axis_mul_before * axis_mul_after, <span class="string">&quot;fused&quot;</span>, kind=<span class="string">&quot;parallel&quot;</span>) <span class="keyword">as</span> fused:</span><br><span class="line">            i = fused // axis_mul_after</span><br><span class="line">            j = fused % axis_mul_after</span><br><span class="line">            base_idx = i * cumsum_axis_len * axis_mul_after + j</span><br><span class="line">            <span class="keyword">if</span> exclusive:</span><br><span class="line">                out_buf[base_idx] = cast(identity_value, dtype)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_buf[base_idx] = maybe_cast(data_buf[base_idx])</span><br><span class="line">            <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, cumsum_axis_len - <span class="number">1</span>, <span class="string">&quot;_k&quot;</span>) <span class="keyword">as</span> _k:</span><br><span class="line">                k = _k + <span class="number">1</span></span><br><span class="line">                cur_idx = base_idx + k * axis_mul_after</span><br><span class="line">                prev_idx = base_idx + (k - <span class="number">1</span>) * axis_mul_after</span><br><span class="line">                <span class="keyword">if</span> exclusive:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[prev_idx]))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[cur_idx]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ib.get()</span><br><span class="line"></span><br><span class="line">    out_buf = decl_buffer(shape, dtype, <span class="string">&quot;out_buf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> extern(</span><br><span class="line">        [shape],</span><br><span class="line">        [data],</span><br><span class="line">        <span class="keyword">lambda</span> ins, outs: gen_ir(ins[<span class="number">0</span>], outs[<span class="number">0</span>]),</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        out_buffers=[out_buf],</span><br><span class="line">        name=op_name,</span><br><span class="line">        tag=op_name,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">    <span class="keyword">return</span> scanop(</span><br><span class="line">        data=data,</span><br><span class="line">        binop=generic.add,</span><br><span class="line">        identity_value=<span class="number">0</span>,</span><br><span class="line">        op_name=<span class="string">&quot;cumsum_generic&quot;</span>,</span><br><span class="line">        axis=axis,</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        exclusive=exclusive,</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="注册算子的compute、schedule"><a href="#注册算子的compute、schedule" class="headerlink" title="注册算子的compute、schedule"></a>注册算子的compute、schedule</h2><p>在实现了算子compute逻辑以后，需要与我们实现的算子接口绑定在一起。在TVM中，这就需要不仅实现算子的compute接口，还要实现对应的schedule。而strategy就是对compute选择合适的schedule。<br>以卷积算子为例，算子编译时，可能会发现这是一个depthwise卷积，进而去选择更高效的schedule实现。</p><p>一般情况下，仅仅考虑CPU、GPU版本即可。<br><code>python/tvm/relay/op/strategy/generic.py</code> <code>python/tvm/relay/op/strategy/cuda.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wrap_compute_scanop</span>(<span class="params">topi_compute</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Wrap scanop style topi compute&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_scanop</span>(<span class="params">attrs, inputs, _</span>):</span><br><span class="line">        <span class="keyword">return</span> [topi_compute(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line">    <span class="keyword">return</span> _compute_scanop</span><br><span class="line"></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="meta">@cumsum_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum cuda strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cuda.cumsum),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_scan),</span><br><span class="line">        name=<span class="string">&quot;cumsum.cuda&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>对于每个strategy，与对应的compute、schedule通过<code>add_implementation</code>关联起来。<br>这里的shape_func时对输入时动态shape厂家推导有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cumsum</span></span><br><span class="line"><span class="meta">@_reg.register_compute(<span class="params"><span class="string">&quot;cumsum&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cumsum</span>(<span class="params">attrs, inputs, output_type</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute definition of cumsum&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [topi.cumsum(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line"></span><br><span class="line">_reg.register_strategy(<span class="string">&quot;cumsum&quot;</span>, strategy.cumsum_strategy)</span><br><span class="line">_reg.register_shape_func(<span class="string">&quot;cumsum&quot;</span>, <span class="literal">False</span>, elemwise_shape_func)</span><br></pre></td></tr></table></figure><h2 id="定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook"><a href="#定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook" class="headerlink" title="定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook"></a>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</h2><p>现在我们有一个可以调用的relay算子了，下一步就是如何通过relay call node调用。这就需要实现一个函数，传递相应的参数给对于的relay算子，并且返回对应算子的Call Node（这个算子最终在Relay表达式的AST里面）。</p><p>当前不支持直接调用 Attrs和参数。所以需要在函数中构造对应的AttrsNode，传递给对应的Call Node。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Expr <span class="title">MakeCumsum</span><span class="params">(Expr data, Integer axis, DataType dtype, Bool exclusive)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;ScanopAttrs&gt;();</span><br><span class="line">    attrs-&gt;dtype = dtype;</span><br><span class="line">    attrs-&gt;axis = axis;</span><br><span class="line">    attrs-&gt;exclusive = exclusive;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(<span class="string">&quot;cumsum&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op._make.cumsum&quot;</span>).<span class="built_in">set_body_typed</span>(MakeCumsum);</span><br></pre></td></tr></table></figure><p><code>Op::Get(&quot;cumsum&quot;)</code>的实现如下。具体怎么注册到<code>OpRegistry</code>的，TODO</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里看一下Call的实现，实际上是得到一个call Node，里面保存了算子及其属性信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Call::<span class="built_in">Call</span>(Expr op, Array&lt;Expr&gt; args, Attrs attrs, Array&lt;Type&gt; type_args, Span span) &#123;</span><br><span class="line">  ObjectPtr&lt;CallNode&gt; n = <span class="built_in">make_object</span>&lt;CallNode&gt;();</span><br><span class="line">  n-&gt;op = std::<span class="built_in">move</span>(op);</span><br><span class="line">  n-&gt;args = std::<span class="built_in">move</span>(args);</span><br><span class="line">  n-&gt;attrs = std::<span class="built_in">move</span>(attrs);</span><br><span class="line">  n-&gt;type_args = std::<span class="built_in">move</span>(type_args);</span><br><span class="line">  n-&gt;span = std::<span class="built_in">move</span>(span);</span><br><span class="line">  data_ = std::<span class="built_in">move</span>(n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Op::Get</code> <code>src/relay/op/tensor/transform.cc</code></p><p>相关接口暴露到python侧，是通过<code>.TVM_REGISTER_GLOBAL</code> <code>MakeCumsum</code> <code>MakeCumprod</code> <code>relay.op._make.cumsum(...)</code> <code>relay.op._make.cumsum(...)</code>实现的。</p><p>细节TODO</p><h2 id="将上面的-Python-API-hook-封装成简洁的调用方式"><a href="#将上面的-Python-API-hook-封装成简洁的调用方式" class="headerlink" title="将上面的 Python API hook 封装成简洁的调用方式"></a>将上面的 Python API hook 封装成简洁的调用方式</h2><p>为更方便的使用，通常的做法是构造单独的函数，因此最好封装成更简洁的python接口。教程的例子，定义在<br><code>TVM_REGISTER_GLOBAL</code> <code>python/tvm/relay/op/transform.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumsum(data, axis, dtype, exclusive)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumprod(data, axis, dtype, exclusive)</span><br></pre></td></tr></table></figure><p>特别的，如果不定参数的，需要包成Tuple形式进行传递。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Concatenate the input tensors along the zero axis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    args: list of Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    tensor: The concatenated tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tup = <span class="type">Tuple</span>(<span class="built_in">list</span>(args))</span><br><span class="line">    <span class="keyword">return</span> _make.concat(tup)</span><br></pre></td></tr></table></figure><h2 id="为新的relay-算子编写测试"><a href="#为新的relay-算子编写测试" class="headerlink" title="为新的relay 算子编写测试"></a>为新的relay 算子编写测试</h2><p>参考 <code>tests/python/relay/test_op_level3.py</code></p><p>ref: <a href="https://tvm.apache.org/docs/dev/relay_add_op.html">https://tvm.apache.org/docs/dev/relay_add_op.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】2. relay算子构造</title>
      <link href="/tvm-relay-op-construct/"/>
      <url>/tvm-relay-op-construct/</url>
      
        <content type="html"><![CDATA[<p>从TVM的官方<a href="https://www.cnblogs.com/wanger-sjtu/p/15046641.html">Tutorial</a>里面，介绍了如何新增自定义算子。(这是我翻译的)</p><p>之前的文章讲到了<a href="../tvm-onnx-to-relay">onnx 算子转换到Relay IR</a>的过程<br>下面以Conv2d算子介绍，编译过程中 Relay IR是如何被调用的。</p><h2 id="relay-算子调用"><a href="#relay-算子调用" class="headerlink" title="relay 算子调用"></a>relay 算子调用</h2><p>上面的<code>get_relay_op</code>实际上是查找所有 relay ir算子，其代码在<code>python/tvm/relay/frontend/common.py</code>中的<code>get_relay_op</code>。继续以conv卷积算子为例介绍。上文所述的转换算子中，有下面的语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate <span class="keyword">in</span> (_op, _op.nn, _op.image, _op.vision, _op.contrib):</span><br><span class="line">    op = <span class="built_in">getattr</span>(candidate, op_name, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> op <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>对于<code>conv2d</code>算子，在<code>_op.nn</code>中，找到conv2d实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data,</span></span><br><span class="line"><span class="params">    weight,</span></span><br><span class="line"><span class="params">    strides=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    padding=(<span class="params"><span class="number">0</span>, <span class="number">0</span></span>),</span></span><br><span class="line"><span class="params">    dilation=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    kernel_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data_layout=<span class="string">&quot;NCHW&quot;</span>,</span></span><br><span class="line"><span class="params">    kernel_layout=<span class="string">&quot;OIHW&quot;</span>,</span></span><br><span class="line"><span class="params">    out_layout=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params">    out_dtype=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(kernel_size, <span class="built_in">int</span>):</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(strides, <span class="built_in">int</span>):</span><br><span class="line">        strides = (strides, strides)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dilation, <span class="built_in">int</span>):</span><br><span class="line">        dilation = (dilation, dilation)</span><br><span class="line">    padding = get_pad_tuple2d(padding)</span><br><span class="line">    <span class="keyword">return</span> _make.conv2d( data, weight, strides, padding, dilation, groups, channels, kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>这里的<code>_make.conv2d</code>是通过下面的PackFunc注册得到的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tvm._ffi._init_api(<span class="string">&quot;relay.op.nn._make&quot;</span>, __name__)</span><br></pre></td></tr></table></figure><p>在<code>src/relay/op/nn/convolution.cc</code>找到conv2d的注册函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op.nn._make.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span><br><span class="line">                       Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span><br><span class="line">                       Array&lt;IndexExpr&gt; kernel_size, String data_layout, String kernel_layout,</span><br><span class="line">                       String out_layout, DataType out_dtype) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">MakeConv</span>&lt;Conv2DAttrs&gt;(data, weight, strides, padding, dilation, groups, channels,</span><br><span class="line">                                   kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">                                   <span class="string">&quot;nn.conv2d&quot;</span>);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>MakeConv 是对所有卷积的模板，根据参数实例化相应的函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> Expr <span class="title">MakeConv</span><span class="params">(Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; kernel_size, std::string data_layout,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string kernel_layout, std::string out_layout, DataType out_dtype,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string op_name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;T&gt;();</span><br><span class="line">  attrs-&gt;strides = std::<span class="built_in">move</span>(strides);</span><br><span class="line">  attrs-&gt;padding = std::<span class="built_in">move</span>(padding);</span><br><span class="line">  attrs-&gt;dilation = std::<span class="built_in">move</span>(dilation);</span><br><span class="line">  attrs-&gt;groups = groups;</span><br><span class="line">  attrs-&gt;channels = std::<span class="built_in">move</span>(channels);</span><br><span class="line">  attrs-&gt;kernel_size = std::<span class="built_in">move</span>(kernel_size);</span><br><span class="line">  attrs-&gt;data_layout = std::<span class="built_in">move</span>(data_layout);</span><br><span class="line">  attrs-&gt;kernel_layout = std::<span class="built_in">move</span>(kernel_layout);</span><br><span class="line">  attrs-&gt;out_layout = std::<span class="built_in">move</span>(out_layout);</span><br><span class="line">  attrs-&gt;out_dtype = std::<span class="built_in">move</span>(out_dtype);</span><br><span class="line">  <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(op_name);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data, weight&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里通过<code>Op::Get(op_name);</code> 获取对应relay算子，在<code>Op::Get</code>函数中发现是通过查表得到。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// find operator by name</span></span><br><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注册是通过C++的<code>RELAY_REGISTER_OP(&quot;nn.conv2d&quot;)</code>宏注册到<code>OpRegistry::Global()</code>中。宏展开为</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> __attribute__((unused))::tvm::OpRegEntry&amp; __make_Op230 =</span><br><span class="line">    ::tvm::OpRegEntry::<span class="built_in">RegisterOrGet</span>(<span class="string">&quot;nn.conv2d&quot;</span>).<span class="built_in">set_name</span>()</span><br></pre></td></tr></table></figure><p>注册过程：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;nn.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(<span class="string">R&quot;code(2D convolution layer (e.g. spatial convolution over images).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This layer creates a convolution kernel that is convolved</span></span><br><span class="line"><span class="string">with the layer input to produce a tensor of outputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- **data**: This depends on the `layout` parameter. Input is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, in_channels, height, width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string">- **weight**: (channels, in_channels, kernel_size[0], kernel_size[1])</span></span><br><span class="line"><span class="string">- **out**:  This depends on the `layout` parameter. Output is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">)code&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_attrs_type</span>&lt;Conv2DAttrs&gt;()</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;weight&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The weight tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Conv2D&quot;</span>, Conv2DRel&lt;Conv2DAttrs&gt;)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;FInferCorrectLayout&gt;(<span class="string">&quot;FInferCorrectLayout&quot;</span>, ConvInferCorrectLayout&lt;Conv2DAttrs&gt;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>返回的是<code>OpRegEntry</code>，后续的<code>set_name</code>等，则是通过<code>OpRegEntry</code>的get接口（返回的是OpNode），构造对应的Relay op</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【tvm解析】3. Operator Strategy 机制</title>
      <link href="/tvm-op-strategy/"/>
      <url>/tvm-op-strategy/</url>
      
        <content type="html"><![CDATA[<p>Relay Operator Strategy是建立Relay IR与TOPI算子库的桥梁，通过Relay Operator Strategy，每个Relay IR至少与一个compute和一个schedule注册关联起来。至少一个原因在于，一个算子在不同后端设备上有不同的实现，而且一个算子可能有多种计算算法，适应不同场景。</p><p>在增加relay IR 的教程里面注册算子的compute、schedule中，就是通过<code>OpStrategy</code>关联算子的compute与schedule</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-Design"><a href="#Operator-Strategy-Design" class="headerlink" title="Operator Strategy Design"></a>Operator Strategy Design</h2><p><code>OpStrategy</code>的核心为<code>OpImplementation</code>，包含了一组compute及对应的schedule，不同实现的名字，选择优先级（参见下文的选择策略）。</p><p>OpStrategy中包含一系列的<code>OpSpecialization</code>，每个<code>OpSpecialization</code>包含一组<code>SpecializedCondition</code>（参考<code>include/tvm/te/schedule.h</code>）. 如果<code>SpecializedCondition</code>为空（null），表示是一个通用的实现，反之则是对于特定情形优化的。<code>SpecializedCondition</code>包含了这一算子的多个TE实现，以及实现被调用的条件。</p><p>最后一点，对给定的workload，一个strategy 函数或者<code>FTVMStrategy</code>,决定了使用哪个compute和schedule，因此这部分需要与relay算子对应起来。<br><code>FTVMStrategy </code>实现位置在<code>include/tvm/target/generic_func.h</code>,是一个通用函数，对于给定硬件平台可以重写。函数签名是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">OpStrategy</span>(<span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> Array&lt;Tensor&gt;&amp; inputs, <span class="type">const</span> Type&amp; out_type, <span class="type">const</span> Target&amp; target)</span><br></pre></td></tr></table></figure><p>对给定算子属性信息、输入、输出类型以及平台设备，这个函数返回相应的<code>OpStrategy</code>.</p><h2 id="手写一个-Strategy-函数"><a href="#手写一个-Strategy-函数" class="headerlink" title="手写一个 Strategy 函数"></a>手写一个 Strategy 函数</h2><p>tvm 推荐在python侧来写Strategy 函数，在python侧提供了OpStrategy类，其中包含一个add_implementation方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_object(<span class="params"><span class="string">&quot;relay.OpStrategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OpStrategy</span>(<span class="title class_ inherited__">Object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator strategy&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__init_handle_by_constructor__(_make.OpStrategy)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_implementation</span>(<span class="params">self, compute, schedule, name=<span class="string">&quot;default&quot;</span>, plevel=<span class="number">10</span></span>):</span><br><span class="line">        _OpStrategyAddImplementation(self, compute, schedule, name, plevel)</span><br></pre></td></tr></table></figure><p>后面以topk的算子为例，介绍了如何手写 Strategy 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;topk_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_topk(topi.topk),</span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.generic&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对GPU CUDA的</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@topk_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_my_new_op(topi.cuda.topk),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>为了满足Strategy 函数对于函数签名的要求（see <code>FTVMCompute</code> and <code>FTVMSchedule</code> in <code>include/tvm/relay/op_attr_types.h</code>），这里对topk的compute和schedule做了一层封装。由于算子属性不同，通常需要算子开发者自己写这部分的封装函数。</p><p>上面的例子比较简单，对于一个设备平台只有一个实现，但对一些其他的复杂算子来说，需要针对不同的算法来写相应的schedule，以卷积算子为例，可以直接写滑窗来计算，也可以使用winograd算法计算。这种情况下有多个implementation：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">strategy.add_implementation(</span><br><span class="line">    wrap_compute_conv2d(topi.cuda.conv2d_nchw),</span><br><span class="line">    wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw),</span><br><span class="line">    name=<span class="string">&quot;conv2d_nchw.cuda&quot;</span>,</span><br><span class="line">    plevel=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> winograd_condition:</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_conv2d(topi.cuda.conv2d_nchw_winograd),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw_winograd),</span><br><span class="line">        name=<span class="string">&quot;conv2d_nchw_winograd.cuda&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><p>可以看到这两个是优先级不同，在满足winograd算法的情况下，会优先选择winograd算法。这样也可以新增条件，新增implentation。<br>同样也可以对不同shape设置不同的优先级策略。下面的例子就是在<code>m &gt; 16</code>时，有额外的计算策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dense_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">  m = inputs[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">  strategy = _op.OpStrategy()</span><br><span class="line">  strategy.add_implementation(</span><br><span class="line">    wrap_compute_dense(dense_compute1),</span><br><span class="line">    wrap_topi_schedule(dense_schedule1),</span><br><span class="line">    name=<span class="string">&quot;dense_common&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tvm.te.SpecializedCondition(m &gt; <span class="number">16</span>):</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_dense(dense_compute2),</span><br><span class="line">        wrap_topi_schedule(dense_schedule2),</span><br><span class="line">        name=<span class="string">&quot;dense_for_large_m&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br><span class="line">  <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="将算子-Strategy-绑定到算子"><a href="#将算子-Strategy-绑定到算子" class="headerlink" title="将算子 Strategy 绑定到算子"></a>将算子 Strategy 绑定到算子</h2><p>定义了算子strategy函数以后，需要跟算子绑定在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_strategy(<span class="string">&quot;topk&quot;</span>, strategy.topk_strategy)</span><br></pre></td></tr></table></figure><p>然而，对于一个算子来说，写它的strategy函数是比较困难的，对简单算子来说，这里提供了两种方案。<br>第一个:算子是单射的、广播、reduce操作时候，可以通过 <code>register_injective_schedule</code>, <code>register_broadcast_schedule</code>、 <code>register_reduce_schedule</code>，这就避免自己手写schedule了。不过这种方式对于任意后端设备都是通用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_broadcast_schedule(<span class="string">&quot;add&quot;</span>)</span><br></pre></td></tr></table></figure><p>第二种：对于没有明确pattern的算子，可以用<code>register_schedule</code>实现对任意后端的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用兜底的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@generic_func</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    <span class="keyword">with</span> target:</span><br><span class="line">        <span class="keyword">return</span> topi.generic.schedule_pool(outs, attrs.layout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果特定target的，需要在对应的文件下增加</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@schedule_pool.register(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool_cpu</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">register_schedule(<span class="string">&quot;nn.max_pool2d&quot;</span>, strategy.schedule_pool)</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-选择"><a href="#Operator-Strategy-选择" class="headerlink" title="Operator Strategy 选择"></a>Operator Strategy 选择</h2><p>一个算子有多个Strategy的时候，选择策略是什么呢？</p><p>对于静态shape：首先会根据搜索时候的tune log选择最佳实现，如果tune log中没有或者已有auto TVM模板中有特定的实现，则会根据优先级选择对应的实现。如果多个实现具有相同优先级，选哪个就不确定了。</p><p>动态shape场景，则会选择高优先级的情况。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tvm-多线程代码生成和运行</title>
      <link href="/tvm-cpu-multi-thread/"/>
      <url>/tvm-cpu-multi-thread/</url>
      
        <content type="html"><![CDATA[<h3 id="调用链"><a href="#调用链" class="headerlink" title="调用链"></a>调用链</h3><p>tvm搜索算子在需要多线程运行的算子，是在codegen阶段时插入<code>TVMBackendParallelLaunch</code>的调用。<br><code>TVMBackendParallelLaunch</code> 是tvm的线程池并行化入口，具体如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief The callback function to execute a parallel lambda</span></span><br><span class="line"><span class="comment"> * \param task_id the task id of the function. //这里实际就是线程池线程编码，对应第几个线程</span></span><br><span class="line"><span class="comment"> * \param penv The parallel environment backs the execution. // num_task, sync</span></span><br><span class="line"><span class="comment"> * \param cdata The supporting closure data.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*FTVMParallelLambda)</span><span class="params">(<span class="type">int</span> task_id, TVMParallelGroupEnv* penv, <span class="type">void</span>* cdata)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Backend function for running parallel jobs.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \param flambda The parallel function to be launched.</span></span><br><span class="line"><span class="comment"> * \param cdata The closure data. // 可以认为时循环的变量 codegen时生成</span></span><br><span class="line"><span class="comment"> * \param num_task Number of tasks to launch, can be 0, means launch</span></span><br><span class="line"><span class="comment"> *           with all available threads. // codegen 时写入的是0，运行时根据配置写入</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \return 0 when no error is thrown, -1 when failure happens</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMBackendParallelLaunch</span><span class="params">(FTVMParallelLambda flambda, <span class="type">void</span>* cdata, <span class="type">int</span> num_task)</span></span>;</span><br></pre></td></tr></table></figure><p><code>flambda</code>的调用在单线程和多线程下略有区别。</p><p>单线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (num_workers == <span class="number">1</span>) &#123;</span><br><span class="line">    std::atomic&lt;<span class="type">int32_t</span>&gt; sync_counter&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    TVMParallelGroupEnv env;</span><br><span class="line">    env.num_task = <span class="number">1</span>;</span><br><span class="line">    env.sync_handle = &amp;sync_counter;</span><br><span class="line">    (*flambda)(<span class="number">0</span>, &amp;env, cdata);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>多线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// launcher-&gt;Init(flambda, cdata, num_task, need_sync != 0);</span></span><br><span class="line"><span class="keyword">this</span>-&gt;cdata = cdata;</span><br><span class="line"><span class="keyword">this</span>-&gt;flambda = flambda;</span><br><span class="line"><span class="keyword">this</span>-&gt;env.num_task = num_task;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (queue-&gt;<span class="built_in">Pop</span>(&amp;task, spin_count)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(task.launcher != <span class="literal">nullptr</span>);</span><br><span class="line">    TVMParallelGroupEnv* penv = &amp;(task.launcher-&gt;env);</span><br><span class="line">    <span class="type">void</span>* cdata = task.launcher-&gt;cdata;</span><br><span class="line">    <span class="keyword">if</span> ((*task.launcher-&gt;flambda)(task.task_id, penv, cdata) == <span class="number">0</span>) &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobFinish</span>();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobError</span>(task.task_id);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>可以看到 待并行函数中 <code>TVMParallelGroupEnv* penv</code> 包含了实际的运行时线程，运行时可以根据这个确定每个线程的工作区间和步长。<br><code>cdata</code>则是线程运行时需要变量信息，闭包变量。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>对要并行的函数，实际上是按照<code>lambda</code>表达式的方式生成的。<code>FTVMParallelLambda</code> 的输入参数前两个是运行时确定的，第三个是捕获的外部变量。</p><h2 id="codegen-过程"><a href="#codegen-过程" class="headerlink" title="codegen 过程"></a>codegen 过程</h2><p>下面验证一下上述的猜测。</p><p>codegen过程中，实际上是在遍历<code>tir Stmt</code>的AST，因为生成的循环都是基于For的，调用过程也比较简单了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span>  <span class="comment">// -&gt; </span></span></span><br><span class="line"><span class="function"><span class="title">CreateParallelLaunch</span><span class="params">(For(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span></span></span><br><span class="line"><span class="params"><span class="function">                        op-&gt;thread_binding, op-&gt;annotations),</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="number">0</span>, std::string(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.c_str())</span></span>;   <span class="comment">// -&gt;</span></span><br><span class="line">CodeGenCPU::<span class="built_in">VisitStmt_</span>(<span class="type">const</span> ForNode* op);</span><br></pre></td></tr></table></figure><p>当遍历到For节点时， 根据属性判断是否并行加速。这里只分析加速场景。此时<code>parallel_env_.penv == nullptr</code> 创建多线程调用函数，进入<code>CreateParallelLaunch</code>函数。<br>然后 再生成 For的遍历逻辑。<code>this-&gt;VisitStmt(body);</code> 这里的<code>body</code>其实还是<code>For</code> ，这时候就进入 </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br></pre></td></tr></table></figure><p>前文的猜测也在这里得到验证。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">  <span class="keyword">if</span> (op-&gt;kind == ForKind::kSerial || op-&gt;kind == ForKind::kUnrolled) &#123;</span><br><span class="line">    CodeGenLLVM::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (op-&gt;kind == ForKind::kParallel) &#123;</span><br><span class="line">    <span class="keyword">if</span> (parallel_env_.penv == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="built_in">CreateParallelLaunch</span>(<span class="built_in">For</span>(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span><br><span class="line">                               op-&gt;thread_binding, op-&gt;annotations),</span><br><span class="line">                           <span class="number">0</span>, std::<span class="built_in">string</span>(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.<span class="built_in">c_str</span>());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.task_id.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.num_task.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.penv != <span class="literal">nullptr</span>);</span><br><span class="line">      DataType t = op-&gt;extent.<span class="built_in">dtype</span>();</span><br><span class="line">      PrimExpr num_task = <span class="built_in">cast</span>(t, parallel_env_.num_task);</span><br><span class="line">      PrimExpr task_id = <span class="built_in">cast</span>(t, parallel_env_.task_id);</span><br><span class="line">      <span class="built_in">ICHECK</span>(!parallel_env_.in_parallel_loop)</span><br><span class="line">          &lt;&lt; <span class="string">&quot;Nested parallel loop is not supported by threadpool, try fuse them instead&quot;</span>;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">if</span> (parallel_env_.stride_pattern) &#123;</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(task_id), <span class="built_in">MakeValue</span>(op-&gt;extent), <span class="built_in">MakeValue</span>(num_task),</span><br><span class="line">                        op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        PrimExpr step = (op-&gt;extent + num_task - <span class="built_in">make_const</span>(t, <span class="number">1</span>)) / num_task;</span><br><span class="line">        PrimExpr begin = <span class="built_in">min</span>(task_id * step, op-&gt;extent);</span><br><span class="line">        PrimExpr end = <span class="built_in">min</span>((task_id + <span class="built_in">make_const</span>(t, <span class="number">1</span>)) * step, op-&gt;extent);</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(begin), <span class="built_in">MakeValue</span>(end),</span><br><span class="line">                        llvm::ConstantInt::<span class="built_in">getSigned</span>(<span class="built_in">GetLLVMType</span>(end), <span class="number">1</span>), op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">false</span>;</span><br><span class="line">      ++parallel_env_.parallel_loop_count;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;cannot handle for type &quot;</span> &lt;&lt; op-&gt;kind;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    const Stmt&amp; body  For 循环的statement</span></span><br><span class="line"><span class="comment">    int num_task, 这里设置的是0，根据运行时参数确定使用线程</span></span><br><span class="line"><span class="comment">    std::string name</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::CreateParallelLaunch</span><span class="params">(<span class="type">const</span> Stmt&amp; body, <span class="type">int</span> num_task, std::string name)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// closure data</span></span><br><span class="line">  llvm::Function* f =</span><br><span class="line">      llvm::Function::<span class="built_in">Create</span>(ftype_tvm_parallel_lambda_, llvm::Function::PrivateLinkage,</span><br><span class="line">                             <span class="string">&quot;__tvm_parallel_lambda&quot;</span>, module_.<span class="built_in">get</span>());</span><br><span class="line">  <span class="built_in">SetTargetAttributes</span>(f);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate and setup the closure, call the closure. //For 循环内部变量。这里需要声明一下</span></span><br><span class="line">  Array&lt;Var&gt; vfields = tir::<span class="built_in">UndefinedVars</span>(body, &#123;&#125;);</span><br><span class="line">  <span class="type">uint64_t</span> nbytes;</span><br><span class="line">  TypedPointer cdata = <span class="built_in">PackClosureData</span>(vfields, &amp;nbytes, <span class="string">&quot;closure_&quot;</span> + name); <span class="comment">// 可以认为时循环的变量</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> TVM_LLVM_VERSION &gt;= 90</span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = llvm::<span class="built_in">FunctionCallee</span>(ftype_tvm_parallel_launch_, <span class="built_in">RuntimeTVMParallelLaunch</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = <span class="built_in">RuntimeTVMParallelLaunch</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  llvm::BasicBlock* par_launch_end = <span class="built_in">CheckCallSuccess</span>(builder_-&gt;<span class="built_in">CreateCall</span>(</span><br><span class="line">      launch_callee,</span><br><span class="line">      &#123;f, builder_-&gt;<span class="built_in">CreatePointerCast</span>(cdata.addr, t_void_p_), <span class="built_in">ConstInt32</span>(num_task)&#125;));</span><br><span class="line">  <span class="comment">// Setup the closure function.</span></span><br><span class="line">  <span class="keyword">auto</span>* lambda_entry =</span><br><span class="line">      llvm::BasicBlock::<span class="built_in">Create</span>(*llvm_target_-&gt;<span class="built_in">GetContext</span>(), <span class="string">&quot;parallel_closure_entry&quot;</span>, f);</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(lambda_entry);</span><br><span class="line">  <span class="keyword">auto</span> it = f-&gt;<span class="built_in">arg_begin</span>();</span><br><span class="line">  llvm::Value* task_id = &amp;(*it++);</span><br><span class="line">  task_id-&gt;<span class="built_in">setName</span>(<span class="string">&quot;task_id&quot;</span>);</span><br><span class="line">  llvm::Value* penv = &amp;(*it++);</span><br><span class="line">  cdata.addr = builder_-&gt;<span class="built_in">CreatePointerCast</span>(&amp;(*it++), cdata.addr-&gt;<span class="built_in">getType</span>());</span><br><span class="line">  <span class="comment">// setup new variable map, swap it with current var context.</span></span><br><span class="line">  std::unordered_map&lt;<span class="type">const</span> VarNode*, llvm::Value*&gt; new_vmap;</span><br><span class="line">  <span class="built_in">UnpackClosureData</span>(cdata, vfields, &amp;new_vmap);</span><br><span class="line">  <span class="comment">// setup parallel env</span></span><br><span class="line">  ParallelEnv par_env;</span><br><span class="line">  par_env.task_id = <span class="built_in">Var</span>(<span class="string">&quot;task_id&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  par_env.num_task = <span class="built_in">Var</span>(<span class="string">&quot;num_task&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  new_vmap[par_env.task_id.<span class="built_in">get</span>()] = task_id;</span><br><span class="line">  new_vmap[par_env.num_task.<span class="built_in">get</span>()] = builder_-&gt;<span class="built_in">CreateLoad</span>(</span><br><span class="line">      t_int32_,</span><br><span class="line">      builder_-&gt;<span class="built_in">CreateInBoundsGEP</span>(t_tvm_parallel_group_env_, penv, &#123;<span class="built_in">ConstInt32</span>(<span class="number">0</span>), <span class="built_in">ConstInt32</span>(<span class="number">1</span>)&#125;),</span><br><span class="line">      <span class="string">&quot;num_task&quot;</span>);</span><br><span class="line">  par_env.penv = penv;</span><br><span class="line">  <span class="keyword">auto</span> new_analyzer = std::<span class="built_in">make_unique</span>&lt;arith::Analyzer&gt;();</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  <span class="keyword">this</span>-&gt;<span class="built_in">VisitStmt</span>(body);</span><br><span class="line">  builder_-&gt;<span class="built_in">CreateRet</span>(<span class="built_in">ConstInt32</span>(<span class="number">0</span>));</span><br><span class="line">  <span class="comment">// swap the var map back, now we are back on track.</span></span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  <span class="built_in">ICHECK_NE</span>(par_env.parallel_loop_count, <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Cannot find parallel loop within parallel launch&quot;</span>;</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(par_launch_end);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++&#39;s most vexing parse</title>
      <link href="/most-vexing-parse/"/>
      <url>/most-vexing-parse/</url>
      
        <content type="html"><![CDATA[<p>C++’s most vexing parse 是 Scott Meyers 在其名著《Effective STL》中创造的一个术语。Scott 用这个术语来形容 C++ 标准对于 declaration 语句的消歧义（ambiguity resolution）约定与常人的认知相悖。</p><p><strong>最令人烦恼的解析</strong> （<strong>most vexing parse</strong>）是C++中的一种反直觉的二义性解析形式。 在一些场景下，编译器无法区分某语句是初始化时某对象的参数，还是声明一个函数时指定参数类型。在这些情况下，编译器将该行解释为函数声明。</p><p>形如 <code>Type()</code> 或 <code>Type(name)</code> 的表达在某些情况下具有歧义（syntax ambiguity）。</p><h3 id="C风格强制类型转换"><a href="#C风格强制类型转换" class="headerlink" title="C风格强制类型转换"></a>C风格强制类型转换</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">double</span> my_dbl)</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span>(my_dbl))</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的第 2 行是有歧义的。一种可能的解释是声明一个变量<code>i</code>，初始值通过转换<code>my_dbl</code> 到一个<code>int</code>而来。但是，<code>C</code> 允许在函数参数声明周围使用多余的括号；因此，声明的i实际上等同于以下代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A function named i takes an integer and returns an integer.</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span> my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><h3 id="未命名的临时对象"><a href="#未命名的临时对象" class="headerlink" title="未命名的临时对象"></a>未命名的临时对象</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Timer</span> &#123;&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">TimeKeeper</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">TimeKeeper</span><span class="params">(Timer t)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">get_time</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br><span class="line">  <span class="keyword">return</span> time_keeper.<span class="built_in">get_time</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br></pre></td></tr></table></figure><p>是有歧义的，它可以被解释为：</p><ol><li>一个变量：定义为类<code>TimeKeeper</code>的变量<code>time_keeper</code>，用类<code>Timer</code>的匿名实例初始化。</li><li>一个函数声明：声明了一个函数<code>time_keeper</code>，返回一个<code>TimeKeeper</code>，有一个（未命名的）参数。参数的类型是一个（指向）不接受输入并返回<code>Timer</code>对象的函数（的指针）。</li></ol><p>[C ++标准]采取第二种解释，这与上面的第9行不一致。例如，<code>Clang++</code>警告第9行存在最令人烦恼的解析，并报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">clang++ time_keeper.cc</span></span><br><span class="line">**timekeeper.cc:9:25: warning: parentheses were disambiguated as a function declaration**</span><br><span class="line">      **[-Wvexing-parse]**</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                        **^~~~~~~~~**</span><br><span class="line">**timekeeper.cc:9:26: note:** add a pair of parentheses to declare a variable</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                         ^</span><br><span class="line">                         (      )</span><br><span class="line">**timekeeper.cc:10:21: error: member reference base type &#x27;TimeKeeper (Timer (*)())&#x27; is not a**</span><br><span class="line">      **structure or union**</span><br><span class="line">  return time_keeper.get_time();</span><br><span class="line">         **~~~~~~~~~~~^~~~~~~~~**</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>这些有歧义的声明往往不会被解析为程序员所期望的语句。C++ 中的函数类型通常隐藏在<code>typedef</code>之后，并且通常具有显式引用或指针限定符。要强制扭转解析的结果，<strong>常见做法是换一种不同的对象创建或转换语法</strong>。</p><p>在类型转换的示例中，有两种替代语法：“C 风格强制类型转换”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// declares a variable of type int</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">((<span class="type">int</span>)my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><p>或一个static_cast转换：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="keyword">static_cast</span>&lt;<span class="type">int</span>&gt;(my_dbl))</span></span>;</span><br></pre></td></tr></table></figure><p>在变量声明的示例中，首选方法（自 C++11 起）是统一（大括号）初始化。 这也允许完全省略类型名称：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Any of the following work:</span></span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer&#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;<span class="built_in">Timer</span>()&#125;;</span><br><span class="line">TimeKeeper time_keeper&#123;Timer&#123;&#125;&#125;;</span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(     &#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;     &#123;&#125;&#125;;</span><br></pre></td></tr></table></figure><p>在 C++11 之前，强制获得预期解释的常用手段是使用额外的括号或拷贝初始化：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">( <span class="comment">/*Avoid MVP*/</span> (Timer()))</span></span>; <span class="comment">// 增加一个括号</span></span><br><span class="line">TimeKeeper time_keeper = <span class="built_in">TimeKeeper</span>(<span class="built_in">Timer</span>());  <span class="comment">// c++ 17 拷贝运算可以被优化</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】1. onnx2relay</title>
      <link href="/tvm-onnx-to-relay/"/>
      <url>/tvm-onnx-to-relay/</url>
      
        <content type="html"><![CDATA[<p><a href="../tvm-onnx">上一篇</a>介绍了onnx模型在tvm中优化的总体流程。</p><p>在这一篇中，介绍onnx模型到relay模型的转换流程，主要涉及了以下几个方面：</p><ul><li>onnx算子到relay算子转换</li><li>relay算子实现</li></ul><p>这一篇介绍onnx算子到relay算子转换过程</p><h2 id="onnx算子到relay算子转换"><a href="#onnx算子到relay算子转换" class="headerlink" title="onnx算子到relay算子转换"></a>onnx算子到relay算子转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br></pre></td></tr></table></figure><p>这部分实现是在<code>python/tvm/relay/frontend/onnx.py</code>中。实现转换过程的核心在于<code>GraphProto</code>这个类。这个类中实现了读取onnx模型各个节点、输入输出，映射onnx算子到relay IR的过程。对外接口为<code>from_onnx</code>这个函数。其伪代码可以大致表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">from_onnx</span>(<span class="params">self, graph, opset, get_output_expr=<span class="literal">False</span></span>):</span><br><span class="line">    inputs, params = read_model_inputs(graph) <span class="comment"># 模型参数</span></span><br><span class="line">    nodes = read_model_node(graph) <span class="comment"># 模型节点、算子信息</span></span><br><span class="line">    convert_map = _get_convert_map(opset) <span class="comment"># 模型转换map</span></span><br><span class="line">    check_op_support(nodes, convert_map)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">        op = self._convert_operator(op_name, inputs, attr, opset)</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>从这里可以知道ONNX前端的每个算子转化与<code>_get_convert_map</code>有关。<br><code>_convert_operator</code>完成了算子转换过程。具体的<code>convert_map</code>包含了所有支持算子的转换函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_convert_operator</span>(<span class="params">self, op_name, inputs, attrs, opset</span>):</span><br><span class="line">    convert_map = _get_convert_map(opset)</span><br><span class="line">    <span class="keyword">if</span> op_name <span class="keyword">in</span> _identity_list: <span class="comment"># 对onnx这里是空的</span></span><br><span class="line">        sym = get_relay_op(op_name)(*inputs, **attrs)</span><br><span class="line">    <span class="keyword">elif</span> op_name <span class="keyword">in</span> convert_map:</span><br><span class="line">        sym = convert_map[op_name](inputs, attrs, self._params)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Operator &#123;&#125; not implemented.&quot;</span>.<span class="built_in">format</span>(op_name))</span><br><span class="line">    <span class="keyword">return</span> sym</span><br></pre></td></tr></table></figure><p>以卷积算子为例，介绍具体的转换过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Conv&quot;</span>: Conv.get_converter(opset)</span><br></pre></td></tr></table></figure><p>Conv算子的实际转换操作来自于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(<span class="title class_ inherited__">OnnxOpConverter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator converter for Conv.&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_impl_v1</span>(<span class="params">cls, inputs, attr, params</span>):</span><br><span class="line">        <span class="comment"># Use shape of input to determine convolution type.</span></span><br><span class="line">        data = inputs[<span class="number">0</span>]</span><br><span class="line">        input_shape = infer_shape(data)</span><br><span class="line">        ndim = <span class="built_in">len</span>(input_shape)</span><br><span class="line">        <span class="comment"># auto_pad ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># construct op from attrs</span></span><br><span class="line">        out = AttrCvt(</span><br><span class="line">            op_name=dimension_picker(<span class="string">&quot;conv&quot;</span>),</span><br><span class="line">            transforms=&#123;</span><br><span class="line">                <span class="string">&quot;kernel_shape&quot;</span>: <span class="string">&quot;kernel_size&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dilations&quot;</span>: (<span class="string">&quot;dilation&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="string">&quot;pads&quot;</span>: (<span class="string">&quot;padding&quot;</span>, <span class="number">0</span>),</span><br><span class="line">                <span class="string">&quot;group&quot;</span>: (<span class="string">&quot;groups&quot;</span>, <span class="number">1</span>),</span><br><span class="line">            &#125;,</span><br><span class="line">            custom_check=dimension_constraint(),</span><br><span class="line">        )([data, inputs[<span class="number">1</span>]], attr, params)</span><br><span class="line"></span><br><span class="line">        use_bias = <span class="built_in">len</span>(inputs) == <span class="number">3</span></span><br><span class="line">        <span class="keyword">if</span> use_bias:</span><br><span class="line">            out = _op.nn.bias_add(out, inputs[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>这里通过<code>AttrCvt</code>类中构建相应的<code>relay</code>算子,<code>python/tvm/relay/frontend/common.py</code></p><p><code>AttrCvt</code>类包括两部分，<code>__init__</code> 和 <code>__call__</code>，前者根据收集初始化参数，后者完成Relay IR算子构建。</p><p><code>__call__</code>中的实现主要完成了算子属性读取、转换。根据转换后输入构建Relay IR</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> get_relay_op(op_name)(*inputs, **new_attrs)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】0.onnx模型优化流程.md</title>
      <link href="/tvm-onnx/"/>
      <url>/tvm-onnx/</url>
      
        <content type="html"><![CDATA[<p>本文以及后续文章，着重于介绍tvm的完整编译流程。<br>后续文章将会按照以上流程，介绍tvm源码。其中涉及一些编程技巧、以及tvm概念，不在此部分进行进一步讲解，另有文章进行介绍。</p><p>首先介绍一下，从onnx模型转为tvm模型的基本步骤。大致可以分为以下几步：</p><ol><li>onnx模型转到relay IR</li><li>基于Relay IR优化</li><li>导出优化模型</li><li>加载运行模型</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">onnx_model = onnx.load(model_path)</span><br><span class="line">target = <span class="string">&quot;llvm&quot;</span></span><br><span class="line">input_name = <span class="string">&quot;1&quot;</span></span><br><span class="line">shape_dict = &#123;input_name: x.shape&#125;</span><br><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br><span class="line"><span class="comment"># model build</span></span><br><span class="line"><span class="keyword">with</span> tvm.transform.PassContext(opt_level=<span class="number">3</span>):</span><br><span class="line">    lib = relay.build(mod, target=target, params=params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the library at local temporary directory.</span></span><br><span class="line">fcompile = ndk.create_shared <span class="keyword">if</span> <span class="keyword">not</span> local_demo <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">lib.export_library(<span class="string">&quot;net.so&quot;</span>, fcompile)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cpp load compiled so</span></span><br><span class="line">tvm::runtime::Module mod_factory = tvm::runtime::Module::<span class="built_in">LoadFromFile</span>(<span class="string">&quot;lib/net.so&quot;</span>);</span><br><span class="line">  <span class="comment">// create the graph executor module</span></span><br><span class="line">tvm::runtime::Module gmod = mod_factory.<span class="built_in">GetFunction</span>(<span class="string">&quot;default&quot;</span>)(dev);</span><br><span class="line">tvm::runtime::PackedFunc set_input = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;set_input&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc get_output = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;get_output&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc run = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;run&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the C++ API</span></span><br><span class="line">tvm::runtime::NDArray x = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line">tvm::runtime::NDArray y = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">    <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(x-&gt;data)[i * <span class="number">2</span> + j] = i * <span class="number">2</span> + j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// set the right input</span></span><br><span class="line"><span class="built_in">set_input</span>(<span class="string">&quot;1&quot;</span>, x);</span><br><span class="line"><span class="comment">// run the code</span></span><br><span class="line"><span class="built_in">run</span>();</span><br><span class="line"><span class="comment">// get the output</span></span><br><span class="line"><span class="built_in">get_output</span>(<span class="number">0</span>, y);</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用 Github Actions 自动部署 Hexo 博客</title>
      <link href="/auto-deploy/"/>
      <url>/auto-deploy/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Github Actions 可以很方便实现 CI&#x2F;CD 工作流，类似 Travis 的用法，来帮我们完成一些工作，比如实现自动化测试、打包、部署等操作。当我们运行 Jobs 时，它会创建一个容器 (runner)，容器支持：Ubuntu、Windows 和 MacOS 等系统，在容器中我们可以安装软件，利用安装的软件帮我们处理一些数据，然后把处理好的数据推送到某个地方。</p><p>本文将介绍利用 Github Actions 实现自动部署 hexo 到 Github Pages，在之前我们需要写完文章执行 <code>hexo generate --deploy</code> 来部署，当你文章比较多的时候，可能还需要等待很久，而且还可能会遇到本地安装的 Node.js 版本与 Hexo 不兼容的问题，目前我就是因为电脑的 Node.js 版本升到 v14 版本导致与 Hexo 不兼容部署不了，才来捣腾 Github Actions 功能的。利用 Github Actions 你将会没有这些烦恼。</p><h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><h2 id="创建所需仓库"><a href="#创建所需仓库" class="headerlink" title="创建所需仓库"></a>创建所需仓库</h2><ol><li>创建 your.github.io 仓库用来存放博客和静态博客页面，这两个在不同分支。</li></ol><h2 id="生成部署密钥"><a href="#生成部署密钥" class="headerlink" title="生成部署密钥"></a>生成部署密钥</h2><p>一路按回车直到生成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-keygen -t rsa -b 4096 -C <span class="string">&quot;<span class="subst">$(git config user.email)</span>&quot;</span> -f gh-pages -N <span class="string">&quot;&quot;</span></span></span><br></pre></td></tr></table></figure><p>当前目录下会有 gh-pages 和 gh-pages.pub 两个文件。</p><h2 id="配置部署密钥"><a href="#配置部署密钥" class="headerlink" title="配置部署密钥"></a>配置部署密钥</h2><p>复制  <code>gh-pages</code> 文件内容，在仓库 <code>Settings -&gt; secrets and variables -&gt; new repository secret</code> 页面上添加。</p><ol><li>在 <code>Name</code> 输入框填写 <code>ACTIONS_DEPLOY_KEY</code>。</li><li>在 <code>secret</code>输入框填写  <code>gh-pages</code> 文件内容。</li></ol><img src="/auto-deploy/1693718105040.png" class="" width="1693718105040"><p>复制  <code>gh-pages.pub</code> 文件内容，在 仓库 Settings -&gt; Deploy keys -&gt; Add deploy key 页面上添加。</p><ol><li>在 Title 输入框填写 HEXO_DEPLOY_PUB。</li><li>在 Key 输入框填写  gh-pages.pub 文件内容。</li><li>勾选 Allow write access 选项。</li></ol><img src="/auto-deploy/1693718293575.png" class="" width="1693718293575"><h1 id="编写-Github-Actions"><a href="#编写-Github-Actions" class="headerlink" title="编写 Github Actions"></a>编写 Github Actions</h1><h2 id="Workflow-模版"><a href="#Workflow-模版" class="headerlink" title="Workflow 模版"></a>Workflow 模版</h2><p>在 blog 仓库根目录下创建 .github&#x2F;workflows&#x2F;deploy.yml 文件，目录结构如下。</p><figure class="highlight plaintext"><figcaption><span>(repository)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">└── .github</span><br><span class="line">    └── workflows</span><br><span class="line">        └── deploy.yml</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在 deploy.yml 文件中粘贴以下内容。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Pages</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 触发器、分支</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">gh-pages</span>  <span class="comment"># default branch</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="comment"># 子任务</span></span><br><span class="line">  <span class="attr">pages:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> <span class="comment"># 定运行所需要的虚拟机环境</span></span><br><span class="line">    <span class="attr">permissions:</span></span><br><span class="line">      <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">submodules:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">fetch-depth:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment"># 每个name表示一个步骤:step</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Use</span> <span class="string">Node.js</span> <span class="number">18.</span><span class="string">x</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/setup-node@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">node-version:</span> <span class="string">&#x27;18.17.1&#x27;</span> <span class="comment"># 自己正在使用的node版本即可</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Global</span> <span class="string">Config</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="attr">ACTIONS_DEPLOY_KEY:</span> <span class="string">$&#123;&#123;secrets.ACTIONS_DEPLOY_KEY&#125;&#125;</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          sudo timedatectl set-timezone &quot;Asia/Shanghai&quot;</span></span><br><span class="line"><span class="string">          mkdir -p ~/.ssh/</span></span><br><span class="line"><span class="string">          echo &quot;$ACTIONS_DEPLOY_KEY&quot; &gt; ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          chmod 600 ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          git config --global user.email &quot;xx&quot;</span></span><br><span class="line"><span class="string">          git config --global user.name &quot;XXX&quot;</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># - run: node -v # 查看node版本号</span></span><br><span class="line">      <span class="comment"># 缓存依赖项: https://docs.github.com/cn/actions/using-workflows/caching-dependencies-to-speed-up-workflows</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Cache</span> <span class="string">NPM</span> <span class="string">dependencies</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/cache@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="comment"># npm cache files are stored in `~/.npm` on Linux/macOS</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">~/.npm</span></span><br><span class="line">          <span class="comment"># path: node_modules</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.OS</span> <span class="string">&#125;&#125;-npm-cache</span></span><br><span class="line">          <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.OS &#125;&#125;-npm-cache</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># 查看路径 : /home/runner/work/blog/blog</span></span><br><span class="line">      <span class="comment"># - name: Look Path</span></span><br><span class="line">      <span class="comment">#   run: pwd</span></span><br><span class="line">      <span class="comment"># 查看文件</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="comment"># 第一次或者依赖发生变化的时候执行 Install Dependencies，其它构建的时候不需要这一步</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Install</span> <span class="string">Dependencies</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Clean</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">clean</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">deploy</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Get</span> <span class="string">the</span> <span class="string">output</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          echo &quot;$&#123;&#123; steps.deploy.outputs.notify &#125;&#125;&quot;</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><h1 id="hexo配置文件"><a href="#hexo配置文件" class="headerlink" title="hexo配置文件"></a>hexo配置文件</h1><p>blog 根目录下，名为 _config.yml，配置一下deploy的分支信息。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span> </span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span> </span><br><span class="line">  <span class="attr">repository:</span> <span class="string">xxx</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">deploy</span></span><br></pre></td></tr></table></figure><h1 id="执行任务"><a href="#执行任务" class="headerlink" title="执行任务"></a>执行任务</h1><p>写一篇文章，push 到 仓库的 master 分支，在仓库 Actions 页面查看当前 task。</p><p>当任务完成后查看您的博客 <a href="https://your.github.io/">https://your.github.io</a>，如果不出意外的话已经可以看到新添加的文章了。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>偷懒是人类发展的动力，人都有偷懒的想法，目的就是为了让自己能够活得更好，经过几千年的不断发展，现在人偷懒的方式无疑更加的先进。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++初始化列表</title>
      <link href="/cpp-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%97%E8%A1%A8/"/>
      <url>/cpp-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%97%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>类对象的构造顺序是：<br>1.分配内存，调用构造函数时，隐式／显示的初始化各数据成员；<br>2.进入构造函数后在构造函数中执行一般赋值与计算。</p><h3 id="使用初始化列表有两个原因："><a href="#使用初始化列表有两个原因：" class="headerlink" title="使用初始化列表有两个原因："></a>使用初始化列表有两个原因：</h3><p><strong>原因1.必须这样做：</strong></p><p>《C++ Primer》中提到在以下三种情况下需要使用初始化成员列表： </p><ol><li>需要<strong>初始化的数据成员是对象</strong>的情况(这里包含了<strong>继承</strong>情况下，通过显示调用父类的构造函数对父类数据成员进行初始化)； </li><li>需要初始化<strong>const修饰的类成员</strong>； </li><li>需要初始化<strong>引用成员数据</strong>；</li></ol><blockquote><p>1 的说明：数据成员是对象，并且这个对象只有含参数的构造函数，没有无参数的构造函数；</p></blockquote><p><strong>原因2.效率要求这样做：</strong><br>类对象的构造顺序显示，进入构造函数体后，进行的是计算，是对成员变量的赋值操作，显然，赋值和初始化是不同的，这样就体现出了效率差异，如果不用成员初始化列表，那么类对自己的类成员分别进行的是一次隐式的默认构造函数的调用，和一次赋值操作符的调用，如果是类对象，这样做效率就得不到保障。 </p><h4 id="类成员是对象时的运行分析"><a href="#类成员是对象时的运行分析" class="headerlink" title="类成员是对象时的运行分析"></a>类成员是对象时的运行分析</h4><p><strong>不使用初始化列表</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="built_in">Base</span>()</span><br><span class="line">&#123;</span><br><span class="line">i = <span class="number">0</span>;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base无参构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Base</span>(<span class="type">int</span> tmp)</span><br><span class="line">&#123;</span><br><span class="line">i = tmp;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base含参构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Base</span>(<span class="type">const</span> Base &amp;tmp)</span><br><span class="line">&#123;</span><br><span class="line">i = tmp.i;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">Base b;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i, Base tmp)</span><br><span class="line">&#123;</span><br><span class="line">a = i;</span><br><span class="line">b = tmp;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test 构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;Test拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Base b;</span><br><span class="line"><span class="function">Test <span class="title">a</span><span class="params">(<span class="number">10</span>, b)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出为</p><blockquote><p>0       Base无参构造函数<br>0       Base拷贝构造函数<br>0       Base无参构造函数<br>10      Test 构造函数<br>10      Test析构函数</p></blockquote><p>如果不使用初始化列表，Test对象初始化时，需要先调用一次无参构造参数，然后赋值运算，如果没有实现，将会报错。</p><p><strong>使用初始化列表</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">Base b;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i, Base tmp):<span class="built_in">a</span>(i),<span class="built_in">b</span>(tmp)</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test 构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;Test拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出为</p><blockquote><p>0       Base无参构造函数<br>0       Base拷贝构造函数<br>0       Base拷贝构造函数<br>10      Test 构造函数<br>10      Test析构函数</p></blockquote><p>使用了初始化列表以后，就可以直接调用拷贝构造函数。如果是传入引用，拷贝构造函数的调用就没有了。</p><h4 id="注"><a href="#注" class="headerlink" title="注"></a>注</h4><ol><li>类里面的任何成员变量在定义时是不能初始化的。 </li><li>一般的数据成员可以在构造函数中初始化。 </li><li>const数据成员必须在构造函数的初始化列表中初始化。 </li><li>static要在类的定义外面初始化。 </li><li>数组成员是不能在初始化列表里初始化的。 </li><li>不能给数组指定明显的初始化。</li></ol><blockquote><p>3和5决定了，类成员中不能定义常量数组。</p></blockquote><p>初始化列表中成员列出的顺序和它们在类中声明的顺序相同.</p><blockquote><p>对一个对象的所有成员来说，它们的析构函数被调用的顺序总是和它们在构造函数里被创建的顺序相反。那么，如果允许上面的情况（即，成员按它们在初始化列表上出现的顺序被初始化）发生，编译器就要为每一个对象跟踪其成员初始化的顺序，以保证它们的析构函数以正确的顺序被调用。这会带来昂贵的开销。所以，为了避免这一开销，同一种类型的所有对象在创建（构造）和摧毁（析构）过程中对成员的处理顺序都是相同的,而不管成员在初始化列表中的顺序如何</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初始化方法-基本到kaiming</title>
      <link href="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/"/>
      <url>/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么需要初始化"><a href="#为什么需要初始化" class="headerlink" title="为什么需要初始化"></a>为什么需要初始化</h3><p>初始化的原因，</p><ul><li>防止每一层的输出太大或者太小，导致梯度反向传播过程中，梯度爆炸或者梯度消失。</li><li>不能采用统一值得原因，因为统一值得初始化会使得每一层网络在不同通道学到得特征相同。</li></ul><p>上述原因都会导致，网络模型不能收敛。</p><h4 id="简单例子得说明"><a href="#简单例子得说明" class="headerlink" title="简单例子得说明"></a>简单例子得说明</h4><p>假如我们有一个输入<code>x</code> ，定义为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">512</span>)</span><br></pre></td></tr></table></figure><p><code>x</code>是&#96;均值为 $0$，方差是 $1$ 的高斯分布。然后定义一个100层的神经网络（注：不包含激活函数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br></pre></td></tr></table></figure><p>那么得到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(nan),tensor(nan))</span><br></pre></td></tr></table></figure><p>输出已经是无穷大了。通过下面的代码，可以知道大概29层以后，输出就已经无法计算了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line">    <span class="keyword">if</span> torch.isnan(x.std()):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(i) <span class="comment"># 28</span></span><br></pre></td></tr></table></figure><p>既然输出太大，我们把神经网络的初始化变小一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)*<span class="number">0.01</span></span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># 0, 0</span></span><br></pre></td></tr></table></figure><p>那么得到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.</span>),tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure><p>这时候的输出就太小，没办法计算了。</p><h3 id="怎么找到合适的初始化方法"><a href="#怎么找到合适的初始化方法" class="headerlink" title="怎么找到合适的初始化方法"></a>怎么找到合适的初始化方法</h3><p>对于神经网络来说，前向传播过程就是矩阵运算，假设一层的输出为$y$<br>$$<br>y_i&#x3D; \sum_{k&#x3D;1}^{n-1}a_{i,k}x_k<br>$$</p><p>$i$ 是矩阵 $\mathbf{m}$ 的行，$k$ 是矩阵 $\mathbf{m}$ 的列。python的计算代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[i] = <span class="built_in">sum</span>([c*d <span class="keyword">for</span> c,d <span class="keyword">in</span> <span class="built_in">zip</span>(a[i], x)])</span><br></pre></td></tr></table></figure><p>可以证明，在给定的层，从标准正态分布初始化的输入$x$ 和权重矩阵 $a$ 的矩阵乘积平均具有非常接近输入<strong>连接数的平方根的标准偏差</strong>，在例子中是$\sqrt{512}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean()/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.00889449315816164  22.629779825053976</span></span><br><span class="line"><span class="built_in">print</span>(math.sqrt(<span class="number">512</span>))</span><br><span class="line"><span class="comment"># 22.627416997969522</span></span><br><span class="line"></span><br><span class="line">mean,var = <span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    b = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    z = b @ y</span><br><span class="line">    mean += z.mean().item()</span><br><span class="line">    var += z.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.6010947234869003 511.8684602024235</span></span><br></pre></td></tr></table></figure><p>如果我们根据如何定义矩阵乘法来看前向传播的过程：</p><p>为了计算 $y$，我们将输入 $x$ 的一个元素的乘以矩阵 $\mathbf{a}$ 的一列的512个乘积然后相加。在使用标准正态分布初始化$x$ 和 $a$ 的示例中，这$512$ 个数字中的每一个的平均值为 $0$，标准差为$1$。</p><blockquote><p><strong>经过一层网络运算以后，均值没变，方差扩大了$\sqrt{512}$倍。</strong></p></blockquote><p>因此在初始化的是，缩小$\sqrt{512}$倍，那么输出结果就能保证不<strong>爆炸</strong>了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.00039810733370250094 1.0007971983717594</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-0.0048) tensor(1.2810)</span></span><br></pre></td></tr></table></figure><h3 id="Xavier-Initialization"><a href="#Xavier-Initialization" class="headerlink" title="Xavier Initialization"></a>Xavier Initialization</h3><p>上面介绍的情况是在不含有激活的函数情形，如果增加了激活函数，是否仍能保持不变呢？对于不同类型的激活函数，是不是有不同的表现呢？最开始用的激活函数多数为对称的，并且导数从中间到两边有递减为0。比如，常用的<code>tanh</code>和<code>sigmoid</code>函数。</p><p>下面的结果是在上面的例子中分别增加了，<code>tanh</code>和<code>sigmoid</code>函数的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sigmoid</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.5057) tensor(0.1180)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tanh</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-0.0051) tensor(0.0879)</span></span><br></pre></td></tr></table></figure><p>可以看到经过激活函数以后，函数方差明显变小了。在训练过程中这就会导致，导致梯度过小，使得训练难以进行。</p><p>上面用的是正态分布，如果采用均匀分布呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-3.8077e-26) tensor(1.2476e-24)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-1.) tensor(0.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(1.0000) tensor(3.8114e-06)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.4934) tensor(0.0659)</span></span><br></pre></td></tr></table></figure><p>方差都出人意料的小。这就几乎不能学习到什么有用的特征了。</p><p>为此，Glorot and Bengio 提出了<code>Xavier initialization</code>的初始化方式</p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>这种初始化方式是从随机均匀分布初始化神经网络的，均匀分布的范围是<br>$$<br>\pm \frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}<br>$$<br>这里的 $n_i$ 是输入神经元数目，$n_{i+1}$ 是输出神经元数目。</p><p>Glorot and Bengio 认为Xavier 初始化方法，可以在包含激活函数的神经网络中保持方差的变化很小。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/Xavier.png" class="" title="img"><p>除此之外，同样证明了，传统方法在底层网络方差大，高层网络方差趋近于0的现象。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/Xavier2.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m,n</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(m,n).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">6.</span>/(m+n))</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.0854) tensor(0.9933)</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.4686) tensor(0.4976)</span></span><br></pre></td></tr></table></figure><h3 id="Kaiming-Initialization"><a href="#Kaiming-Initialization" class="headerlink" title="Kaiming Initialization"></a>Kaiming Initialization</h3><p>进来CV领域中，激活方法多是采用<code>Relu</code> 函数。对于这个函数。之前的初始化方法，又有哪些不一样？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = torch.relu(a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(4.6656e-16) tensor(6.7154e-16)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.relu( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(nan) tensor(nan)</span></span><br></pre></td></tr></table></figure><p>之前的初始化方法，对于<code>Relu</code>函数都不奏效了。那对于每一层来说，有什么变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    y = torch.relu(a @ x)</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#9.01142036409378 15.991211348807246</span></span><br><span class="line"><span class="built_in">print</span>(math.sqrt(<span class="number">512</span>/<span class="number">2</span>))</span><br><span class="line"><span class="comment">#16.0</span></span><br></pre></td></tr></table></figure><p>可以看到，这时候的输出跟输入网络层数大小是有关系的。在下面的实验验证以下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>/<span class="number">2.</span>)</span><br><span class="line">    y = torch.relu(a @ x)</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.5640919140070677 1.0003173674661943</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming</span>(<span class="params">m,n</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.randn(m,n)*math.sqrt(<span class="number">2.</span>/m)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = kaiming(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = torch.relu( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(0.8135) tensor(1.2431)</span></span><br></pre></td></tr></table></figure><p>对照本部分开始的结果<code>kaiming</code>方法在对于<code>Relu</code>函数更有优势。</p><p>下图给出了两种方法在一个30层CNN上的结果。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/kaiming.png" class="" title="kaiming method"><hr><p><strong>来源</strong>  <a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表反转</title>
      <link href="/%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/"/>
      <url>/%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/</url>
      
        <content type="html"><![CDATA[<h2 id="leetcode-206-单链表反转"><a href="#leetcode-206-单链表反转" class="headerlink" title="leetcode 206 单链表反转"></a>leetcode 206 单链表反转</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL</span><br><span class="line">输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL</span><br></pre></td></tr></table></figure><h3 id="迭代方法"><a href="#迭代方法" class="headerlink" title="迭代方法"></a>迭代方法</h3><p>首先设置<code>pre,cur,lat</code>三个指针</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pre   cur  lat</span><br><span class="line">null   1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>接着<code>cur.next = pre</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pre   cur  lat</span><br><span class="line">null &lt;-1    2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>接着<code>pre = cur，cur = lat，lat = lat.next</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">      pre  cur  lat</span><br><span class="line">null &lt;-1    2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>重复上述操作直到<code>lat=None</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">                     pre  cur  lat</span><br><span class="line">null &lt;-1 &lt;- 2 &lt;- 3 &lt;- 4    5 -&gt; null</span><br></pre></td></tr></table></figure><p><strong>代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line">        cur = head </span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            cur.<span class="built_in">next</span>, pre, cur =  pre, cur,cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head</span>):</span><br><span class="line">    <span class="keyword">if</span> head == <span class="literal">None</span> <span class="keyword">or</span> head.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> head</span><br><span class="line">    node = self.reverseList(head.<span class="built_in">next</span>)</span><br><span class="line">    head.<span class="built_in">next</span>.<span class="built_in">next</span> = head</span><br><span class="line">    head.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure><h2 id="leetcode-25-单链表-k组反转"><a href="#leetcode-25-单链表-k组反转" class="headerlink" title="leetcode 25 单链表-k组反转"></a>leetcode 25 单链表-k组反转</h2><blockquote><p>给定这个链表：<code>1-&gt;2-&gt;3-&gt;4-&gt;5</code><br>当 <code>k = 2</code> 时，应当返回: <code>2-&gt;1-&gt;4-&gt;3-&gt;5</code><br>当 <code>k = 3</code> 时，应当返回: <code>3-&gt;2-&gt;1-&gt;4-&gt;5</code></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>柔性数组</title>
      <link href="/%E6%9F%94%E6%80%A7%E6%95%B0%E7%BB%84/"/>
      <url>/%E6%9F%94%E6%80%A7%E6%95%B0%E7%BB%84/</url>
      
        <content type="html"><![CDATA[<p>这个问题是阿里的一个面试题。当时没有很清楚，答得很差，特地实验看一下运行结果。</p><blockquote><p>在结构体中定义了一个<code>char*</code>指针，与定义一个零元素的<code>char</code>数组有什么区别？</p></blockquote><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>常用来构成缓冲区。比起指针，用空数组有这样的优势：</p><ul><li>不需要初始化，数组名直接就是所在的偏移；</li><li>不占任何空间，指针需要占用int长度空间，空数组不占任何空间。<blockquote><p>“这个数组不占用任何内存”，意味着这样的结构节省空间；<br>“该数组的内存地址就和它后面的元素地址相同”，意味着无需初始化，数组名就是后面元素的地址，直接就能当指针使用。</p></blockquote></li></ul><p>这样的写法最适合制作动态buffer，因为可以这样分配空间<code>malloc(sizeof(structXXX) + buff_len)</code>; 直接就把buffer的结构体和缓冲区一块分配了。用起来也非常方便，因为现在空数组其实变成了buff_len长度的数组了。这样的好处是：</p><ul><li>一次分配解决问题，省了不少麻烦。为了防止内存泄露，如果是分两次分配(结构体和缓冲区)，那么要是第二次malloc失败了，必须回滚释放第一个分配的结构体。这样带来了编码麻烦。其次，分配了第二个缓冲区以后，如果结构里面用的是指针，还要为这个指针赋值。同样，在free这个buffer的时候，用指针也要两次free。如果用空数组，所有问题一次解决。</li><li>小内存的管理是非常困难的，如果用指针，这个buffer的struct部分就是小内存了，在系统内存在多了势必严重影响内存管理的性能。要是用空数组把struct和实际数据缓冲区一次分配大块问题，就没有这个问题。如此看来，用空数组既简化编码，又解决了小内存碎片问题提高了性能。</li></ul><p>结构体最后使用0或1长度数组的原因：</p><blockquote><p>为了方便的管理内存缓冲区(其实就是分配一段连续的内存，减少内存的碎片化)，如果直接使用指针而不使用数组，那么，在分配内存缓冲区时，就必须分配结构体一次，然后再分配结构体内的指针一次，(而此时分配的内存已经与结构体的内存不连续了，所有要分别管理即申请和释放)而如果使用数组，那么只需要一次就可以全部分配出来，反过来，释放时也是一样，使用数组，一次释放。使用指针，得先释放结构体内的指针，再释放结构体，还不能颠倒顺序</p></blockquote><p>结构体中最后一个成员为[1]长度数组的用法：与长度为[0]数组的用法相同，改写为[1]是出于可移植性的考虑。有些编译器不支持[0]数组，可将其改成[]或[1].</p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span>&#123;</span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span>&#123;</span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了说明这个问题，我们定义一下几个结构体作为比较：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">D</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">1</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">E</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">10</span>];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(A) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(B) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(C) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(D) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(E) &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>输出占用空间大小为</p><blockquote><p>4 8 4 8 16</p></blockquote><p>可以看到<code>struct A</code>大小为<code>int</code>大小 4字节，<code>struct B</code>由于包含了一个指针，在32位系统中，大小为 4字节，总共8字节。<code>strcut C</code>大小为4字节，明显<code>char[0]</code>没有分配内存。<code>struct D</code>大小由于内存对齐原因得到为8字节。<code>struct E</code>大小同样由于内存对齐原因得到为16字节。</p><p>由此可以看到长度为0的数组没有分配内存。<br>为了更详细的说明内存分配情况，我们查看一下每个的地址.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A a; B b; C c; D d; E e;</span><br><span class="line">cout &lt;&lt; &amp;a.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;b.b - (<span class="type">int</span>)&amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;b.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;c.b - (<span class="type">int</span>)&amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;c.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;d.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;d.b - (<span class="type">int</span>)&amp;d.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;d.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;e.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;e.b - (<span class="type">int</span>)&amp;e.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;e.b &lt;&lt; endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出为:</p><blockquote><p>00AFFB4C<br>00AFFB3C        4       00AFFB40<br>00AFFB30        4       00AFFB34<br>00AFFB20        4       00AFFB24<br>00AFFB08        4       00AFFB0C</p></blockquote><p>可以看到每个<code>b</code>都指向了同一位置,<code>int a</code>后面一位的地址.<br>为了更清楚的描述,在中间插入一个<code>char c</code>可以看到有:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">A a;</span><br><span class="line">B b;</span><br><span class="line">C c;</span><br><span class="line">cout &lt;&lt; &amp;a.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;b.b - (<span class="type">int</span>)&amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;b.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;c.b - (<span class="type">int</span>)&amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;c.b &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>输出为:</p><blockquote><p>012FF9E8<br>012FF9D4        8       012FF9DC<br>012FF9C4        5       012FF9C9</p></blockquote><p>很明显可以得到结论,<code>char b[0]</code>不分配内存,但是可以获得结构体的末尾地址.</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++构造函数</title>
      <link href="/cpp-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0/"/>
      <url>/cpp-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="调用拷贝构造函数的几种情况"><a href="#调用拷贝构造函数的几种情况" class="headerlink" title="调用拷贝构造函数的几种情况"></a>调用拷贝构造函数的几种情况</h2><blockquote><p>当类中成员有<strong>指针变量</strong>、类中有<strong>动态内存分配</strong>时常常需要用户自己定义拷贝构造函数。</p></blockquote><p>在什么情况下系统会调用拷贝构造函数：</p><blockquote><p>（1）用类的一个对象去初始化另一个对象时<br>（2）当函数的形参是类的对象时（也就是<strong>值传递</strong>时），如果是引用传递则不会调用<br>（3）当函数的<strong>返回值是类的对象或引用</strong>时</p></blockquote><p>代码示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i):<span class="built_in">a</span>(i)</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">Test <span class="title">a</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line"><span class="function">Test <span class="title">b</span><span class="params">(<span class="number">20</span>)</span></span>;</span><br><span class="line"><span class="built_in">Add</span>(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="调用函数为值传递"><a href="#调用函数为值传递" class="headerlink" title="调用函数为值传递"></a>调用函数为值传递</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test <span class="title">Add</span><span class="params">(Test a, Test b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出为</p><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td>a,b初始化</td></tr><tr><td>20      拷贝构造函数</td><td></td></tr><tr><td>10      拷贝构造函数</td><td>形参的拷贝构造，右到左</td></tr><tr><td>30      构造函数</td><td>Test(a.a + b.a)临时对象</td></tr><tr><td>30      拷贝构造函数</td><td>赋值给res</td></tr><tr><td>30      析构函数</td><td>Test(a.a + b.a)临时对象析构</td></tr><tr><td>10      析构函数</td><td></td></tr><tr><td>20      析构函数</td><td>形参析构</td></tr><tr><td>30      析构函数</td><td>返回值res的析构函数</td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td>a,b析构函数</td></tr></tbody></table><h4 id="参数为引用传递"><a href="#参数为引用传递" class="headerlink" title="参数为引用传递"></a>参数为引用传递</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test <span class="title">Add</span><span class="params">(Test&amp; a, Test&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td>a,b 初始化</td></tr><tr><td>30      构造函数</td><td>Test(a.a + b.a)临时变量初始化</td></tr><tr><td>30      拷贝构造函数</td><td>res的复制构造（值传递）</td></tr><tr><td>30      析构函数</td><td>临时对象析构</td></tr><tr><td>30      析构函数</td><td>res析构</td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td>a,b析构</td></tr></tbody></table><h4 id="返回值为引用"><a href="#返回值为引用" class="headerlink" title="返回值为引用"></a>返回值为引用</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test&amp; <span class="title">Add</span><span class="params">(Test&amp; a, Test&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td></td></tr><tr><td>30      构造函数</td><td>res 是 临时对象的引用，因此不会拷贝构造</td></tr><tr><td>30      析构函数</td><td></td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>516.最长回文子序列</title>
      <link href="/Longest_Palindromic_Subsequence/"/>
      <url>/Longest_Palindromic_Subsequence/</url>
      
        <content type="html"><![CDATA[<p><strong>题目</strong></p><blockquote><p>给定字符串s，求其最长回文子序列（可以非连续）的长度</p></blockquote><hr><p><strong>DP</strong></p><blockquote><p>当已知一个序列是回文时，添加首尾元素后的序列存在两种情况，一种是首尾元素相等，则最长回文的长度加2，当首尾元素不相等，则最长回文序列为仅添加首元素时的最长回文与仅添加尾元素时的最长回文之间的最大值。我们可以用$dp[i][j]$表示$s[i…j]$中的最长回文序列，而状态转移方程则是 </p><ol><li>$i &gt; j，dp[i][j] &#x3D; 0；$ </li><li>$i &#x3D;&#x3D; j，dp[i][j] &#x3D; 1；$ </li><li>$i &lt; j且s[i] &#x3D;&#x3D; s[j]，dp[i][j] &#x3D; dp[i + 1][j - 1] + 2； $</li><li>$i &lt; j且s[i]！&#x3D; s[j]，dp[i][j] &#x3D; max(dp[i + 1][j]，dp[i][j - 1])；$</li></ol><p>从状态转移方程可以看出，计算$dp[i][j]$时需要用到$dp[i+1][j - 1]$和$dp[i + 1][j]$，所以对于$i$的遍历应该从尾部开始，最后返回$dp[0][s.length() - 1]$就行。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindromeSubseq</span>(<span class="params">self, s</span>):</span><br><span class="line">memo = [[<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))]</span><br><span class="line"><span class="keyword">return</span> self.__shrink_recursion(s, <span class="number">0</span>, <span class="built_in">len</span>(s)-<span class="number">1</span>, memo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__shrink_recursion</span>(<span class="params">self, s, left, right,  memo</span>):</span><br><span class="line"><span class="keyword">if</span> (memo[left][right] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line"><span class="keyword">return</span> memo[left][right]</span><br><span class="line"><span class="keyword">if</span> (left &gt; right):</span><br><span class="line">memo[left][right] = <span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> (left == right):</span><br><span class="line">memo[left][right] = <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (s[left] == s[right]):</span><br><span class="line">memo[left][right] = self.__shrink(s, left+<span class="number">1</span>, right-<span class="number">1</span>, memo) + <span class="number">2</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">memo[left][right] = <span class="built_in">max</span>(self.__shrink_recursion(s, left+<span class="number">1</span>, right, memo),\</span><br><span class="line">                                    self.__shrink_recursion(s, left, right-<span class="number">1</span>, memo))</span><br><span class="line"><span class="comment"># print(memo)</span></span><br><span class="line"><span class="keyword">return</span> memo[left][right]</span><br><span class="line">   </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DP_iter</span>(<span class="params">self, s</span>):</span><br><span class="line">lens = <span class="built_in">len</span>(s)</span><br><span class="line">i = j = lens // <span class="number">2</span></span><br><span class="line">memo = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lens-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">memo[i][i] = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, lens):</span><br><span class="line"><span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">memo[i][j] = memo[i+<span class="number">1</span>][j-<span class="number">1</span>]+<span class="number">2</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">memo[i][j] = <span class="built_in">max</span>(memo[i][j-<span class="number">1</span>], memo[i+<span class="number">1</span>][j])</span><br><span class="line"><span class="keyword">return</span> memo[<span class="number">0</span>][-<span class="number">1</span>]</span><br><span class="line">   </span><br></pre></td></tr></table></figure><p>时间复杂度是$O(n^2)$，空间复杂度是$O(n^2)$。</p><hr><p><strong>改进</strong></p><blockquote><p>上述的算法，从状态转移方程来看，计算$dp[i][x]$时，只用到了$dp[i][y]$和$dp[i + 1][z]$，即计算当前行时，只用到了当前行和下一行，因此可以对上一个算法进行改进，需要用两行空间存储就能完成计算。</p><p>用一个变量cur表示当前行的下标，cur的取值为0或1，1 - cur表示的就是另外一行，因此状态转移方程变成了： </p><ol><li>$i &gt; j，dp[cur][j] &#x3D; 0； $</li><li>$i &#x3D;&#x3D; j，dp[cur][j] &#x3D; 1； $</li><li>$i &lt; j且s[i] &#x3D;&#x3D; s[j]，dp[cur][j] &#x3D; dp[1 - cur][j - 1] + 2；$</li><li>$i &lt; j且s[i]！&#x3D; s[j]，dp[cur][j] &#x3D; max(dp[1 - cur][j]，dp[cur][j - 1])； $</li></ol><p>注意每次计算完一个$i$后需要更新$cur$的值，即$cur &#x3D; 1 - cur$。因为循环执行最后一次之后会多更新一次cur，所以返回的是$dp[1 - cur][s.length() - 1]$的值。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestPalindromeSubseq</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">length</span>(), cur = <span class="number">0</span>;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(<span class="number">2</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            dp[cur][i] = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s[i] == s[j]) &#123;</span><br><span class="line">                    dp[cur][j] = dp[<span class="number">1</span> - cur][j - <span class="number">1</span>] + <span class="number">2</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[cur][j] = <span class="built_in">max</span>(dp[<span class="number">1</span> - cur][j], dp[cur][j - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = <span class="number">1</span> - cur;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">1</span> - cur][n - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>53.最大子序列和</title>
      <link href="/MaximumSubarray/"/>
      <url>/MaximumSubarray/</url>
      
        <content type="html"><![CDATA[<blockquote><p>对于给定序列，得到最大和的子序列</p><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;Input: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">&gt;Output: 6</span><br><span class="line">&gt;Explanation: [4,-1,2,1] has the largest sum = 6.</span><br></pre></td></tr></table></figure></blockquote><p><strong>brute force</strong></p><blockquote><p>遍历所有的可能答案，得到最大子序列和。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> L <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):<span class="comment"># 左边界</span></span><br><span class="line">        <span class="keyword">for</span> R <span class="keyword">in</span> <span class="built_in">range</span>(L,<span class="built_in">len</span>(nums)):<span class="comment"># 右边界</span></span><br><span class="line">            cur_sum = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L,R):</span><br><span class="line">                cur_sum+=nums[i]</span><br><span class="line">            <span class="keyword">if</span> cur_sum &gt; max_sum:</span><br><span class="line">                max_sum = cur_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure><p>时间复杂度 $O(n^3)$</p><p><strong>改进版穷举</strong></p><blockquote><p>上面的方法，可以改进，去掉最内层的循环。以左边界为起点，记录连续的求和，只取最大的即可。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> L <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):<span class="comment"># 左边界</span></span><br><span class="line">        cur_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> R <span class="keyword">in</span> <span class="built_in">range</span>(L,<span class="built_in">len</span>(nums)):</span><br><span class="line">            cur_sum+=nums[R]</span><br><span class="line">            <span class="keyword">if</span> cur_sum &gt; max_sum:</span><br><span class="line">                max_sum = cur_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure><p>此时时间复杂度 $O(n^2)$</p><p><strong>分治</strong></p><blockquote><p>这个问题可以递归求解，</p><p>在例子中，最大子序列的和只可能出现在3个地方：</p><ol><li>出现在输入数据的左半部分</li><li>出现在输入数据的右半部分</li><li>跨越输入数据的中部而位于左右两个部分</li></ol><p>前两种情况可以递归求解，第三种情况的最大和可以通过求出前半部分（包含前半部分的最后一个元素）的最大和以及后半部分（包括后半部分的第一个元素）的最大和，再将二者相加得到。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Devided_Conquer</span>(<span class="params">nums, left, right</span>):</span><br><span class="line">  <span class="keyword">if</span> left == right:</span><br><span class="line">    <span class="keyword">return</span> nums[left] <span class="comment">#if nums[left] &gt; 0 else 0</span></span><br><span class="line">  </span><br><span class="line">  center = (left+right) // <span class="number">2</span></span><br><span class="line">  max_left  = Devided_Conquer(nums, left, center)</span><br><span class="line">  max_right = Devided_Conquer(nums, center+<span class="number">1</span>, right)</span><br><span class="line">  </span><br><span class="line">  left_Sum = <span class="number">0</span></span><br><span class="line">  maxLeft_Sum = nums[center]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(center, left-<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">    left_Sum += nums[i]</span><br><span class="line">    <span class="keyword">if</span> left_Sum &gt; maxLeft_Sum:</span><br><span class="line">      maxLeft_Sum = left_Sum</span><br><span class="line">  </span><br><span class="line">  right_sum = <span class="number">0</span></span><br><span class="line">  max_right_sum = nums[center+<span class="number">1</span>] </span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(center+<span class="number">1</span>, right+<span class="number">1</span>):</span><br><span class="line">    right_sum += nums[i]</span><br><span class="line">    <span class="keyword">if</span> right_sum &gt; max_right_sum:</span><br><span class="line">      max_right_sum = right_sum</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(max_left, max_right, maxLeft_Sum+max_right_sum)</span><br></pre></td></tr></table></figure><p>时间复杂度 $O (N\log N)$</p><p><strong>One-Pass</strong></p><blockquote><p>考虑</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">One_Pass</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = nums[<span class="number">0</span>]</span><br><span class="line">    this_sum = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line">        this_sum = <span class="built_in">max</span>(num, this_sum+num)</span><br><span class="line">        <span class="keyword">if</span> this_sum &gt; max_sum:</span><br><span class="line">            max_sum = this_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>122.Best Time to Buy and Sell Stock II</title>
      <link href="/Best_Time_to_Buy_and_Sell_Stock_II/"/>
      <url>/Best_Time_to_Buy_and_Sell_Stock_II/</url>
      
        <content type="html"><![CDATA[<blockquote><p>与<a href="./Best_Time_to_Buy_and_Sell_Stock">121</a>不同的在于，121只能操作一次，而这个是可以操作任意次。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br></pre></td></tr></table></figure></blockquote><hr><p><strong>brute force</strong></p><blockquote><p>暴力搜索，没什么好说的</p><p>当前位置 $i$ ，搜索其后所有的可能答案，取最大的</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">arr, start</span>):</span><br><span class="line">  <span class="keyword">if</span> start &gt;= <span class="built_in">len</span>(arr):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(arr)):</span><br><span class="line">    tmp_max_profit = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start+<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">      <span class="keyword">if</span> arr[j] &gt; arr[i]:</span><br><span class="line">        profit = brute_force(arr, j+<span class="number">1</span>) + arr[j] - arr[i]</span><br><span class="line">        <span class="keyword">if</span> profit &gt; tmp_max_profit:</span><br><span class="line">          tmp_max_profit = profit</span><br><span class="line">    <span class="keyword">if</span> tmp_max_profit&gt;max_profit:</span><br><span class="line">      max_profit = tmp_max_profit</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br></pre></td></tr></table></figure><p>时间复杂度$O(n^n)$ ， 空间复杂度$O(n)$</p><hr><p> <strong>Peak Valley Approach</strong></p><blockquote><p>给定的<code>price</code>数组为$[7, 1, 5, 3, 6, 4]$. 绘制图片有（来自<a href="https://leetcode.com/media/original_images/122_maxprofit_1.PNG">leetcode</a>）</p><p><img src="https://leetcode.com/media/original_images/122_maxprofit_1.PNG"></p><p>显然有，最终的收益来自于所有的峰值减去谷值之和</p><p>$Total_Profit &#x3D; \sum_i(height(peak_i)-height(valley_i))$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Peak_Valley</span>(<span class="params">prices</span>):</span><br><span class="line">  valley = prices[<span class="number">0</span>]</span><br><span class="line">  peak  = prices[<span class="number">0</span>]</span><br><span class="line">  idx = <span class="number">0</span></span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span>:</span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span> <span class="keyword">and</span> prices[idx+<span class="number">1</span>] &lt;= prices[idx]:</span><br><span class="line">      idx+=<span class="number">1</span></span><br><span class="line">    vally = prices[idx]</span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span> <span class="keyword">and</span> prices[idx+<span class="number">1</span>] &gt; prices[idx]:</span><br><span class="line">      idx+=<span class="number">1</span></span><br><span class="line">    peak = prices[idx]</span><br><span class="line">    max_profit += peak-vally</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br></pre></td></tr></table></figure><p>时间复杂度$O(n)$, 空间复杂度$O(1)$</p><hr><p><strong>Simple One Pass</strong></p><blockquote><p>跟上面略微不同的是，只要斜率是正的，就一直买入卖出就可以获得最大利润</p><p><img src="https://leetcode.com/media/original_images/122_maxprofit_2.PNG" alt="Profit Graph"></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">One_Pass</span>(<span class="params">prices</span>):</span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> idx, price <span class="keyword">in</span> <span class="built_in">enumerate</span>(prices):</span><br><span class="line">    <span class="keyword">if</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> price &gt; prices[idx-<span class="number">1</span>]:</span><br><span class="line">      max_profit += price-prices[idx-<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br><span class="line">        </span><br></pre></td></tr></table></figure><p>时间复杂度$O(n)$, 空间复杂度$O(1)$</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>70. climbing stairs</title>
      <link href="/ClimbingStairs/"/>
      <url>/ClimbingStairs/</url>
      
        <content type="html"><![CDATA[<p><strong>描述</strong></p><blockquote><p>n阶楼梯，每次一步或者两步，一共有多少种方法</p></blockquote><p>[Solutions](..&#x2F;prob_70_Climbing Stairs.py)</p><p><strong>brute_force</strong></p><blockquote><p>$f(n)&#x3D;f(n-1)+f(n-2)$</p><p>显然有，到第n阶楼梯有两种方法，从n-1过去，和n-2过去。即到n阶的方法等于这两种方法的和</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span> <span class="keyword">or</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> brute_force(n-<span class="number">1</span>)+brute_force(n-<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这种方法的时间复杂度为 $2^n$. 图片来自于<a href="https://leetcode.com/problems/climbing-stairs/solution/">leetcode</a><img src="https://leetcode.com/problems/climbing-stairs/Figures/70_Climbing_Stairs_rt.jpg" alt="Climbing_Stairs"></p><p><strong>带记忆的递归计算</strong></p><blockquote><p>在上面的计算中，显然有大量的重复计算，如果这个数值已经存下来了，就可以减小运算时间</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">memo = &#123;&#125;</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">recursion_memo</span>(<span class="params">n, memo</span>):</span><br><span class="line">    <span class="keyword">if</span> n==<span class="number">1</span> <span class="keyword">or</span> n ==<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> memo.keys():</span><br><span class="line">        <span class="keyword">return</span> memo[n]</span><br><span class="line">    value = recursion_memo(n-<span class="number">1</span>, memo) + recursion_memo(n-<span class="number">2</span>, memo)</span><br><span class="line">    memo.update(&#123;n:value&#125;)</span><br><span class="line">    <span class="keyword">return</span> memo[n]</span><br></pre></td></tr></table></figure><p><strong>动态规划</strong></p><blockquote><p>在暴力搜索里提到了，$f(n)&#x3D;f(n-1)+f(n-2)$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dynamic</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span> <span class="keyword">or</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    result = [<span class="number">0</span>]*(n)</span><br><span class="line">    result[<span class="number">0</span>] =<span class="number">1</span></span><br><span class="line">    result[<span class="number">1</span>] =<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n):</span><br><span class="line">        result[i] = result[i-<span class="number">1</span>]+result[i-<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> result[n-<span class="number">1</span>]</span><br><span class="line">        </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>121. Best Time to Buy and Sell Stock</title>
      <link href="/Best_Time_to_Buy_and_Sell_Stock%20/"/>
      <url>/Best_Time_to_Buy_and_Sell_Stock%20/</url>
      
        <content type="html"><![CDATA[<p>Say you have an array for which the $i^{th}$ element is the price of a given stock on day <em>i</em>.</p><p>If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</p><p>Note that you cannot sell a stock before you buy one.</p><p><strong>Example 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.</span><br><span class="line">             Not 7-1 = 6, as selling price needs to be larger than buying price.</span><br></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><p>题目要求为，选择最佳的买入卖出时间，得到最大收益。</p><h2 id="暴力搜索"><a href="#暴力搜索" class="headerlink" title="暴力搜索"></a>暴力搜索</h2><p>复杂度$O(n^2)$ 确切一点是 $O(\frac{n(n+1)}{2})$</p><h2 id="one-pass"><a href="#one-pass" class="headerlink" title="one pass"></a>one pass</h2><p>对于$[7, 1, 5, 3, 6, 4] $</p><p><img src="https://leetcode.com/media/original_images/121_profit_graph.png" alt="Profit Graph"> </p><p>可以知道，我们感兴趣的是峰谷之间的差值。则我们只需要找到当前值与之前最小值的最大差值即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxProfit</span><span class="params">(<span class="type">int</span> prices[])</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">minprice</span> <span class="operator">=</span> Integer.MAX_VALUE;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxprofit</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; prices.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[i] &lt; minprice)</span><br><span class="line">                minprice = prices[i];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (prices[i] - minprice &gt; maxprofit)</span><br><span class="line">                maxprofit = prices[i] - minprice;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxprofit;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.longest substring without repeating</title>
      <link href="/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/"/>
      <url>/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<p>Given a string, find the length of the <strong>longest substring</strong> without repeating characters.</p><p><strong>Examples:</strong></p><p>Given <code>&quot;abcabcbb&quot;</code>, the answer is <code>&quot;abc&quot;</code>, which the length is 3.</p><p>Given <code>&quot;bbbbb&quot;</code>, the answer is <code>&quot;b&quot;</code>, with the length of 1.</p><p>Given <code>&quot;pwwkew&quot;</code>, the answer is <code>&quot;wke&quot;</code>, with the length of 3. Note that the answer must be a <strong>substring</strong>, <code>&quot;pwke&quot;</code> is a <em>subsequence</em> and not a substring.</p><p>最长不重复子串</p><h2 id="Brute-Force"><a href="#Brute-Force" class="headerlink" title="Brute Force"></a>Brute Force</h2><p>有一个判断当前字符串为不重复的函数 <code>boolean allUnique(String substring)</code> .</p><p>然后两重循环求解，时间复杂度$O(n^3)$</p><h2 id="sliding-window"><a href="#sliding-window" class="headerlink" title="sliding window"></a>sliding window</h2><p>在暴力搜索中， <code>boolean allUnique(String substring)</code> 对于字串是从头开始搜索的。如果使用<code>set</code>的结构，可以将复杂度降到$O(n^2)$</p><h2 id="Sliding-Window-Optimized"><a href="#Sliding-Window-Optimized" class="headerlink" title="Sliding Window Optimized"></a>Sliding Window Optimized</h2><p>暴力搜索会产生很多不必要的操作，比如$s_{i,j}​$代表字符串 $i​$ 到 $j-1​$ 没有重复字串。则我们只需要判断第 $j​$ 个是否含于 $s_{i, j}​$ 即可。如果不包含，则 $s_{i,j}​$ 变为 $s_{i,j+1}​$ 。如果包含，则从包含的下标的下一位置开始(记录对应的位置)。</p><p>如下图所示：</p><img src="/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/1.jpg" class=""><p>下一坐标起始点即为2</p><p>这样就把时间复杂度降到了$O(n)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  :type s: str</span></span><br><span class="line"><span class="string">  :rtype: int</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  used = &#123;&#125;</span><br><span class="line">  max_length = start = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">    <span class="keyword">if</span> c <span class="keyword">in</span> used <span class="keyword">and</span> start &lt;= used[c]:</span><br><span class="line">      start = used[c] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:   </span><br><span class="line">      max_length = <span class="built_in">max</span>(max_length, i - start + <span class="number">1</span>)</span><br><span class="line">          </span><br><span class="line">    used[c] = i</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> max_length</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>93. Restore IP Address</title>
      <link href="/Restore_IP_Addresses/"/>
      <url>/Restore_IP_Addresses/</url>
      
        <content type="html"><![CDATA[<p>Given a string containing only digits, restore it by returning all possible valid IP address combinations.</p><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;25525511135&quot;</span><br><span class="line">Output: [&quot;255.255.11.135&quot;, &quot;255.255.111.35&quot;]</span><br></pre></td></tr></table></figure><p>给定一个字符串，输出所有可能的IP地址</p><h2 id="四分法"><a href="#四分法" class="headerlink" title="四分法"></a>四分法</h2><blockquote><p>三个点将字符串分成四段，验证每一段是否是有效的。我们只要控制这三个分割点就行了，注意约束条件有两个，一个是一段字符串不超过3个字母，另一个是控制好每段字符串最远结束的位置，比如第一个字符串最多延伸到倒数第4个字母</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">restoreIpAddresses</span>(<span class="params">s, res</span>):</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,i+<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> j &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(j, j+<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> k &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                s1 = s[<span class="number">0</span>:i]</span><br><span class="line">                s2 = s[i:j]</span><br><span class="line">                s3 = s[j:k]</span><br><span class="line">                s4 = s[k:]</span><br><span class="line">                <span class="keyword">if</span> isValid(s1) <span class="keyword">and</span> isValid(s2) <span class="keyword">and</span> isVAlid(s3) <span class="keyword">and</span> isValid(s4):</span><br><span class="line">                    res.append(s1+<span class="string">&#x27;.&#x27;</span>+s2+<span class="string">&#x27;.&#x27;</span>+s3+<span class="string">&#x27;.&#x27;</span>+s4)</span><br><span class="line">     <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="递归求解"><a href="#递归求解" class="headerlink" title="递归求解"></a>递归求解</h2><blockquote><p>因为<code>ip</code>地址为4段，每段数值在<code>0-255</code>之间。从头开始，判断。只要满足每段在此范围内，即可进入下一段的操作。如果最后一段以后，长度为<code>0</code>。则此答案为正确答案。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restoreIpAddresses</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># split the string into 4 part</span></span><br><span class="line">        res = []</span><br><span class="line">        self.getip(s, <span class="number">4</span>, <span class="string">&quot;&quot;</span>, res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getip</span>(<span class="params">self, s, k, out, result</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        s ： input str</span></span><br><span class="line"><span class="string">        out : result str</span></span><br><span class="line"><span class="string">        result : all possible result</span></span><br><span class="line"><span class="string">        k      : k-th part</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span>:</span><br><span class="line">                result.append(out)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 0-3 每段最长3</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="comment"># len(s) &gt;= i 保证足够分的</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(s) &gt;= i <span class="keyword">and</span> self._isValid(s[<span class="number">0</span>:i]):</span><br><span class="line">                    <span class="comment"># k==1 即最后一段了</span></span><br><span class="line">                    <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">                        self.getip(s[i:], k-<span class="number">1</span>, out+s[<span class="number">0</span>:i], result)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.getip(s[i:], k-<span class="number">1</span>, out+s[<span class="number">0</span>:i]+<span class="string">&#x27;.&#x27;</span>, result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_isValid</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(s) &gt; <span class="number">3</span> <span class="keyword">or</span> (<span class="built_in">len</span>(s)&gt;<span class="number">1</span> <span class="keyword">and</span> s[<span class="number">0</span>] ==<span class="string">&#x27;0&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> &lt;= <span class="built_in">int</span>(s) &lt;= <span class="number">255</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义分割中的度量标准</title>
      <link href="/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/"/>
      <url>/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="pixel-accuracy-PA，像素精度"><a href="#pixel-accuracy-PA，像素精度" class="headerlink" title="pixel accuracy (PA，像素精度)"></a>pixel accuracy (PA，像素精度)</h1><p>标记正确的像素占总像素的比例</p><p>$$<br>PA&#x3D;\frac{\sum_{i&#x3D;1}^kp_{ii}}{\sum_{i&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ij}}<br>$$</p><h1 id="mean-pixel-accuracy-MPA-均像素精度"><a href="#mean-pixel-accuracy-MPA-均像素精度" class="headerlink" title="mean pixel accuracy (MPA, 均像素精度)"></a>mean pixel accuracy (MPA, 均像素精度)</h1><p>计算每个类中被正确分类像素的比例，然后平均</p><p>$$<br>MPA&#x3D;\frac{1}{k+1}\sum_{i&#x3D;0}^{k}\frac{p_{ii}}{\sum_{j&#x3D;0}^kp_{ij}}<br>$$</p><h1 id="Mean-Intersection-over-Union-MIoU-均交并比"><a href="#Mean-Intersection-over-Union-MIoU-均交并比" class="headerlink" title="Mean Intersection over Union(MIoU, 均交并比)"></a>Mean Intersection over Union(MIoU, 均交并比)</h1><p>语义分割标准度量。计算两个集合的交集和并集之比。在semantic segmentation中，为真实值（ground truth）与预测值（predicted segmentation）的比值。这个比例变形为正真数（intersection）比上真正、假负、假正（并集）之和。在每个类上计算IoU，平均。</p><p>$$<br>MIoU&#x3D;\frac{1}{k+1}\sum_{i&#x3D;0}^k\frac{p_{ii}}{\sum_{j&#x3D;0}^k p_{ij}+\sum_{j&#x3D;0}^kp_{ji}-p_{ii}}<br>$$</p><img src="/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/1.jpg" class="" title="miou"><h1 id="Frequency-Weight-Intersection-over-Union-FWIoU-频权交并比"><a href="#Frequency-Weight-Intersection-over-Union-FWIoU-频权交并比" class="headerlink" title="Frequency Weight Intersection over Union(FWIoU, 频权交并比)"></a>Frequency Weight Intersection over Union(FWIoU, 频权交并比)</h1><p>MIoU的提升。根据每个类出现的频率设置权重</p><p>$$<br>FWIoU&#x3D;\frac{1}{\sum_{i&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ij}}\sum_{i&#x3D;0}^k\frac{p_{ii}}{\sum_{j&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ji}-p_{ii}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算label_true和label_pred对应相同的就在矩阵中对应坐标加1。a和b保存着各个像素的分的类别</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_fast_hist</span>(<span class="params">label_true, label_pred, n_class</span>):</span><br><span class="line">    <span class="comment">#过滤掉多余的分类</span></span><br><span class="line">    mask = (label_true &gt;= <span class="number">0</span>) &amp; (label_true &lt; n_class)</span><br><span class="line">    <span class="comment">#bincount用于统计在范围内出现的个数，即直方图，如果不够n^2个，</span></span><br><span class="line">    <span class="comment">#那就填充到n^2，这样可以reshpe为n*n的矩阵，正好表示分割图和正确标记图在相同</span></span><br><span class="line">    <span class="comment">#类别上像素出现的个数</span></span><br><span class="line">    hist = np.bincount(</span><br><span class="line">        n_class * label_true[mask].astype(<span class="built_in">int</span>) +</span><br><span class="line">        label_pred[mask], minlength=n_class ** <span class="number">2</span>).reshape(n_class, n_class)</span><br><span class="line">    <span class="keyword">return</span> hist</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">label_accuracy_score</span>(<span class="params">label_trues, label_preds, n_class</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns accuracy score evaluation result.</span></span><br><span class="line"><span class="string">      - overall accuracy</span></span><br><span class="line"><span class="string">      - mean accuracy</span></span><br><span class="line"><span class="string">      - mean IU</span></span><br><span class="line"><span class="string">      - fwavacc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    hist = np.zeros((n_class, n_class))</span><br><span class="line">    <span class="keyword">for</span> lt, lp <span class="keyword">in</span> <span class="built_in">zip</span>(label_trues, label_preds):</span><br><span class="line">        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)</span><br><span class="line">    acc = np.diag(hist).<span class="built_in">sum</span>() / hist.<span class="built_in">sum</span>()</span><br><span class="line">    acc_cls = np.diag(hist) / hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    acc_cls = np.nanmean(acc_cls)</span><br><span class="line">    iu = np.diag(hist) / (hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>) + hist.<span class="built_in">sum</span>(axis=<span class="number">0</span>) - np.diag(hist))</span><br><span class="line">    mean_iu = np.nanmean(iu)</span><br><span class="line">    freq = hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>) / hist.<span class="built_in">sum</span>()</span><br><span class="line">    fwavacc = (freq[freq &gt; <span class="number">0</span>] * iu[freq &gt; <span class="number">0</span>]).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> acc, acc_cls, mean_iu, fwavacc</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
