<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只特立独行的猪</title>
  
  <subtitle>既然我存在，就不能装作不存在</subtitle>
  <link href="https://wanger-sjtu.github.io/atom.xml" rel="self"/>
  
  <link href="https://wanger-sjtu.github.io/"/>
  <updated>2023-12-13T14:22:23.002Z</updated>
  <id>https://wanger-sjtu.github.io/</id>
  
  <author>
    <name>王二</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MLX 框架浅析</title>
    <link href="https://wanger-sjtu.github.io/mlx-apple/"/>
    <id>https://wanger-sjtu.github.io/mlx-apple/</id>
    <published>2023-12-13T14:14:43.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>最近Apple 新发布了一个MLX的DL框架，这是继 ML Compute 可用于在 Mac 上进行 TensorFlow 模型的训练，PyTorch 在 M1芯片后可使用 Metal Performance Shaders (MPS) 作为GPU 加速的 PyTorch 机器学习模型训练之后，进一步的尝试。</p><p>与MLX同时开源的还有数据读取的框架MLX-data，以及最近大模型相关的一些Example代码，MLX-examples</p><h2 id="MLX特性"><a href="#MLX特性" class="headerlink" title="MLX特性"></a>MLX特性</h2><ul><li><p><strong>Familiar APIs</strong>: MLX的python API 设计基本上与numpy和Pytorch对齐，基础的数据结构array设置可以隐式的转换为numpy的 Array。高层次的API mlx.nn 和mlx.optimizers则基本与pytorch对齐，使用方式也基本一致。C++ 的API与python基本一致。 这对算法开发人员来说上手的成本较低，历史代码也比较好迁移和继承。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlx.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> mlx.core <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure></li><li><p><strong>Composable function transformations</strong>:  JAX </p></li><li><p>MLX has composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.</p></li><li><p><strong>Lazy computation</strong>: 延迟计算，每部分的计算结果都是按需求值，也包括内存申请。</p></li><li><p><strong>Dynamic graph construction</strong>: 动态图构建，函数输入shape发生变化时，并不会触发编译，debug也很简单符合直觉。</p></li><li><p><strong>Multi-device</strong>: 多IP计算支持。当前支持CPU、GPU。因为ANE在Apple内部处于闭源的工具链，在这一现状没变化时，不会支持ANE。ref：<a href="https://github.com/ml-explore/mlx/issues/18#issuecomment-1846492294">link</a></p></li><li><p><strong>Unified memory</strong>: 最大的特点在于统一的内存模型，MLX 中的在共享内存上分配，跨IP计算时无需拷贝移动数据。</p></li></ul><h2 id="为什么还需要一个MLX"><a href="#为什么还需要一个MLX" class="headerlink" title="为什么还需要一个MLX"></a>为什么还需要一个MLX</h2><ul><li>Apple silicon first</li><li>Alternative Design and API</li><li>Simple, Flexible, Baggage-Free</li><li>More Exploration, More Diversity</li></ul><p><a href="https://github.com/ml-explore/mlx/issues/12">https://github.com/ml-explore/mlx/issues/12</a></p><h2 id="缺少哪些部分"><a href="#缺少哪些部分" class="headerlink" title="缺少哪些部分"></a>缺少哪些部分</h2><ol><li>图优化几近于无</li><li>序列化、反序列化功能</li><li>JIT 编译</li><li>INT量化</li></ol><h2 id="性能以及现状"><a href="#性能以及现状" class="headerlink" title="性能以及现状"></a>性能以及现状</h2><p>M2 Max</p><p>FP16 10 token&#x2F;s</p><img src="/mlx-apple/20231213220124.png" class=""><img src="/mlx-apple/20231213220841.png" class=""><h2 id="可能的-Roadmap"><a href="#可能的-Roadmap" class="headerlink" title="可能的 Roadmap"></a>可能的 Roadmap</h2><ol><li>有微调模型诉求的非算法人员。 比如lora、大模型的</li><li>基础模型开源、lora微调以后，通过core ml导出</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近Apple 新发布了一个MLX的DL框架，这是继 ML Compute 可用于在 Mac 上进行 TensorFlow 模型的训练，PyTorch 在 M1芯片后可使用 Metal Performance Shaders (MPS) 作为GPU 加速的 PyTorch </summary>
      
    
    
    
    <category term="竞分" scheme="https://wanger-sjtu.github.io/categories/%E7%AB%9E%E5%88%86/"/>
    
    
    <category term="竞分" scheme="https://wanger-sjtu.github.io/tags/%E7%AB%9E%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>从向量数据库到 ANN search</title>
    <link href="https://wanger-sjtu.github.io/ANN-algo/"/>
    <id>https://wanger-sjtu.github.io/ANN-algo/</id>
    <published>2023-09-10T03:54:04.000Z</published>
    <updated>2023-12-13T14:22:22.982Z</updated>
    
    <content type="html"><![CDATA[<p>LLM的模型的爆火，意外带动了向量数据库的热度。之前名不见经传的一些初创公司也突然备受追捧。最近在分析端侧LLM场景的时候也分析了相关的一些向量数据库的相关知识。</p><h1 id="GPT的缺陷"><a href="#GPT的缺陷" class="headerlink" title="GPT的缺陷"></a>GPT的缺陷</h1><p>chatgpt在对话过程中表现出的能力包括了一定的上下文检索能力。但这个能力是基于LLM本身的上下文理解能力完成的，但受限于多数模型是基于kv cache结构的记忆历史对话信息的，kv cache size是有限的，在长程记忆上就天然存在一些缺陷。另一方面，在跨对话的场景下，这些上下文信息也不能使用。如果在端侧作为一个数字助理的场景来看，这显然是不合格的。</p><p>不同模型对于 token 的限制也不同，gpt-4 是 32K tokens 的限制，而目前最大的 token 限制是 Claude 模型的 100K，这意味可以输入大约 75000 字的上下文给 GPT，这也意味着 GPT 直接理解一部《哈利波特》的所有内容并回答相关问题。</p><p>这时候就可能觉得，那我把上下文信息一起发给LLM模型不就可以了。这就到了向量数据库的场景范畴了。在处理用户输入的时候，先去通过向量查找得到一些相关信息，一起输入给LLM模型，这样就可以正确回答相关信息了。</p><img src="/ANN-algo/Embedding.png" class=""><h1 id="ANN-Search"><a href="#ANN-Search" class="headerlink" title="ANN Search"></a>ANN Search</h1><p>向量数据库说起来并不是一个新鲜的技术了，在统计机器学习时代，做KNN算法的时候就已经在研究相关的技术了。这里就简要的介绍一下原理和算法。</p><p>ANN搜索（Approximate nearest neighbor）, 本质上是在很多稠密向量中，迅速找到目标点的临近点，并认为这认为是相似的节点，主要用于图像检索、高维检索。这里隐含了一个假设，映射在同一向量空间且距离相近的点，具有相似的语义特征，距离越近越相关，反之关系越远。</p><p>当前 ANN 搜索的方法大都是对空间进行切分，可以迅速找到子空间，并与子空间的数据进行计算。方法主要有基于树的方法、哈希方法、矢量量化、基于图的方法。</p><h2 id="基于树的方法"><a href="#基于树的方法" class="headerlink" title="基于树的方法"></a>基于树的方法</h2><p>基于树的方法最经典的就是KD树了。</p><img src="/ANN-algo/kd-tree.png" class=""><p><strong>构建</strong><br>KD树构建的过程就是迭代二分空间的过程<br>经典算法：<br>选择方差最大的维度,计算中位数点，作为划分点，分为左右子树，迭代上述过程, 直到空间上的点小于阈值</p><p><strong>检索</strong><br>因为ANN这个任务并不像关系数据库中那样需要精准的结果，而是得到其中Top-K的候选结果返回。<br>KD树的检索过程其实就是一个二叉树的回溯搜索过程：</p><ol><li>根据目标p的坐标和kd树的结点向下进行搜索，如果树的结点root是以数据集的维度d以来切分的，那么如果p的维度d坐标值小于root，则走左子结点，否则走右子结点。</li><li>到达叶子结点时，将其标记为已访问。如果S中不足k个点，则将该结点加入到S中；否则如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点。</li><li>如果当前结点不是根节点，执行（a）；否则，结束算法。<br>  a.  回退到当前结点的父结点，此时的结点为当前结点（回退之后的结点）。将当前结点标记为已访问，执行（b）和（c）；如果当前结点已经被访过，再次执行（a）。<br>  b. 如果此时S中不足k个点，则将当前结点加入到S中；如果S中已有k个点，且当前结点与p点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。<br>  c. 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已有k个点，执行步骤3；如果该距离小于S中最远的距离或S中没有k个点，从当前结点的另一子节点开始执行步骤1；如果当前结点没有另一子结点，执行步骤3。</li></ol><h2 id="LSH"><a href="#LSH" class="headerlink" title="LSH"></a>LSH</h2><p>LSH即 local sensitive hash，局部敏感哈希。不同于sha256、MD5这种避免碰撞的函数，这里我们选取hash函数的时候希望语义相近的向量可以映射到同一个桶里。这里有一个前提在的：</p><blockquote><p>原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。</p></blockquote><img src="/ANN-algo/lsh.png" class=""><p><strong>构建</strong></p><ol><li>选取一组的LSH hash functions；</li><li>将所有数据经过 LSH hash function 哈希到相应的hash码，所有hash数据构成了一个hash table；</li></ol><p><strong>检索</strong></p><ol><li>将查询数据经过LSH hash function哈希得到相应的编码；</li><li>通过hamming 距离计算query数据与底库数据的距离，返回最近邻的数据</li></ol><p>当然也有其他的实现方案，这里不一一列举了。</p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>LSH这一类算法给了一个很好的加速方案，既然在原始向量空间内存在计算慢的问题，那么把向量数据映射到一个新的空间是不是就可以加速了。量化的算法就是这么想的，float型数据内存占用大，计算慢，那映射到整型数据就快了。</p><h3 id="PQ量化"><a href="#PQ量化" class="headerlink" title="PQ量化"></a>PQ量化</h3><p>PQ量化，即乘积量化，这里的乘积指的是笛卡尔积。<br>如图所示。我们有一个向量库，里面有N个向量，每个向量D维。简要介绍一下算法原理：</p><img src="/ANN-algo/PQ.png" class=""><p>PQ 量化一般分为三个步骤：</p><p><strong>Train</strong></p><ol><li>向量切分：将D维向量切分成M组子向量，每个子向量 $\frac{D}{M}$ 维。</li><li>聚类：分别在每一组子向量集合内，做Kmeans聚类，在每个子向量空间中，产生K个聚类中心。<ul><li>每个聚类中心就是一个 $\frac{D}{M}$ 维子向量，由一个id来表示，叫做clusterid。</li><li>一个子空间中所有的clusterid，构造了一个属于当前子空间的codebook。对于当前向量库，就有M个codebook。</li><li>这M个codebook所能表示的样本量级就是 $K^M$，也就是 M个codebook的笛卡尔积。</li></ul></li></ol><p><strong>建库</strong><br>对于子向量空间中的N个子向量样本，在完成Kmeans聚类之后，用这个聚类中心的clusterid来代表这个子向量。这就是构建底库的过程。</p><p>原本我们的向量库的大小为 $N\times D\times 32bit$，压缩后，clusterid按照8bit来算的话，那就是 $N\times M * 8bit $，相比压缩前少了很多。</p><p><strong>查找</strong><br>这里查找的过程存在两种方式：SDC和ADC</p><img src="/ANN-algo/SDC_ADC.png" class=""><p><strong>SDC</strong><br>S&#x3D;symmetric，对称的。如图symmetric case。图中x就是query检索向量，y就是向量库里面的向量(注意，y已经是量化过了的，就是上文中说的那个用数字id替代向量)。那么如何计算x与y的距离呢？</p><ul><li>首先，计算q(x)，拿到x对应的聚类中心；同样的，计算q(y)，拿到y对应的聚类中心。</li><li>q(x)和q(y)就是两个完整的子向量，我们计算这两个向量的距离，便是当前子空间下的距离。</li></ul><p>为什么名字叫symmetric呢？因为他俩都是用对应的聚类中心来计算距离，所以是对称的。<br>优点:</p><ul><li>两两聚类中心之间的距离，可以离线就计算好，在线直接查表，提升了在线query的效率。</li></ul><p>缺点：</p><ul><li>误差也比ADC来的大，因为有x和q(x)，y和q(y)两个量化误差。</li></ul><p><strong>ADC</strong><br>A&#x3D;asymmetric，不对称的。上文中讲了对称是因为SDC都用了对应的聚类中心。那么ADC，就只有向量库中的y使用了聚类中心，而query向量x没有。那么，计算距离的时候，计算的就是x和q(y)的距离了。ADC的精确度更高，因为只有y和q(y)这一个量化误差；当然必须要在线计算(x是用户请求带过来的)，计算速度不如SDC。</p><p><strong>计算过程</strong></p><p>将每一个子空间下的所有距离的平方相加再开根号，就是最终的X跟Y的距离了(就是使用每个子空间的向量距离进行了一次欧氏距离计算)。</p><h3 id="SQ量化"><a href="#SQ量化" class="headerlink" title="SQ量化"></a>SQ量化</h3><p>SQ量化，又叫标量量化。是按照向量维度统计min-max最值，然后将每一维向量归一化指定bit数整数的量化方式。</p><img src="/ANN-algo/SQ.jpg" class=""><p>基本原理如上图所示。</p><h2 id="IVF类方法"><a href="#IVF类方法" class="headerlink" title="IVF类方法"></a>IVF类方法</h2><p>上面讲的量化算法，仅仅并没有解决全库计算的问题，虽然数据上做了压缩，如果数据量一大，计算量还是很大。如果可以只计算最相关的一部分，是不是就可以进一步减少了呢。这就是IVF算法的思路。</p><img src="/ANN-algo/IVF.jpg" class=""><p>概括一下：<br>IVF主要利用倒排的思想保存每个聚类中心下的向量(id，vector)，每次查询向量的时候找到最近的几个中心，分别搜索这几个中心下的向量。通过减小搜索范围，大大提升搜索效率。</p><p>这里额外补充一点：</p><ul><li>IVF跟PQ结合的时候，IVF的聚类中心里面向量按照PQ量化的聚类时，我们将不会在样本上直接做PQ量化，而是对样本Y和聚类中心q(Y)的残差向量(向量减法，Y-q(Y))做PQ量化。</li></ul><h2 id="基于图的方法"><a href="#基于图的方法" class="headerlink" title="基于图的方法"></a>基于图的方法</h2><p>让我们重新回顾一下ANN这个任务：</p><img src="/ANN-algo/points.png" class=""><p>已有的向量数据库内容就是图中的点，ANN的任务就是对给定一个点找到距离最近的点。那么如果每个点都知道离自己近的点，那么是不是就可以沿着这个连接线找到相近的点了。这样就避免了与所有数据计算距离。这就是基于图算法出发点。</p><h3 id="NSW"><a href="#NSW" class="headerlink" title="NSW"></a>NSW</h3><p>NSW（navigate small world）,漫游小世界算法。对于每个新的传入元素，我们从结构中找到其最近邻居的集合（近似的 Delaunay 图， 就是上面的右图）。该集合连接到元素。随着越来越多的元素被插入到结构中，以前用作短距离边现在变成长距离边，形成可导航的小世界。</p><img src="/ANN-algo/NSW.png" class=""><p>圆（顶点）是度量空间中的数据，黑边是近似的 Delaunay 图，红边是用于对数缩放的长距离边。箭头显示从入口点到查询的贪心算法的示例路径（显示为绿色）。</p><p>图中的边有两个不同的目的：</p><ul><li>Short-range edges，用作贪婪搜索算法所需的近似 Delaunay 图。</li><li>Long-range edges，用于贪婪搜索的对数缩放。负责构造图形的可导航小世界（NSW）属性。</li></ul><p><strong>NSW查找步骤</strong></p><ol><li>随机选一个点作为初始进入点，建立空废弃表g和动态列表c，g是变长的列表，c是定长为s的列表（s&gt;m）,将初始点放入动态列表c（附上初始点和待查找q的距离信息），制作动态列表的影子列表c’。</li><li>对动态列表c中的所有点并行找出其“友点”，查看这些“友点”是否存储在废弃表g中，如果存在，则丢弃，如不存在，将这些 剩余“友点”记录在废弃列表g中（以免后续重复查找，走冤枉路）。</li><li>并行计算这些剩余“友点”距离待查找点q的距离，将这些点及其各自的距离信息放入c。</li><li>对动态列表c去重，然后按距离排序（升序），储存前s个点及其距离信息。</li><li>查看动态列表c和c’是否一样，如果一样，结束本次查找，返回动态列表中前m个结果。如果不一样，将c’的内容更新为c的 内容，执行第2步。</li></ol><p>NSW有什么问题呢：</p><ul><li>先插入的点构建的边，大都是长边；后插入的大都是短边。边的的连接关系不是很均衡。实际搜索的时候优化空间还比较大。</li></ul><h3 id="HNSW"><a href="#HNSW" class="headerlink" title="HNSW"></a>HNSW</h3><p>HNSW（Hierarchical Navigable Small World）是对 NSW 的一种改进。HNSW 借鉴了跳表的思想，根据连接的长度（距离）将连接划分为不同的层，然后就可以在多层图中进行搜索。在这种结构中，搜索从较长的连接（上层）开始，贪婪地遍历所有元素直到达到局部最小值，之后再切换到较短的连接（下层），然后重复该过程，如下图所示：</p><img src="/ANN-algo/HNSW.jpg" class=""><p>利用这种结构可以将原来 NSW 的多重对数（Polylogarithmic）计算复杂度降低至对数（Logarithmic）复杂度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LLM的模型的爆火，意外带动了向量数据库的热度。之前名不见经传的一些初创公司也突然备受追捧。最近在分析端侧LLM场景的时候也分析了相关的一些向量数据库的相关知识。&lt;/p&gt;
&lt;h1 id=&quot;GPT的缺陷&quot;&gt;&lt;a href=&quot;#GPT的缺陷&quot; class=&quot;headerlink</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="ANN" scheme="https://wanger-sjtu.github.io/tags/ANN/"/>
    
  </entry>
  
  <entry>
    <title>L1 data 缓存为什么一般只有32K或者64K</title>
    <link href="https://wanger-sjtu.github.io/L1-cache-size/"/>
    <id>https://wanger-sjtu.github.io/L1-cache-size/</id>
    <published>2023-09-09T14:16:32.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>L1 data缓存为什么一般只有32K或者64K？为什么不能更大一点？更大不是更好吗？</p><p>至少有这么两个原因。L1缓存因为会频繁被访问，所以优化目标是hit time，缓存size越大，hit time越长。另外现代CPU普遍采用virtually index physically tagged（VIPT）的L1缓存，所以L1数据缓存的大小实际上就是page size * associativity。譬如linux-x86上page size一般是4K，那L1d缓存每一个way就只能放4K大小的数据，想缓存总大小大一点就得增加associativity，譬如如果associativity是8，L1d就能是32K。但是associativity太大也会导致hit time上去。再譬如像Mac OS上page size是16K，L1d缓存就能做得更大一点。</p><p>注：实际VIPT做缓存查找时，虚拟地址的部分就是页表项，所以实际上虚拟地址部分就对应了page size。为什么跟associativity有关，因为associativity决定了一个页表项的虚拟地址可以映射到几个cacheline，为了最大化利用associativity也就是page size*associativity</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;L1 data缓存为什么一般只有32K或者64K？为什么不能更大一点？更大不是更好吗？&lt;/p&gt;
&lt;p&gt;至少有这么两个原因。L1缓存因为会频繁被访问，所以优化目标是hit time，缓存size越大，hit time越长。另外现代CPU普遍采用virtually index </summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="体系结构" scheme="https://wanger-sjtu.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    <category term="CPU" scheme="https://wanger-sjtu.github.io/tags/CPU/"/>
    
  </entry>
  
  <entry>
    <title>ndk std_thread 获取pid</title>
    <link href="https://wanger-sjtu.github.io/ndk-pid/"/>
    <id>https://wanger-sjtu.github.io/ndk-pid/</id>
    <published>2023-09-09T07:49:36.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>最近在解决tvm绑核问题时，发现android下绑核只有<code>sched_setaffinity</code>函数，这导致无法使用标准库中的<code>td::thread::native_handle_type thread</code> 进行绑核操作。虽然在ndk 21以上的版本提供了<code>pthread_gettid_np</code>函数获取线程相应的pid，但在较低版本中，还是没办法直接使用。</p><p>看下ndk 中 std 标准库上thread 的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_LIBCPP_TYPE_VIS</span> thread</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">__libcpp_thread_t</span> __t_;</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">typedef</span> __thread_id id;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="type">__libcpp_thread_t</span> native_handle_type;</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">pthread_t</span> <span class="type">__libcpp_thread_t</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">pthread_t</span>;</span><br></pre></td></tr></table></figure><p>上面可以看出，在ndk的实现中<code>native_handle_type</code> 等价于<code>pthread_t</code>, 再根据<code>pthread_gettid_np</code>的实现，可以发现 ，<code>pthread_t</code> 其实就是<code>pthread_internal_t</code>的地址。在<code>pthread_internal_t</code>中保存了线程的<code>tid</code> </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">pid_t</span> <span class="title">pthread_gettid_np</span><span class="params">(<span class="type">pthread_t</span> t)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> __pthread_internal_gettid(t, <span class="string">&quot;pthread_gettid_np&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> __pthread_internal_gettid(<span class="type">pthread_t</span> thread_id, <span class="type">const</span> <span class="type">char</span>* caller) &#123;</span><br><span class="line"><span class="type">pthread_internal_t</span>* thread = __pthread_internal_find(thread_id, caller);</span><br><span class="line"><span class="keyword">return</span> thread ? thread-&gt;tid : <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* prev;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_attr_t</span> attr;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> tid;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> allocated_on_heap;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_cond_t</span> join_cond;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> join_count;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>* return_value;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> internal_flags;</span><br><span class="line"></span><br><span class="line"><span class="type">__pthread_cleanup_t</span>* cleanup_stack;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>** tls; <span class="comment">/* thread-local storage area */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* The dynamic linker implements dlerror(3), which makes it hard for us to implement this</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* per-thread buffer by simply using malloc(3) and free(3).</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __BIONIC_DLERROR_BUFFER_SIZE 512</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> dlerror_buffer[__BIONIC_DLERROR_BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">&#125; <span class="type">pthread_internal_t</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在解决tvm绑核问题时，发现android下绑核只有&lt;code&gt;sched_setaffinity&lt;/code&gt;函数，这导致无法使用标准库中的&lt;code&gt;td::thread::native_handle_type thread&lt;/code&gt; 进行绑核操作。虽然在ndk</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
    <category term="CPP" scheme="https://wanger-sjtu.github.io/tags/CPP/"/>
    
    <category term="NDK" scheme="https://wanger-sjtu.github.io/tags/NDK/"/>
    
  </entry>
  
  <entry>
    <title>了解LLM——LLM&amp;&amp; SD 基本概念</title>
    <link href="https://wanger-sjtu.github.io/LLM_SD_Basic/"/>
    <id>https://wanger-sjtu.github.io/LLM_SD_Basic/</id>
    <published>2023-09-09T07:44:17.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Causual-LM"><a href="#Causual-LM" class="headerlink" title="Causual LM"></a>Causual LM</h2><p>这里以llama模型为例，通常在执行用户输入之前会有一个[[文章&#x2F;LM basic知识#Prefill]]的过程。然后根据用户promts 得到输出。</p><img src="/LLM_SD_Basic/2462804-20230609220042409-2086756901.png" class=""><h3 id="Perfix-LM"><a href="#Perfix-LM" class="headerlink" title="Perfix LM"></a>Perfix LM</h3><p>这里以GLM为例介绍，展示了基本的流程。</p><img src="/LLM_SD_Basic/2462804-20230609220056534-615175021.png" class=""><h2 id="prefix-LM和causal-LM的区别"><a href="#prefix-LM和causal-LM的区别" class="headerlink" title="prefix LM和causal LM的区别"></a>prefix LM和causal LM的区别</h2><p>attention mask不同，prefix LM的prefix部分的token互相能看到，causal LM严格遵守只有后面的token才能看到前面的token的规则。</p><h2 id="Prefill"><a href="#Prefill" class="headerlink" title="Prefill"></a>Prefill</h2><p>对于causual LM，在正式推理前，需要一部分前置输入，这个过程就是Prefill。主要目的是产生 kv cache</p><blockquote><p>the prefill stage which takes a prompt sequence to generate the key-value cache (KV cache) for each transformer layer of the LLM</p></blockquote><p><strong>prefill phase</strong></p><p>$$<br>x^i_K &#x3D; x^i · w^i_K; x^i_V &#x3D; x^i · w^i_V<br>$$</p><p>$$<br>x^i_Q &#x3D; x^i · w^i_Q<br>$$<br>$$<br>x^i_{Out} &#x3D; fSoftmax(\frac{x^i_Q (x^i_K)^T}{\sqrt{h}}) · x^i_V · w^i_O + x^i \</p><p>$$</p><p>$$<br>x^(i+1) &#x3D; frelu(x^i_{out} ·w_1)·w_2+x^i_{out}<br>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Causual-LM&quot;&gt;&lt;a href=&quot;#Causual-LM&quot; class=&quot;headerlink&quot; title=&quot;Causual LM&quot;&gt;&lt;/a&gt;Causual LM&lt;/h2&gt;&lt;p&gt;这里以llama模型为例，通常在执行用户输入之前会有一个[[文章&amp;#x2F;</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="LLM" scheme="https://wanger-sjtu.github.io/tags/LLM/"/>
    
    <category term="SD" scheme="https://wanger-sjtu.github.io/tags/SD/"/>
    
  </entry>
  
  <entry>
    <title>了解LLM —— LoRA</title>
    <link href="https://wanger-sjtu.github.io/LoRA/"/>
    <id>https://wanger-sjtu.github.io/LoRA/</id>
    <published>2023-09-09T07:41:16.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<ul><li>论文链接：<a href="https://arxiv.org/abs/2106.09685">link</a></li><li>code: <a href="https://github.com/microsoft/LoRA">github</a></li></ul><h2 id="什么是LoRA"><a href="#什么是LoRA" class="headerlink" title="什么是LoRA"></a>什么是LoRA</h2><p>LoRA，英文全称<strong>L</strong>ow-<strong>R</strong>ank <strong>A</strong>daptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT（参数高效性微调方法），这是微软的研究人员为了解决大语言模型微调而开发的一项技术。当然除了LoRA，参数高效性微调方法中实现最简单的方法还是Prompt tuning，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型，建议可从Prompt tuning开始学起。</p><p>LoRA的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果</p><img src="/LoRA/2462804-20230609214112382-1836386385.png" class=""><h2 id="why-works"><a href="#why-works" class="headerlink" title="why works"></a>why works</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>给定一个预训练模型$P_{\Phi}(y|x)$ , fine tuning 的过程可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi}(y_t|x,y&lt;t))}<br>$$<br>对于fine tuning前后参数变化，其实就是<br>$$<br>\Phi &#x3D; \Phi_0+\Delta \Phi<br>$$<br>这种方案有一个缺点，对不同的下游任务，$\Delta \Phi$ 需要训练，而且$\Delta \Phi$ 的参数维度跟$\Phi$一样大，如果是GPT-3的话参数量要175B了。<br>如果$\Delta \Phi$ 够小，只调整$\Delta \Phi$ 这部分参数是不是就可以减少资源使用了。所以问题可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi_0+\Delta \Phi(\Theta)}(y_t|x,y&lt;t))}<br>$$</p><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>对于NN模型来说，权重都是满秩的。但是对于特定任务来说，</p><blockquote><p>预训练的语言模型具有较低的“固有维度”，尽管随机投影到较小的子空间，但仍然可以有效地学习<br>the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace</p></blockquote><p>基于此，假设与训练的LLM也具有这个性质，finetuning 的过程中也有一个低秩的性质。</p><p>对于权重  $W_0 \in \mathbb{R}^{d\times k}$ ,权重更新可以表示为 $W_0+\Delta W$ ,考虑低秩分解，即为$W_0+\Delta W &#x3D; W_0+BA$ , 其中$B \in \mathbb{R}^{d\times r}$, $A\in \mathbb{R}^{r\times k}$ , $r &lt;&lt; \min(d,k)$<br>则：<br>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h3><ul><li>code <a href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py">link</a></li></ul><p><a href="https://spaces.ac.cn/archives/9590">梯度视角下的lora</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/microsoft/LoRA&quot;&gt;github&lt;/a&gt;&lt;/li&gt;</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="LLM" scheme="https://wanger-sjtu.github.io/tags/LLM/"/>
    
    <category term="Deep Learning" scheme="https://wanger-sjtu.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TVM－MLC LLM 调优方案</title>
    <link href="https://wanger-sjtu.github.io/mlc-llm/"/>
    <id>https://wanger-sjtu.github.io/mlc-llm/</id>
    <published>2023-09-09T07:39:15.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>LLM 等GPT大模型大火以后,TVM社区推出了自己的部署方案，支持Llama，Vicuna，Dolly等模型在iOS、Android、GPU、浏览器等平台上部署运行。</p><p><a href="https://github.com/mlc-ai/mlc-llm">https://github.com/mlc-ai/mlc-llm</a></p><p>本文在之前作者介绍的基础上,简要介绍一下mlc的调优部署方案。</p><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><p>在正式介绍TVM mlc.ai部署LLM方案之前，首先简要介绍一下当前主流LLM的一个工作流程。</p><img src="/mlc-llm/2462804-20230621222850510-751335110.png" class=""><blockquote><p>需要说明一点的是，上图中的prefill跟Decode指的的同一个模型，只是输入的shape存在差异。</p></blockquote><p>这里的示意图省略了很多，只是大致描述一下pipeline。<br>在处理用户输入时，此时长度大小是不能确定的，这时候是完全的是一个完全的动态shape的。但在decode过程中由于是token by token的，这时候网络中的中除了kv cache相关几个部分，其他大多数的操作都是固定shape的，就可以用已有的算法调优了。</p><h2 id="MLC-AI-部署调优方案"><a href="#MLC-AI-部署调优方案" class="headerlink" title="MLC.AI 部署调优方案"></a>MLC.AI 部署调优方案</h2><p>以下以RedPajama3B模型的tuning跟build过程介绍一下mlc的方案。</p><h3 id="pipeline-组成"><a href="#pipeline-组成" class="headerlink" title="pipeline 组成"></a>pipeline 组成</h3><p>在已经支持的几个模型里面均有<code>get_model</code> 这个函数，在这个函数里面会创建下面4个IRModel。</p><ul><li>encoding_func</li><li>decoding_func</li><li>create_kv_cache_func</li><li>create_softmax_func</li><li>create_metadata_func</li></ul><p><strong>encoding_func</strong><br>这对应了上图中的prefill过程，在每次用户输入后调用。由于用户输入的不确定性，所以这个过程基本上都是动态shape的，很难确定到底输入是多大，也不适合搜索调优。</p><p><strong>decoding_func</strong><br>这是上图中decode过程的一部分，因为这个过程是token by token的，在计算过程中大部分的计算是固定shape的。</p><p><strong>create kv cache func</strong><br>这里是直接调用的<code>relax.vm</code>中的函数，创建的是kv cache的存储相关。</p><p><strong>create softmax func</strong><br>这个也是解码过程的一部分，确切的说是采样过程中计算的一部分</p><p>** create_metadata_func **<br>模型的meta信息，比如<code>model_name</code>、<code>stop_tokens</code>等</p><h3 id="部署优化"><a href="#部署优化" class="headerlink" title="部署优化"></a>部署优化</h3><p>构建完以后，就进入到优化的阶段了。下面根据build.py过程描述一下过程。</p><ol><li><p>API构图构建了相关的模型，读取权重</p></li><li><p>量化</p></li><li><p>优化PASS</p><ol><li>FuseTransposeMatmul</li><li>FuseDecodeMatmulEwise</li><li>DeadCodeElimination</li><li>LiftTransformParams</li><li>split_transform_deploy_mod</li></ol></li><li><p>Codegen 生成代码</p><ol><li>DispatchTIROperatorAdreno&#x2F;DispatchTIROperator&#x2F;DefaultGPUSchedule 手动优化的sch</li><li>MetaScheduleApplyDatabase搜索的log生成固定shape的sch</li></ol></li></ol><h3 id="Tuning"><a href="#Tuning" class="headerlink" title="Tuning"></a>Tuning</h3><p>在MLC-LLM的代码仓里面已经提供了tuning的脚本，有一点需要先做一下，先调用build.py的文件，把静态shape的相关的函数分离出来。就得到了tuning文件中需要的<code>mod_tir_static.py</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LLM 等GPT大模型大火以后,TVM社区推出了自己的部署方案，支持Llama，Vicuna，Dolly等模型在iOS、Android、GPU、浏览器等平台上部署运行。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mlc-ai/mlc-llm&quot;&gt;h</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
    <category term="LLM" scheme="https://wanger-sjtu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>TVM 源码阅读PASS — VectorizeLoop</title>
    <link href="https://wanger-sjtu.github.io/VectorizeLoop/"/>
    <id>https://wanger-sjtu.github.io/VectorizeLoop/</id>
    <published>2023-09-09T06:30:56.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>VectorizeLoop这个PASS就是对标记为<code>ForKind::kVectorized</code>的<code>For</code>循环做向量化处理，并对For循环中的语句涉及到的变量，替换为<code>Ramp</code>，以便于在Codegen的过程中生成相关的向量化运算的指令。</p><p>VectorizeLoop这个PASS的入口函数如下，只有在打开<code>enable_vectorize=true</code>的情况下载才会被启用，否则<code>VectorizeSkipper</code>会把<code>ForKind::kVectorized</code>的<code>For</code>循环替换为普通循环。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Pass <span class="title">VectorizeLoop</span><span class="params">(<span class="type">bool</span> enable_vectorize)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) &#123;</span><br><span class="line">    <span class="keyword">auto</span>* n = f.<span class="built_in">CopyOnWrite</span>();</span><br><span class="line">    <span class="keyword">if</span> (enable_vectorize) &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">LoopVectorizer</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">VectorizeSkipper</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> f;</span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">CreatePrimFuncPass</span>(pass_func, <span class="number">0</span>, <span class="string">&quot;tir.VectorizeLoop&quot;</span>, &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面就以UT中的几个例子，介绍一下源码实现。</p><h2 id="vectorize-loop"><a href="#vectorize-loop" class="headerlink" title="vectorize_loop"></a>vectorize_loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dtype = <span class="string">&quot;int64&quot;</span></span><br><span class="line">n = te.var(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">ib = tvm.tir.ir_builder.create()</span><br><span class="line">A = ib.pointer(<span class="string">&quot;float32&quot;</span>, name=<span class="string">&quot;A&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ib.for_range(<span class="number">0</span>, n) <span class="keyword">as</span> i:</span><br><span class="line"> <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, <span class="number">4</span>, kind=<span class="string">&quot;vectorize&quot;</span>) <span class="keyword">as</span> j:</span><br><span class="line">     A[i*<span class="number">4</span>+j] += tvm.tir.const(<span class="number">1</span>, A.dtype)</span><br><span class="line">stmt = ib.get()</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(stmt.body, tvm.tir.For)</span><br><span class="line">mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([A, n], stmt))</span><br><span class="line">stmt = tvm.tir.transform.VectorizeLoop()(mod)[<span class="string">&quot;main&quot;</span>].body</span><br></pre></td></tr></table></figure><p>上面的这个代码完成的是，向量加法，长度为4n的向量A，对每个元素+1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  vectorized (j, <span class="number">0</span>, <span class="number">4</span>) &#123;</span><br><span class="line">    A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + 1f)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># after</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] = (A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] + x4(1f))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到在经过<code>VectorizeLoop</code>的PASS以后，内层的循环消掉了，替换成为了一个Ramp的向量指令，这个在CPU中会被替换为SIMD指令（neon，AVX等）</p><h4 id="PASS流程"><a href="#PASS流程" class="headerlink" title="PASS流程"></a>PASS流程</h4><p>在向量化的处理的PASS中是在LoopVectorizer中处理的，处理For循环部分。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoopVectorizer</span> : <span class="keyword">public</span> StmtMutator &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">Stmt <span class="title">VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> <span class="keyword">final</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (op-&gt;kind == ForKind::kVectorized) &#123;</span><br><span class="line">      <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">      <span class="keyword">auto</span>* extent_as_int = op-&gt;extent.<span class="built_in">as</span>&lt;IntImmNode&gt;();</span><br><span class="line">      <span class="keyword">if</span> (!extent_as_int || extent_as_int-&gt;value &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Failed to vectorize loop with extent &quot;</span> &lt;&lt; op-&gt;extent;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">Vectorizer</span>(op-&gt;loop_var, <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(extent_as_int-&gt;value))(op-&gt;body);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> StmtMutator::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当遇到需要向量化的节点时，首先记录循环变量和范围，这个在后续替换相应的Load和Store操作为Ramp时用到。然后就到了Vectorizer部分，遍历For循环体，修改相应的stmt。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Vectorizer</span>(Var var, <span class="type">int</span> var_lanes) : <span class="built_in">var_</span>(var), <span class="built_in">var_lanes_</span>(var_lanes) &#123;</span><br><span class="line">    ramp_ = <span class="built_in">Ramp</span>(<span class="number">0</span>, <span class="number">1</span>, var_lanes);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Vectorizer中对不同的<code>PrimExpr</code>、<code>Stmt</code>做了重载。这里不逐一介绍，就以上面的向量加计算，介绍一下用到的函数以及流程。</p><p>首先看一下这里的上面sch的For的循环内的计算逻辑：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + <span class="number">1f</span>)</span><br></pre></td></tr></table></figure><p>因为TVM中，Stmt的表达可以视为一个DSL的语言，访问的时候也是按照深度优先的策略遍历的AST，这里把上面的计算过程简单表示为一个AST的语法树，然后再分析一下流程中调用的各个函数是如何处理的。</p><img src="/VectorizeLoop/2462804-20230624144328795-2055285024.png" class=""><p>从上面的AST的示意图可以看出来，对于上面的sch，依次访问了<code>BufferStoreNode</code>、<code>Add</code> <code>Mul</code>、<code>BufferLoadNode</code> 等。这里就以这几个Node的处理介绍一下向量化的过程。</p><p>所谓向量化的过程就是把这个标记为<code>kVectorized</code>的标量循环操作映射到向量化的操作，对于上面的例子来说就是把所有关于<code>j</code>的访问映射为RampNode，以便于后续处理可以正确生成相应的指令。</p><h5 id="BufferStoreNode"><a href="#BufferStoreNode" class="headerlink" title="BufferStoreNode"></a>BufferStoreNode</h5><p><code>BufferStoreNode</code>中有三部分：</p><ul><li>buffer——写入的buffer</li><li>value——待写入的值或者表达式</li><li>indices——写入buffer的坐标<br>这里的目的就是修改<code>value</code>和<code>indices</code>中的内容。<br>对于<code>indices</code>，是在这里完成的。最终通过<code>MapHelper</code>依次访问了<code>indices</code>的表达式。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> fmutate = [<span class="keyword">this</span>](<span class="type">const</span> PrimExpr&amp; index) &#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(index); &#125;;</span><br><span class="line">Array&lt;PrimExpr&gt; indices = op-&gt;indices.<span class="built_in">Map</span>(fmutate);</span><br></pre></td></tr></table></figure><p>对于<code>value</code> 则是直接遍历。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr value = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;value);</span><br></pre></td></tr></table></figure><h5 id="AddNode"><a href="#AddNode" class="headerlink" title="AddNode"></a>AddNode</h5><p>对于<code>AddNode</code>和<code>SubNode</code> 都会走到<code>AddSubVec</code>这个模板函数。<br>这个函数里面首先会遍历左右表达式，</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"> <span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; b_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a, b_ramp-&gt;base),</span><br><span class="line"> <span class="built_in">fcompute</span>(<span class="built_in">make_zero</span>(b_ramp-&gt;stride.<span class="built_in">dtype</span>()), b_ramp-&gt;stride), b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; a_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a_ramp-&gt;base, b), a_ramp-&gt;stride, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">fcompute</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br></pre></td></tr></table></figure><p>如果遍历之后没有变化，就直接返回了。而对于这里的我们需要计算的是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((i*<span class="number">4</span>) + j)</span><br></pre></td></tr></table></figure><p><code>j</code> 是需要向量化的坐标。<code>i*4</code> 是没有变化的。遍历以后<code>a</code>没变化，<code>b</code>变成了<code>T.Ramp(0, 1, 4)</code> 这时候<code>lanes=4</code>，会走到第一个<code>if</code>分支，返回的是新构造的<code>RampNode</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T.Ramp(i * 4, 1, 4)</span><br></pre></td></tr></table></figure><p>其他的分支也类似。比如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A[i * <span class="number">4</span> + j] + T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// --- after ---</span></span><br><span class="line">A[i * <span class="number">4</span>:i * <span class="number">4</span> + <span class="number">4</span>]   T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里会把a、b broadcast为一个向量再做计算。</p><h5 id="VarNode"><a href="#VarNode" class="headerlink" title="VarNode"></a>VarNode</h5><p>对于这里的VarNode判断就比较简单了，如果匹配到的是需要向量化的变量，就返回构造函数中构造的<code>RampNode</code>，否则就返回。其他的操作，暂时略过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Var var = <span class="built_in">GetRef</span>&lt;Var&gt;(op);</span><br><span class="line"><span class="keyword">if</span> (var.<span class="built_in">same_as</span>(var_)) &#123;</span><br><span class="line"> <span class="keyword">return</span> ramp_;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"> <span class="keyword">return</span> std::<span class="built_in">move</span>(var);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="MulNode"><a href="#MulNode" class="headerlink" title="MulNode"></a>MulNode</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a_ramp &amp;&amp; b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(b &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(a_ramp-&gt;base * b, a_ramp-&gt;stride * b, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b_ramp &amp;&amp; a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(a &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(b_ramp-&gt;base * a, b_ramp-&gt;stride * a, b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">Mul</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">BinaryVec</span>&lt;Mul&gt;(op);</span><br></pre></td></tr></table></figure><p>这里的处理逻辑与Add基本一致。只是在计算RampNode的时候有点区别。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;VectorizeLoop这个PASS就是对标记为&lt;code&gt;ForKind::kVectorized&lt;/code&gt;的&lt;code&gt;For&lt;/code&gt;循环做向量化处理，并对For循环中的语句涉及到的变量，替换为&lt;code&gt;Ramp&lt;/code&gt;，以便于在Codegen的过程</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
    <category term="CPP" scheme="https://wanger-sjtu.github.io/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>SVE特性以及寄存器</title>
    <link href="https://wanger-sjtu.github.io/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/"/>
    <id>https://wanger-sjtu.github.io/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/</id>
    <published>2023-09-05T14:18:30.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>SVE对比NEON有几个新增的地方。</p><ol><li>变长的向量</li><li>支持Gather-load &amp;&amp; Scatter-store</li></ol><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/gather.png" class=""><ol start="3"><li><p>可以由P寄存器控制向量通道的计算</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/Pvector.png" class=""></li><li><p>由软件控制的向量切分。</p><ol><li>基于First Fault 寄存器完成的，加载不合法内存页的时候，会有记录 <img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/20230905222847.png" class=""></li></ol></li><li><p>扩展浮点和位运算的水平缩减</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/140339805.png" class=""></li></ol><h2 id="SVE-寄存器"><a href="#SVE-寄存器" class="headerlink" title="SVE 寄存器"></a>SVE 寄存器</h2><ul><li>Scalable vector registers<br><code>Z0-Z15</code>, 支持double、float、float16，int64、int32、int16、int8<br>向量寄存器长度128-2048bit可变，具体取决于SoC厂商确定，当前手机上上商用的由联发科的天玑9200，长度是128bit，这部分与NEON共用。</li><li>Scalable predicate registers<br>谓词寄存器，<ul><li>P0-P7 控制的数据加载、存取、计算</li><li>P8-P15做循环控制</li><li>FFR ： 用来软件推测的FFR寄存器<img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/SVE.png" class=""></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;SVE对比NEON有几个新增的地方。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;变长的向量&lt;/li&gt;
&lt;li&gt;支持Gather-load &amp;amp;&amp;amp; Scatter-store&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&quot;/SVE%E7%89%B9%E6%80%A7%E4%BB%A</summary>
      
    
    
    
    
    <category term="体系结构" scheme="https://wanger-sjtu.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>tir_to_llvm_ir</title>
    <link href="https://wanger-sjtu.github.io/tir-to-llvm-ir/"/>
    <id>https://wanger-sjtu.github.io/tir-to-llvm-ir/</id>
    <published>2023-09-03T12:57:55.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>TVM在编译过程中，经历了</p><pre class="mermaid">graph LR  A[3rd IR] --> B[Relay IR]  B --> C[TIR]  C --> D[LLVM IR]  C -->E[Source]</pre><p>这一系列的过程。其中在生成cpu、rocm、nvptx、hexagon等平台的相关代码的时候，会先由TVM的<code>TIR</code>转换为<code>LLVM IR</code>,在后续由LLVM生成相关的机器码。</p><p>这一步是由<code>tvm::codegen::Build</code>调用转换的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">runtime::Module <span class="title">Build</span><span class="params">(IRModule mod, Target target)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (transform::PassContext::<span class="built_in">Current</span>()</span><br><span class="line">          -&gt;<span class="built_in">GetConfig</span>&lt;Bool&gt;(<span class="string">&quot;tir.disable_assert&quot;</span>, <span class="built_in">Bool</span>(<span class="literal">false</span>))</span><br><span class="line">          .<span class="built_in">value</span>()) &#123;</span><br><span class="line">    mod = tir::transform::<span class="built_in">SkipAssert</span>()(mod);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> target_attr_map = tvm::TargetKind::<span class="built_in">GetAttrMap</span>&lt;FTVMTIRToRuntime&gt;(<span class="string">&quot;TIRToRuntime&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (target_attr_map.<span class="built_in">count</span>(target-&gt;kind)) &#123;</span><br><span class="line">    <span class="keyword">return</span> target_attr_map[target-&gt;kind](mod, target);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the build function.</span></span><br><span class="line">  std::string build_f_name = <span class="string">&quot;target.build.&quot;</span> + target-&gt;kind-&gt;name;</span><br><span class="line">  <span class="type">const</span> PackedFunc* bf = runtime::Registry::<span class="built_in">Get</span>(build_f_name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(bf != <span class="literal">nullptr</span>) &lt;&lt; build_f_name &lt;&lt; <span class="string">&quot; is not enabled&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> (*bf)(mod, target);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在LLVM相关的target时候，这里的<code>build_f_name</code>就是<code>target.build.llvm</code></p><p>这时候会走到</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;target.build.llvm&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](IRModule mod, Target target) -&gt; runtime::Module &#123;</span><br><span class="line">      <span class="keyword">auto</span> n = <span class="built_in">make_object</span>&lt;LLVMModuleNode&gt;();</span><br><span class="line">      n-&gt;<span class="built_in">Init</span>(mod, target);</span><br><span class="line">      <span class="keyword">return</span> runtime::<span class="built_in">Module</span>(n);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>在<code>Init</code>函数中创建codegen的具体类：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LLVMModuleNode::Init</span><span class="params">(<span class="type">const</span> IRModule&amp; mod, <span class="type">const</span> Target&amp; target)</span> </span>&#123;</span><br><span class="line">  llvm_instance_ = std::<span class="built_in">make_unique</span>&lt;LLVMInstance&gt;();</span><br><span class="line">  <span class="function">With&lt;LLVMTarget&gt; <span class="title">llvm_target</span><span class="params">(*llvm_instance_, target)</span></span>;</span><br><span class="line">  llvm::TargetMachine* tm = llvm_target-&gt;<span class="built_in">GetOrCreateTargetMachine</span>();</span><br><span class="line">  <span class="comment">// 这里会根据target得到不同的codegen的实现类</span></span><br><span class="line">  std::unique_ptr&lt;CodeGenLLVM&gt; cg = CodeGenLLVM::<span class="built_in">Create</span>(llvm_target.<span class="built_in">get</span>());</span><br><span class="line"></span><br><span class="line">  std::string entry_func;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  skip crt/cpp systemlib options</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> kv : mod-&gt;functions) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!kv.second-&gt;<span class="built_in">IsInstance</span>&lt;PrimFuncNode&gt;()) &#123;</span><br><span class="line">      <span class="comment">// (@jroesch): we relax constraints here, Relay functions will just be ignored.</span></span><br><span class="line">      <span class="built_in">DLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Can only lower IR Module with PrimFuncs, but got &quot;</span> &lt;&lt; kv.second-&gt;<span class="built_in">GetTypeKey</span>();</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> f = <span class="built_in">Downcast</span>&lt;PrimFunc&gt;(kv.second);</span><br><span class="line">    <span class="keyword">auto</span> global_symbol = f-&gt;<span class="built_in">GetAttr</span>&lt;String&gt;(tvm::attr::kGlobalSymbol);</span><br><span class="line">    <span class="type">bool</span> is_entry_func = f-&gt;<span class="built_in">HasNonzeroAttr</span>(tir::attr::kIsEntryFunc);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (global_symbol) &#123;</span><br><span class="line">      function_names_.<span class="built_in">push_back</span>(global_symbol.<span class="built_in">value</span>());</span><br><span class="line">      <span class="keyword">if</span> (is_entry_func) &#123;</span><br><span class="line">        entry_func = global_symbol.<span class="built_in">value</span>();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 初始化CodeGenLLVM, 会产生builder_, module_等llvm 中codegen需要的基础数据结构</span></span><br><span class="line">  cg-&gt;<span class="built_in">Init</span>(<span class="string">&quot;TVMMod&quot;</span>, llvm_target.<span class="built_in">get</span>(), system_lib_prefix, </span><br><span class="line">             system_lib_prefix.<span class="built_in">defined</span>(),</span><br><span class="line">           target_c_runtime);</span><br><span class="line">  cg-&gt;<span class="built_in">SetFastMathFlags</span>(llvm_target-&gt;<span class="built_in">GetFastMathFlags</span>());</span><br><span class="line">    <span class="comment">// 核心功能,tir 转化为llvm ir就在此</span></span><br><span class="line">  cg-&gt;<span class="built_in">AddFunctionsOrdered</span>(mod-&gt;functions.<span class="built_in">begin</span>(), mod-&gt;functions.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">if</span> (entry_func.<span class="built_in">length</span>() != <span class="number">0</span>) &#123;</span><br><span class="line">    cg-&gt;<span class="built_in">AddMainFunction</span>(entry_func);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  module_owning_ptr_ = cg-&gt;<span class="built_in">Finish</span>();</span><br><span class="line">  module_ = module_owning_ptr_.<span class="built_in">get</span>();</span><br><span class="line">  llvm_target-&gt;<span class="built_in">SetTargetMetadata</span>(module_);</span><br><span class="line">  module_-&gt;<span class="built_in">addModuleFlag</span>(llvm::Module::Override, <span class="string">&quot;Debug Info Version&quot;</span>,</span><br><span class="line">                         llvm::DEBUG_METADATA_VERSION);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TVM在编译过程中，经历了&lt;/p&gt;
&lt;pre class=&quot;mermaid&quot;&gt;graph LR
  A[3rd IR] --&gt; B[Relay IR]
  B --&gt; C[TIR]
  C --&gt; D[LLVM IR]
  C --&gt;E[Source]&lt;/pre&gt;

&lt;p&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Linux_shell中提取文件名和路径</title>
    <link href="https://wanger-sjtu.github.io/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/"/>
    <id>https://wanger-sjtu.github.io/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/</id>
    <published>2023-09-03T12:55:35.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<p>首先假设我的文件全称：&#x2F;home&#x2F;luna&#x2F;Desktop&#x2F;Software&#x2F;softHLA&#x2F;HLAreporter.v103&#x2F;HLAreporter.sh.</p><h1 id="获取文件名"><a href="#获取文件名" class="headerlink" title="获取文件名"></a>获取文件名</h1><h1 id="使用-，-str"><a href="#使用-，-str" class="headerlink" title="使用${}，${str##*/}"></a>使用<code>$&#123;&#125;，$&#123;str##*/&#125;</code></h1><p>这个命令的作用就是去掉变量str从左边算起的最后一个&#x2F;字符及其左边的内容，返回的值是从左边算起的最后一个&#x2F;（不含该字符）的右边的所有内容，例子很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=<span class="variable">$&#123;str##*/&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$file</span></span><br><span class="line">HLAreporter.sh  <span class="comment">## 运行结果</span></span><br></pre></td></tr></table></figure><h1 id="使用awk语句"><a href="#使用awk语句" class="headerlink" title="使用awk语句"></a>使用awk语句</h1><p>因为在ubuntu下面，路径都是以&#x2F;为隔开的，那么我们就以&#x2F;为分隔符，然后把最后部分打印，赋值，例子如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=`echo $str | awk -F &quot;/&quot; &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="使用官方函数basename"><a href="#使用官方函数basename" class="headerlink" title="使用官方函数basename"></a>使用官方函数basename</h1><p>bash shell本身提供了basename命令，可以直接获取路径名最后的文件名，实现代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$(basename $str)</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="后缀和文件名分开"><a href="#后缀和文件名分开" class="headerlink" title="后缀和文件名分开"></a>后缀和文件名分开</h1><p>使用${}<br>在这里分别使用&#x2F;和.作为分隔符来进行处理，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%.*&#125;</span><br><span class="line">suffix=$&#123;file##*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.sh, HLAreporter, sh</span><br><span class="line"></span><br><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103.tar.gz</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%%.*&#125;</span><br><span class="line">suffix=$&#123;file#*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.v103.tar.gz, HLAreporter, v103.tar.gz</span><br></pre></td></tr></table></figure><p>用的是Shell的参数扩展(Parameter Extension)功能，解释如下：</p><p><code>$&#123;str##*/&#125;</code>: 从左边开始删除str中最大匹配(longest matching pattern) <em>&#x2F; 的字符串<br><code>$&#123;str%/*&#125;</code>：从右边开始删除str中最小匹配(shortest matching pattern) &#x2F;</em> 的字符串<br><code>$&#123;file##*.&#125;</code>：从左边开始删除file中最大匹配(longest matching pattern) <em>. 的字符串<br><code>$&#123;file%.*&#125;</code>：从右边开始删除file中最小匹配(shortest matching pattern) .</em> 的字符串<br><code>$&#123;file%%.*&#125;</code>：从右边开始删除file中最大匹配(longest matching pattern) .* 的字符串<br><code>$&#123;file#*.&#125;</code>：从左边开始删除file中小匹配(shortest matching pattern) *. 的字符串<br><code>#</code>：表示从左边算起第一个<br><code>%</code>：表示从右边算起第一个<br><code>##</code>：表示从左边算起最后一个<br><code>%%</code>：表示从右边算起最后一个<br>换句话来说，<code>＃</code>总是表示左边算起，<code>％</code>总是表示右边算起。<br>参数扩展有多种形式，在shell编程中可以用作参数的拼接，字符串的替换，参数列表截取，变量初值等操作，这里不再详述，请参考右边的功能列表和官方文档.</p><h1 id="参数扩展功能列表"><a href="#参数扩展功能列表" class="headerlink" title="参数扩展功能列表"></a>参数扩展功能列表</h1><p>参数形式扩展后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x&#123;y,z&#125;xy xz</span><br><span class="line">$&#123;x&#125;&#123;y, z&#125;$&#123;x&#125;y $&#123;x&#125;z</span><br><span class="line">$&#123;x&#125;&#123;y, $z&#125;$&#123;x&#125;y $&#123;x&#125;$&#123;z&#125;</span><br><span class="line">$&#123;param#pattern&#125;从param前面删除pattern的最小匹配</span><br><span class="line">$&#123;param##pattern&#125;从param前面删除pattern的最大匹配</span><br><span class="line">$&#123;param%pattern&#125;从param后面删除pattern的最小匹配</span><br><span class="line">$&#123;param%%pattern&#125;从param后面删除pattern的最大匹配</span><br><span class="line">$&#123;param/pattern/string&#125;从param中用string替换pattern的第一次匹配，string可为空</span><br><span class="line">$&#123;param//pattern/string&#125;从param中用string替换pattern的所有匹配，string可为空</span><br><span class="line">$&#123;param:3:2&#125;截取$param中索引3开始的2个字符</span><br><span class="line">$&#123;param:3&#125;截取$param中索引3至末尾的字符</span><br><span class="line">$&#123;@:3:2&#125;截取参数列表$@中第3个开始的2个参数</span><br><span class="line">$&#123;param:-word&#125;若$param为空或未设置，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:+word&#125;若$param为非空，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:=word&#125;若$param为空或为设置，则参数式返回word，同时$param设置为word</span><br><span class="line">$&#123;param:?message&#125;若$param为空或为设置，则输出错误信息message，若包含空白符，则需引号</span><br></pre></td></tr></table></figure><p>获取路径名<br>使用${}，${str%&#x2F;*}<br>去掉变量var从右边算起的第一个’&#x2F;’字符及其右边的内容，返回从右边算起的第一个’&#x2F;’（不含该字符）的左边的内容。使用例子及结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">path=$&#123;str%/*&#125;</span><br><span class="line">echo $path</span><br><span class="line">/home/luna/Desktop/Software/softHLA/HLAreporter.v103</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;首先假设我的文件全称：&amp;#x2F;home&amp;#x2F;luna&amp;#x2F;Desktop&amp;#x2F;Software&amp;#x2F;softHLA&amp;#x2F;HLAreporter.v103&amp;#x2F;HLAreporter.sh.&lt;/p&gt;
&lt;h1 id=&quot;获取文件名&quot;&gt;&lt;a </summary>
      
    
    
    
    
    <category term="shell" scheme="https://wanger-sjtu.github.io/tags/shell/"/>
    
    <category term="linux" scheme="https://wanger-sjtu.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>packfunc</title>
    <link href="https://wanger-sjtu.github.io/packfunc/"/>
    <id>https://wanger-sjtu.github.io/packfunc/</id>
    <published>2023-09-03T12:52:39.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>为实现多种语言支持，需要满足以下几点：</p><ul><li>部署：编译结果可以从<code>python/javascript/c++</code>调用。</li><li>Debug: 在python中定义一个函数，在编译函数中调用。</li><li>链接：编写驱动程序以调用设备特定代码（如CUDA），可以在编译的host侧调用</li><li>原型：python侧定义IR PASS，并从C++后端调用该代码</li><li>接口暴露：c++后端代码暴露到python侧</li><li>实验：将编译的函数运送到嵌入式设备，可以直接在嵌入式设备上运行</li></ul><p>tvm希望在任何一个语言中定义的函数，可以在其他的语言中都可以调用。同样希望runtime尽可能的轻量化，以方便在嵌入式设备上部署。</p><h1 id="PackedFunc"><a href="#PackedFunc" class="headerlink" title="PackedFunc"></a>PackedFunc</h1><p><code>PackedFunc</code>是解决上述问题的一个优雅的方案。一个<code>PackedFunc</code>对象对应着一个函数调用，即使定义与调用分散在不同语言之间也可以满足。下面展示一个C++的例子。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tvm/runtime/packed_func.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MyAdd</span><span class="params">(TVMArgs args, TVMRetValue* rv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// automatically convert arguments to desired type.</span></span><br><span class="line">  <span class="type">int</span> a = args[<span class="number">0</span>];</span><br><span class="line">  <span class="type">int</span> b = args[<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// automatically assign value return to rv</span></span><br><span class="line">  *rv = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CallPacked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  PackedFunc myadd = <span class="built_in">PackedFunc</span>(MyAdd);</span><br><span class="line">  <span class="comment">// get back 3</span></span><br><span class="line">  <span class="type">int</span> c = <span class="built_in">myadd</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的例子中，定义了一个<code>MyAdd</code>的<code>PackedFunc</code>，接受两个参数，<code>args</code>表示输入参数， <code>rv</code>表示返回值。这个参数是类型无关的(type-erased)，这意味着函数签名中对输入输出参数的类型没有限制。这样，当调用这个函数的时候， 从栈上获取输入参数（TVMArgs），通过TVMRetValue返回函数返回值。</p><p>通过C++的模板技巧，可以像正常函数一样调用<code>PackedFunc</code>。由于类型无关的特性，可以在像python这样的动态类型的语言中调用<code>PackedFunc</code>，而无需插入额外其他的胶水代码。下面展示了<code>PackedFunc</code> 的注册及其在python端的调用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// register a global packed function in c++</span></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>(MyAdd);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line">myadd = tvm.get_global_func(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line"><span class="comment"># prints 3</span></span><br><span class="line"><span class="built_in">print</span>(myadd(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>多数的<code>PackedFunc</code>技巧依赖于<code>TVMArgs</code>和<code>TVMRetValue</code>，我们限制其中的参数类型，下面是主要用的类型：</p><ul><li>int, float and string</li><li>PackedFunc itself</li><li>Module for compiled modules</li><li>DLTensor* for tensor object exchange</li><li>TVM Object to represent any object in IR</li></ul><p>这个限制，使得实现及其简单而且无需序列化操作。虽然增加了限制，但对于DL开发来说，大多数场景下仅仅需要传递<code>DLTensor</code>和数字就够了。</p><p>既然<code>PackedFunc</code>可以将另外的PackedFunc作为函数参数，那就可以在python与c++之间传递函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">callback</span>(<span class="params">msg</span>):</span><br><span class="line">  <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PackedFunc</span></span><br><span class="line">f = tvm.convert(callback)</span><br><span class="line">callhello = tvm.get_global_func(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line"><span class="comment"># prints hello world</span></span><br><span class="line">callhello(f)</span><br></pre></td></tr></table></figure><p>TVM 提供了极简的C API，使得将PackedFunc可以方便地嵌入到其他的语言中。除python外，还支持java、JavaScript。</p><p>PackFunction不仅用于tvm编译器中，同样也用于开发的技术栈中。在tvm中所有的PASS函数都通过PackedFunc暴露给前端的。编译结果同样是通过PackedFunc打包的。</p><p>为了保证runtime尽可能的小，runtime中隔离了IR对象的支持。这使得runtime大小只有200~600k，具体的大小取决于平台驱动部分。</p><p>PackedFunc带来的调用开销很小，仅仅是通过栈传递了一些参数对象，只要不通过它包装较小的函数，就是OK的。总之，PackedFunc是tvm中通用的胶水代码，支持了tvm的编译部署。</p><p>额外的部分：</p><h2 id="c-注册，python调用"><a href="#c-注册，python调用" class="headerlink" title="c++ 注册，python调用"></a>c++ 注册，python调用</h2><p>上文中介绍注册时，使用到了一个C++宏<code>TVM_REGISTER_GLOBAL</code>，这里介绍中间是如何链接起来的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//展开就是</span></span><br><span class="line"><span class="built_in">TVM_STR_CONCAT</span>(TVM_FUNC_REG_VAR_DEF, __COUNTER__) = ::tvm::runtime::Registry::<span class="built_in">Register</span>(<span class="string">&quot;callhello&quot;</span>).<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>这里的<code>::tvm::runtime::Registry::Register</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Registry&amp; <span class="title">Registry::Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> can_override)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  Manager* m = Manager::<span class="built_in">Global</span>();<span class="comment">//这是个静态对象，Manager持有一个map来记录注册对象</span></span><br><span class="line">  <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(m-&gt;mutex)</span></span>;</span><br><span class="line">  <span class="keyword">if</span> (m-&gt;fmap.<span class="built_in">count</span>(name)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(can_override) &lt;&lt; <span class="string">&quot;Global PackedFunc &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is already registered&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Registry* r = <span class="keyword">new</span> <span class="built_in">Registry</span>();</span><br><span class="line">  r-&gt;name_ = name;</span><br><span class="line">  m-&gt;fmap[name] = r;</span><br><span class="line">  <span class="keyword">return</span> *r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面看下Registry的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Registry for global function */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Registry</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">//设置函数体</span></span><br><span class="line">  <span class="function">TVM_DLL Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc f)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc::FType f)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">PackedFunc</span>(f));</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//给一个任意函数，萃取函数签名</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> FLambda&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_typed</span><span class="params">(FLambda f)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> FType = <span class="keyword">typename</span> detail::function_signature&lt;FLambda&gt;::FType;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;FType&gt;(std::<span class="built_in">move</span>(f), name_).<span class="built_in">packed</span>());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//给一个类成员函数、返回值、参数，使用lambda包装</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...))</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...) <span class="type">const</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](<span class="type">const</span> T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(<span class="type">const</span> T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...) <span class="type">const</span>) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      <span class="type">const</span> TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> Registry&amp; <span class="title">Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> <span class="keyword">override</span> = <span class="literal">false</span>)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">bool</span> <span class="title">Remove</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">const</span> PackedFunc* <span class="title">Get</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>; </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> std::vector&lt;std::string&gt; <span class="title">ListNames</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  std::string name_;</span><br><span class="line">  PackedFunc func_;</span><br><span class="line">  <span class="keyword">friend</span> <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上面注册以后是在一个全局对象中，下一部就看python侧如何调用的。</p><p>python端最终会调用到 <code>_get_global_func</code>函数，具体实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_func</span>(<span class="params">name, allow_missing=<span class="literal">False</span></span>):</span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    check_call(_LIB.TVMFuncGetGlobal(c_str(name), ctypes.byref(handle)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> handle.value:</span><br><span class="line">        <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> allow_missing:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot find global function %s&quot;</span> % name)</span><br></pre></td></tr></table></figure><p>进而会调用到<code>TVMFuncGetGlobal</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncGetGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle* out)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  <span class="type">const</span> tvm::runtime::PackedFunc* fp = tvm::runtime::Registry::<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="keyword">if</span> (fp != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    *out = <span class="keyword">new</span> tvm::runtime::<span class="built_in">PackedFunc</span>(*fp);  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *out = <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里既可以发现<code>tvm::runtime::Registry::Get(name)</code>来查找相关注册函数的。</p><h2 id="python注册，c-调用"><a href="#python注册，c-调用" class="headerlink" title="python注册，c++ 调用"></a>python注册，c++ 调用</h2><p>如下面的函数，通过装饰器注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_func(<span class="params"><span class="string">&quot;relay.backend.lower_call&quot;</span></span>)</span></span><br></pre></td></tr></table></figure><p>在c++中调用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">auto</span> flower_call = tvm::runtime::Registry::<span class="built_in">Get</span>(<span class="string">&quot;relay.backend.lower_call&quot;</span>);</span><br></pre></td></tr></table></figure><p>下面介绍以下python的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_func</span>(<span class="params">func_name, f=<span class="literal">None</span>, override=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">callable</span>(func_name):</span><br><span class="line">        f = func_name</span><br><span class="line">        func_name = f.__name__</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(func_name, <span class="built_in">str</span>):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;expect string function name&quot;</span>)</span><br><span class="line"></span><br><span class="line">    ioverride = ctypes.c_int(override)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">myf</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;internal register function&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(myf, PackedFuncBase):</span><br><span class="line">            myf = convert_to_tvm_func(myf) <span class="comment">#转化为packfunc</span></span><br><span class="line">        <span class="comment">#注册</span></span><br><span class="line">        check_call(_LIB.TVMFuncRegisterGlobal(c_str(func_name), myf.handle, ioverride))</span><br><span class="line">        <span class="keyword">return</span> myf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        <span class="keyword">return</span> register(f)</span><br><span class="line">    <span class="keyword">return</span> register</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to_tvm_func</span>(<span class="params">pyfunc</span>):</span><br><span class="line">    local_pyfunc = pyfunc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cfun</span>(<span class="params">args, type_codes, num_args, ret, _</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; ctypes function &quot;&quot;&quot;</span></span><br><span class="line">        num_args = num_args.value <span class="keyword">if</span> <span class="built_in">isinstance</span>(num_args, ctypes.c_int) <span class="keyword">else</span> num_args</span><br><span class="line">        pyargs = (C_TO_PY_ARG_SWITCH[type_codes[i]](args[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_args))</span><br><span class="line">        <span class="comment"># pylint: disable=broad-except</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rv = local_pyfunc(*pyargs)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            msg = traceback.format_exc()</span><br><span class="line">            msg = py2cerror(msg)</span><br><span class="line">            _LIB.TVMAPISetLastError(c_str(msg))</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rv <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(rv, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;PackedFunction can only support one return value&quot;</span>)</span><br><span class="line">            temp_args = []</span><br><span class="line">            values, tcodes, _ = _make_tvm_args((rv,), temp_args)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(ret, TVMRetValueHandle):</span><br><span class="line">                ret = TVMRetValueHandle(ret)</span><br><span class="line">            <span class="keyword">if</span> _LIB.TVMCFuncSetReturn(ret, values, tcodes, ctypes.c_int(<span class="number">1</span>)) != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">            _ = temp_args</span><br><span class="line">            _ = rv</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    f = TVMPackedCFunc(cfun)</span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> We will need to use python-api to increase ref count of the f</span></span><br><span class="line">    <span class="comment"># TVM_FREE_PYOBJ will be called after it is no longer needed.</span></span><br><span class="line">    pyobj = ctypes.py_object(f)</span><br><span class="line">    ctypes.pythonapi.Py_IncRef(pyobj)</span><br><span class="line">    <span class="keyword">if</span> _LIB.TVMFuncCreateFromCFunc(f, pyobj, TVM_FREE_PYOBJ, ctypes.byref(handle)) != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">    <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncRegisterGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle f, <span class="type">int</span> <span class="keyword">override</span>)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  tvm::runtime::Registry::<span class="built_in">Register</span>(name, <span class="keyword">override</span> != <span class="number">0</span>)</span><br><span class="line">      .<span class="built_in">set_body</span>(*<span class="built_in">static_cast</span>&lt;tvm::runtime::PackedFunc*&gt;(f));</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为实现多种语言支持，需要满足以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部署：编译结果可以从&lt;code&gt;python/javascript/c++&lt;/code&gt;调用。&lt;/li&gt;
&lt;li&gt;Debug: 在python中定义一个函数，在编译函数中调用。&lt;/li&gt;
&lt;li&gt;链接：编写驱</summary>
      
    
    
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
    <category term="CPP" scheme="https://wanger-sjtu.github.io/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>【TVM教程】 自定义relay算子</title>
    <link href="https://wanger-sjtu.github.io/tvm-custom-op/"/>
    <id>https://wanger-sjtu.github.io/tvm-custom-op/</id>
    <published>2023-08-09T07:52:02.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>本文为tvm 教程的翻译版。这部分介绍了如何在tvm中添加新的relay算子，具体的是以一个累乘（cumprod）算子为例进行介绍。</p><p>新增relay算子基本是下面几个步骤：</p><ol><li>定义新增算子的属性节点（Attribute Node），声明在编译时已知的固定参数</li><li>为新增算子编写类型关系，以集成到relay的类型系统中</li><li>使用C++ <code>RELAY_REGISTER_OP</code> 宏，为新增算子注册生命参数数量、类型、提示信息</li><li>算子的compute</li><li>注册算子的compute、schedule</li><li>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</li><li>将上面的 Python API hook 封装成简洁的调用方式</li><li>为新的relay 算子编写测试</li></ol><h2 id="新增算子的属性节点"><a href="#新增算子的属性节点" class="headerlink" title="新增算子的属性节点"></a>新增算子的属性节点</h2><p>算子属性是编译期已知的参数。以卷积算子为例，strid、dilation就属于卷积算子的属性。这部分算子属性定义在<code>include/tvm/relay/attrs/</code>下。<br>最终来说，我们期望定义有如下属性说明的算子，其python侧的接口如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Numpy style cumprod op. Return the cumulative inclusive product of the elements along</span></span><br><span class="line"><span class="string">    a given axis.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    data : relay.Expr</span></span><br><span class="line"><span class="string">        The input data to the operator.</span></span><br><span class="line"><span class="string">    axis : int, optional</span></span><br><span class="line"><span class="string">        Axis along which the cumulative product is computed. The default (None) is to compute</span></span><br><span class="line"><span class="string">        the cumprod over the flattened array.</span></span><br><span class="line"><span class="string">    dtype : string, optional</span></span><br><span class="line"><span class="string">        Type of the returned array and of the accumulator in which the elements are multiplied.</span></span><br><span class="line"><span class="string">        If dtype is not specified, it defaults to the dtype of data.</span></span><br><span class="line"><span class="string">    exclusive : bool, optional</span></span><br><span class="line"><span class="string">        If true will return exclusive product in which the first element is not</span></span><br><span class="line"><span class="string">        included. In other terms, if true, the j-th output element would be</span></span><br><span class="line"><span class="string">        the product of the first (j-1) elements. Otherwise, it would be the product of</span></span><br><span class="line"><span class="string">        the first j elements. The product of zero elements will be 1.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    result : relay.Expr</span></span><br><span class="line"><span class="string">        The result has the same size as data, and the same shape as data if axis is not None.</span></span><br><span class="line"><span class="string">        If axis is None, the result is a 1-d array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><code>.cumsum()</code>有类似的接口。</p><p>因此，在定义我们新增算子（cumprod）属性时，需要选择操作的轴、数据类型和排他性作为属性字段。<code>include/tvm/relay/attrs/transform.h</code></p><p>ScanopAttrs 这里定义了对累加、累乘等操作的属性定义。对累乘来说就不需要额外定义了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Attributes used in cumsum and cumprod operator */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ScanopAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;ScanopAttrs&gt; &#123;</span><br><span class="line">  Integer axis;</span><br><span class="line">  DataType dtype;</span><br><span class="line">  Bool exclusive = <span class="built_in">Bool</span>(<span class="literal">false</span>);</span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(ScanopAttrs, <span class="string">&quot;relay.attrs.ScanopAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to operate over&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;Integer&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(dtype).<span class="built_in">describe</span>(<span class="string">&quot;Output data type&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;DataType&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(exclusive)</span><br><span class="line">        .<span class="built_in">describe</span>(<span class="string">&quot;The first element is not included&quot;</span>)</span><br><span class="line">        .<span class="built_in">set_default</span>(<span class="built_in">Bool</span>(<span class="literal">false</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>但是如果是其他的算子，需要自己定义相应的属性节点。如<code>BiasAdd</code>就需要单独定义</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">BiasAddAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;BiasAddAttrs&gt; &#123;</span><br><span class="line">  <span class="type">int</span> axis;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(BiasAddAttrs, <span class="string">&quot;relay.attrs.BiasAddAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to add the bias&quot;</span>).<span class="built_in">set_default</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="类型推导-Type-Relation"><a href="#类型推导-Type-Relation" class="headerlink" title="类型推导 Type Relation"></a>类型推导 Type Relation</h2><p>为了算子注册的灵活性以及relay算子有更好的泛化能力，relay算子通过输入输出之间的类型关系来实例化。<br>这些关系通过一系列的函数进行表示（这些函数是以算子输入输出类型为参数，返回满足类型关系的输入输出列表）， 、、？<br>这包括编译期已知的输入输出的shape 信息<br>本质上，算子relation除了推到输出类型外，还能够强制指定类型规则（检查输入类型）。</p><p>然后就是官网教程的给的例子<code>src/relay/op/tensor/transform.cc</code>。这里依旧是<code>ScanopAttrs</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_NODE_TYPE</span>(ScanopAttrs);</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ScanopRel</span><span class="params">(<span class="type">const</span> Array&lt;Type&gt;&amp; types, <span class="type">int</span> num_inputs, <span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> TypeReporter&amp; reporter)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// types: [data, output]</span></span><br><span class="line">    <span class="built_in">ICHECK_EQ</span>(types.<span class="built_in">size</span>(), <span class="number">2</span>) &lt;&lt; <span class="string">&quot;Expects two types, one for the input and another for the output&quot;</span>;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* data = types[<span class="number">0</span>].<span class="built_in">as</span>&lt;TensorTypeNode&gt;(); <span class="comment">//输入的tensor信息</span></span><br><span class="line">    <span class="keyword">if</span> (data == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="built_in">ICHECK</span>(types[<span class="number">0</span>].<span class="built_in">as</span>&lt;IncompleteTypeNode&gt;())</span><br><span class="line">        &lt;&lt; <span class="string">&quot;Scanop: expect input type to be TensorType but get &quot;</span> &lt;&lt; types[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* param = attrs.<span class="built_in">as</span>&lt;ScanopAttrs&gt;(); <span class="comment">//算子属性</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> dtype = param-&gt;dtype;</span><br><span class="line">    <span class="keyword">if</span> (dtype.<span class="built_in">is_void</span>()) &#123;</span><br><span class="line">        dtype = data-&gt;dtype;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//设置输出tensor属性</span></span><br><span class="line">    <span class="keyword">if</span> (param-&gt;axis.<span class="built_in">defined</span>()) &#123;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(data-&gt;shape, dtype));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">auto</span> prod = data-&gt;shape[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">1</span>; i &lt; data-&gt;shape.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            prod = prod * data-&gt;shape[i];</span><br><span class="line">        &#125;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(&#123;prod&#125;, dtype));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的例子可以看出 XXXOpRel 的主要功能是根据输入类型确定输出类型。特别的， <code>TensorType</code>的构造函数可以看出，需要指定输出的shape信息，这部分主要目的就是infershape和infertype。</p><h2 id="关联算子的参数数目、属性"><a href="#关联算子的参数数目、属性" class="headerlink" title="关联算子的参数数目、属性"></a>关联算子的参数数目、属性</h2><p>这一步的操作，为自定义算子注册算子名称，通过调用接口增加算子注释。这里需要用到C++的宏<code>RELAY_REGISTER_OP</code><br>涉及的参数含义如下：</p><ul><li>Arity（参数数量）</li><li>位置参数的名称和描述</li><li>支持级别（1 表示内部实现;较高的数字表示较少的内部支持或外部支持的算子）</li><li>算子的类型关系</li><li>优化算子时有用的其他注释。<br><code>src/relay/op/tensor/transform.cc</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumsum&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative sum of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumsum&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);</span><br><span class="line"></span><br><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumprod&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative product of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumprod&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);<span class="comment">// 不融合</span></span><br></pre></td></tr></table></figure><p>注：<code>set_attr&lt;TOpPattern&gt;(&quot;TOpPattern&quot;, );</code>此处表示融合算子是，跳过此算子。</p><h2 id="编写的算子compute"><a href="#编写的算子compute" class="headerlink" title="编写的算子compute"></a>编写的算子compute</h2><p>到现在，我们已经实现了算子的接口，但是还缺少算子的compute逻辑。这部分内容超出了这个教程的范围。<br>对于<code>cumprod</code>和<code>cumsum</code>，CPU实现可以参考<code>python/tvm/topi/scan.py</code>，GPU实现可以参考<code>python/tvm/topi/cuda/scan.py</code>。<br>这里这两个的实现，直接在TIR基础上实现得到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scanop</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    binop: <span class="type">Callable</span>[[<span class="string">&quot;tvm.Expr&quot;</span>, <span class="string">&quot;tvm.Expr&quot;</span>], <span class="string">&quot;tvm.Expr&quot;</span>],</span></span><br><span class="line"><span class="params">    identity_value: <span class="string">&quot;tvm.Expr&quot;</span>,</span></span><br><span class="line"><span class="params">    op_name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> dtype == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        dtype = data.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exclusive <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        exclusive = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maybe_cast</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">if</span> dtype != data.dtype:</span><br><span class="line">            <span class="keyword">return</span> cast(x, dtype)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    axis_mul_before = <span class="number">1</span></span><br><span class="line">    axis_mul_after = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> axis <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        axis = <span class="number">0</span></span><br><span class="line">        cumsum_axis_len = prod(data.shape)</span><br><span class="line">        shape = (cumsum_axis_len,)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(axis, <span class="built_in">int</span>):</span><br><span class="line">            axis = get_const_int(axis)</span><br><span class="line"></span><br><span class="line">        shape = data.shape</span><br><span class="line">        cumsum_axis_len = shape[axis]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> axis &lt; <span class="number">0</span>:</span><br><span class="line">            axis = <span class="built_in">len</span>(shape) + axis</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(shape, <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; axis:</span><br><span class="line">                axis_mul_before *= value</span><br><span class="line">            <span class="keyword">elif</span> i &gt; axis:</span><br><span class="line">                axis_mul_after *= value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_ir</span>(<span class="params">data_buf, out_buf</span>):</span><br><span class="line">        ib = ir_builder.create()</span><br><span class="line">        data_buf = ib.buffer_ptr(data_buf)</span><br><span class="line">        out_buf = ib.buffer_ptr(out_buf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, axis_mul_before * axis_mul_after, <span class="string">&quot;fused&quot;</span>, kind=<span class="string">&quot;parallel&quot;</span>) <span class="keyword">as</span> fused:</span><br><span class="line">            i = fused // axis_mul_after</span><br><span class="line">            j = fused % axis_mul_after</span><br><span class="line">            base_idx = i * cumsum_axis_len * axis_mul_after + j</span><br><span class="line">            <span class="keyword">if</span> exclusive:</span><br><span class="line">                out_buf[base_idx] = cast(identity_value, dtype)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_buf[base_idx] = maybe_cast(data_buf[base_idx])</span><br><span class="line">            <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, cumsum_axis_len - <span class="number">1</span>, <span class="string">&quot;_k&quot;</span>) <span class="keyword">as</span> _k:</span><br><span class="line">                k = _k + <span class="number">1</span></span><br><span class="line">                cur_idx = base_idx + k * axis_mul_after</span><br><span class="line">                prev_idx = base_idx + (k - <span class="number">1</span>) * axis_mul_after</span><br><span class="line">                <span class="keyword">if</span> exclusive:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[prev_idx]))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[cur_idx]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ib.get()</span><br><span class="line"></span><br><span class="line">    out_buf = decl_buffer(shape, dtype, <span class="string">&quot;out_buf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> extern(</span><br><span class="line">        [shape],</span><br><span class="line">        [data],</span><br><span class="line">        <span class="keyword">lambda</span> ins, outs: gen_ir(ins[<span class="number">0</span>], outs[<span class="number">0</span>]),</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        out_buffers=[out_buf],</span><br><span class="line">        name=op_name,</span><br><span class="line">        tag=op_name,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">    <span class="keyword">return</span> scanop(</span><br><span class="line">        data=data,</span><br><span class="line">        binop=generic.add,</span><br><span class="line">        identity_value=<span class="number">0</span>,</span><br><span class="line">        op_name=<span class="string">&quot;cumsum_generic&quot;</span>,</span><br><span class="line">        axis=axis,</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        exclusive=exclusive,</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="注册算子的compute、schedule"><a href="#注册算子的compute、schedule" class="headerlink" title="注册算子的compute、schedule"></a>注册算子的compute、schedule</h2><p>在实现了算子compute逻辑以后，需要与我们实现的算子接口绑定在一起。在TVM中，这就需要不仅实现算子的compute接口，还要实现对应的schedule。而strategy就是对compute选择合适的schedule。<br>以卷积算子为例，算子编译时，可能会发现这是一个depthwise卷积，进而去选择更高效的schedule实现。</p><p>一般情况下，仅仅考虑CPU、GPU版本即可。<br><code>python/tvm/relay/op/strategy/generic.py</code> <code>python/tvm/relay/op/strategy/cuda.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wrap_compute_scanop</span>(<span class="params">topi_compute</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Wrap scanop style topi compute&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_scanop</span>(<span class="params">attrs, inputs, _</span>):</span><br><span class="line">        <span class="keyword">return</span> [topi_compute(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line">    <span class="keyword">return</span> _compute_scanop</span><br><span class="line"></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="meta">@cumsum_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum cuda strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cuda.cumsum),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_scan),</span><br><span class="line">        name=<span class="string">&quot;cumsum.cuda&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>对于每个strategy，与对应的compute、schedule通过<code>add_implementation</code>关联起来。<br>这里的shape_func时对输入时动态shape厂家推导有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cumsum</span></span><br><span class="line"><span class="meta">@_reg.register_compute(<span class="params"><span class="string">&quot;cumsum&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cumsum</span>(<span class="params">attrs, inputs, output_type</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute definition of cumsum&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [topi.cumsum(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line"></span><br><span class="line">_reg.register_strategy(<span class="string">&quot;cumsum&quot;</span>, strategy.cumsum_strategy)</span><br><span class="line">_reg.register_shape_func(<span class="string">&quot;cumsum&quot;</span>, <span class="literal">False</span>, elemwise_shape_func)</span><br></pre></td></tr></table></figure><h2 id="定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook"><a href="#定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook" class="headerlink" title="定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook"></a>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</h2><p>现在我们有一个可以调用的relay算子了，下一步就是如何通过relay call node调用。这就需要实现一个函数，传递相应的参数给对于的relay算子，并且返回对应算子的Call Node（这个算子最终在Relay表达式的AST里面）。</p><p>当前不支持直接调用 Attrs和参数。所以需要在函数中构造对应的AttrsNode，传递给对应的Call Node。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Expr <span class="title">MakeCumsum</span><span class="params">(Expr data, Integer axis, DataType dtype, Bool exclusive)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;ScanopAttrs&gt;();</span><br><span class="line">    attrs-&gt;dtype = dtype;</span><br><span class="line">    attrs-&gt;axis = axis;</span><br><span class="line">    attrs-&gt;exclusive = exclusive;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(<span class="string">&quot;cumsum&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op._make.cumsum&quot;</span>).<span class="built_in">set_body_typed</span>(MakeCumsum);</span><br></pre></td></tr></table></figure><p><code>Op::Get(&quot;cumsum&quot;)</code>的实现如下。具体怎么注册到<code>OpRegistry</code>的，TODO</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里看一下Call的实现，实际上是得到一个call Node，里面保存了算子及其属性信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Call::<span class="built_in">Call</span>(Expr op, Array&lt;Expr&gt; args, Attrs attrs, Array&lt;Type&gt; type_args, Span span) &#123;</span><br><span class="line">  ObjectPtr&lt;CallNode&gt; n = <span class="built_in">make_object</span>&lt;CallNode&gt;();</span><br><span class="line">  n-&gt;op = std::<span class="built_in">move</span>(op);</span><br><span class="line">  n-&gt;args = std::<span class="built_in">move</span>(args);</span><br><span class="line">  n-&gt;attrs = std::<span class="built_in">move</span>(attrs);</span><br><span class="line">  n-&gt;type_args = std::<span class="built_in">move</span>(type_args);</span><br><span class="line">  n-&gt;span = std::<span class="built_in">move</span>(span);</span><br><span class="line">  data_ = std::<span class="built_in">move</span>(n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Op::Get</code> <code>src/relay/op/tensor/transform.cc</code></p><p>相关接口暴露到python侧，是通过<code>.TVM_REGISTER_GLOBAL</code> <code>MakeCumsum</code> <code>MakeCumprod</code> <code>relay.op._make.cumsum(...)</code> <code>relay.op._make.cumsum(...)</code>实现的。</p><p>细节TODO</p><h2 id="将上面的-Python-API-hook-封装成简洁的调用方式"><a href="#将上面的-Python-API-hook-封装成简洁的调用方式" class="headerlink" title="将上面的 Python API hook 封装成简洁的调用方式"></a>将上面的 Python API hook 封装成简洁的调用方式</h2><p>为更方便的使用，通常的做法是构造单独的函数，因此最好封装成更简洁的python接口。教程的例子，定义在<br><code>TVM_REGISTER_GLOBAL</code> <code>python/tvm/relay/op/transform.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumsum(data, axis, dtype, exclusive)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumprod(data, axis, dtype, exclusive)</span><br></pre></td></tr></table></figure><p>特别的，如果不定参数的，需要包成Tuple形式进行传递。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Concatenate the input tensors along the zero axis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    args: list of Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    tensor: The concatenated tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tup = <span class="type">Tuple</span>(<span class="built_in">list</span>(args))</span><br><span class="line">    <span class="keyword">return</span> _make.concat(tup)</span><br></pre></td></tr></table></figure><h2 id="为新的relay-算子编写测试"><a href="#为新的relay-算子编写测试" class="headerlink" title="为新的relay 算子编写测试"></a>为新的relay 算子编写测试</h2><p>参考 <code>tests/python/relay/test_op_level3.py</code></p><p>ref: <a href="https://tvm.apache.org/docs/dev/relay_add_op.html">https://tvm.apache.org/docs/dev/relay_add_op.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文为tvm 教程的翻译版。这部分介绍了如何在tvm中添加新的relay算子，具体的是以一个累乘（cumprod）算子为例进行介绍。&lt;/p&gt;
&lt;p&gt;新增relay算子基本是下面几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定义新增算子的属性节点（Attribute Node），声明</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>【TVM模型编译】2. relay算子构造</title>
    <link href="https://wanger-sjtu.github.io/tvm-relay-op-construct/"/>
    <id>https://wanger-sjtu.github.io/tvm-relay-op-construct/</id>
    <published>2023-08-09T07:51:21.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>从TVM的官方<a href="https://www.cnblogs.com/wanger-sjtu/p/15046641.html">Tutorial</a>里面，介绍了如何新增自定义算子。(这是我翻译的)</p><p>之前的文章讲到了<a href="../tvm-onnx-to-relay">onnx 算子转换到Relay IR</a>的过程<br>下面以Conv2d算子介绍，编译过程中 Relay IR是如何被调用的。</p><h2 id="relay-算子调用"><a href="#relay-算子调用" class="headerlink" title="relay 算子调用"></a>relay 算子调用</h2><p>上面的<code>get_relay_op</code>实际上是查找所有 relay ir算子，其代码在<code>python/tvm/relay/frontend/common.py</code>中的<code>get_relay_op</code>。继续以conv卷积算子为例介绍。上文所述的转换算子中，有下面的语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate <span class="keyword">in</span> (_op, _op.nn, _op.image, _op.vision, _op.contrib):</span><br><span class="line">    op = <span class="built_in">getattr</span>(candidate, op_name, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> op <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>对于<code>conv2d</code>算子，在<code>_op.nn</code>中，找到conv2d实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data,</span></span><br><span class="line"><span class="params">    weight,</span></span><br><span class="line"><span class="params">    strides=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    padding=(<span class="params"><span class="number">0</span>, <span class="number">0</span></span>),</span></span><br><span class="line"><span class="params">    dilation=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    kernel_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data_layout=<span class="string">&quot;NCHW&quot;</span>,</span></span><br><span class="line"><span class="params">    kernel_layout=<span class="string">&quot;OIHW&quot;</span>,</span></span><br><span class="line"><span class="params">    out_layout=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params">    out_dtype=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(kernel_size, <span class="built_in">int</span>):</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(strides, <span class="built_in">int</span>):</span><br><span class="line">        strides = (strides, strides)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dilation, <span class="built_in">int</span>):</span><br><span class="line">        dilation = (dilation, dilation)</span><br><span class="line">    padding = get_pad_tuple2d(padding)</span><br><span class="line">    <span class="keyword">return</span> _make.conv2d( data, weight, strides, padding, dilation, groups, channels, kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>这里的<code>_make.conv2d</code>是通过下面的PackFunc注册得到的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tvm._ffi._init_api(<span class="string">&quot;relay.op.nn._make&quot;</span>, __name__)</span><br></pre></td></tr></table></figure><p>在<code>src/relay/op/nn/convolution.cc</code>找到conv2d的注册函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op.nn._make.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span><br><span class="line">                       Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span><br><span class="line">                       Array&lt;IndexExpr&gt; kernel_size, String data_layout, String kernel_layout,</span><br><span class="line">                       String out_layout, DataType out_dtype) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">MakeConv</span>&lt;Conv2DAttrs&gt;(data, weight, strides, padding, dilation, groups, channels,</span><br><span class="line">                                   kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">                                   <span class="string">&quot;nn.conv2d&quot;</span>);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>MakeConv 是对所有卷积的模板，根据参数实例化相应的函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> Expr <span class="title">MakeConv</span><span class="params">(Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; kernel_size, std::string data_layout,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string kernel_layout, std::string out_layout, DataType out_dtype,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string op_name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;T&gt;();</span><br><span class="line">  attrs-&gt;strides = std::<span class="built_in">move</span>(strides);</span><br><span class="line">  attrs-&gt;padding = std::<span class="built_in">move</span>(padding);</span><br><span class="line">  attrs-&gt;dilation = std::<span class="built_in">move</span>(dilation);</span><br><span class="line">  attrs-&gt;groups = groups;</span><br><span class="line">  attrs-&gt;channels = std::<span class="built_in">move</span>(channels);</span><br><span class="line">  attrs-&gt;kernel_size = std::<span class="built_in">move</span>(kernel_size);</span><br><span class="line">  attrs-&gt;data_layout = std::<span class="built_in">move</span>(data_layout);</span><br><span class="line">  attrs-&gt;kernel_layout = std::<span class="built_in">move</span>(kernel_layout);</span><br><span class="line">  attrs-&gt;out_layout = std::<span class="built_in">move</span>(out_layout);</span><br><span class="line">  attrs-&gt;out_dtype = std::<span class="built_in">move</span>(out_dtype);</span><br><span class="line">  <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(op_name);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data, weight&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里通过<code>Op::Get(op_name);</code> 获取对应relay算子，在<code>Op::Get</code>函数中发现是通过查表得到。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// find operator by name</span></span><br><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注册是通过C++的<code>RELAY_REGISTER_OP(&quot;nn.conv2d&quot;)</code>宏注册到<code>OpRegistry::Global()</code>中。宏展开为</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> __attribute__((unused))::tvm::OpRegEntry&amp; __make_Op230 =</span><br><span class="line">    ::tvm::OpRegEntry::<span class="built_in">RegisterOrGet</span>(<span class="string">&quot;nn.conv2d&quot;</span>).<span class="built_in">set_name</span>()</span><br></pre></td></tr></table></figure><p>注册过程：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;nn.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(<span class="string">R&quot;code(2D convolution layer (e.g. spatial convolution over images).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This layer creates a convolution kernel that is convolved</span></span><br><span class="line"><span class="string">with the layer input to produce a tensor of outputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- **data**: This depends on the `layout` parameter. Input is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, in_channels, height, width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string">- **weight**: (channels, in_channels, kernel_size[0], kernel_size[1])</span></span><br><span class="line"><span class="string">- **out**:  This depends on the `layout` parameter. Output is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">)code&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_attrs_type</span>&lt;Conv2DAttrs&gt;()</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;weight&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The weight tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Conv2D&quot;</span>, Conv2DRel&lt;Conv2DAttrs&gt;)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;FInferCorrectLayout&gt;(<span class="string">&quot;FInferCorrectLayout&quot;</span>, ConvInferCorrectLayout&lt;Conv2DAttrs&gt;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>返回的是<code>OpRegEntry</code>，后续的<code>set_name</code>等，则是通过<code>OpRegEntry</code>的get接口（返回的是OpNode），构造对应的Relay op</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从TVM的官方&lt;a href=&quot;https://www.cnblogs.com/wanger-sjtu/p/15046641.html&quot;&gt;Tutorial&lt;/a&gt;里面，介绍了如何新增自定义算子。(这是我翻译的)&lt;/p&gt;
&lt;p&gt;之前的文章讲到了&lt;a href=&quot;../tvm-</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>【tvm解析】3. Operator Strategy 机制</title>
    <link href="https://wanger-sjtu.github.io/tvm-op-strategy/"/>
    <id>https://wanger-sjtu.github.io/tvm-op-strategy/</id>
    <published>2023-08-09T07:50:30.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>Relay Operator Strategy是建立Relay IR与TOPI算子库的桥梁，通过Relay Operator Strategy，每个Relay IR至少与一个compute和一个schedule注册关联起来。至少一个原因在于，一个算子在不同后端设备上有不同的实现，而且一个算子可能有多种计算算法，适应不同场景。</p><p>在增加relay IR 的教程里面注册算子的compute、schedule中，就是通过<code>OpStrategy</code>关联算子的compute与schedule</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-Design"><a href="#Operator-Strategy-Design" class="headerlink" title="Operator Strategy Design"></a>Operator Strategy Design</h2><p><code>OpStrategy</code>的核心为<code>OpImplementation</code>，包含了一组compute及对应的schedule，不同实现的名字，选择优先级（参见下文的选择策略）。</p><p>OpStrategy中包含一系列的<code>OpSpecialization</code>，每个<code>OpSpecialization</code>包含一组<code>SpecializedCondition</code>（参考<code>include/tvm/te/schedule.h</code>）. 如果<code>SpecializedCondition</code>为空（null），表示是一个通用的实现，反之则是对于特定情形优化的。<code>SpecializedCondition</code>包含了这一算子的多个TE实现，以及实现被调用的条件。</p><p>最后一点，对给定的workload，一个strategy 函数或者<code>FTVMStrategy</code>,决定了使用哪个compute和schedule，因此这部分需要与relay算子对应起来。<br><code>FTVMStrategy </code>实现位置在<code>include/tvm/target/generic_func.h</code>,是一个通用函数，对于给定硬件平台可以重写。函数签名是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">OpStrategy</span>(<span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> Array&lt;Tensor&gt;&amp; inputs, <span class="type">const</span> Type&amp; out_type, <span class="type">const</span> Target&amp; target)</span><br></pre></td></tr></table></figure><p>对给定算子属性信息、输入、输出类型以及平台设备，这个函数返回相应的<code>OpStrategy</code>.</p><h2 id="手写一个-Strategy-函数"><a href="#手写一个-Strategy-函数" class="headerlink" title="手写一个 Strategy 函数"></a>手写一个 Strategy 函数</h2><p>tvm 推荐在python侧来写Strategy 函数，在python侧提供了OpStrategy类，其中包含一个add_implementation方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_object(<span class="params"><span class="string">&quot;relay.OpStrategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OpStrategy</span>(<span class="title class_ inherited__">Object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator strategy&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__init_handle_by_constructor__(_make.OpStrategy)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_implementation</span>(<span class="params">self, compute, schedule, name=<span class="string">&quot;default&quot;</span>, plevel=<span class="number">10</span></span>):</span><br><span class="line">        _OpStrategyAddImplementation(self, compute, schedule, name, plevel)</span><br></pre></td></tr></table></figure><p>后面以topk的算子为例，介绍了如何手写 Strategy 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;topk_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_topk(topi.topk),</span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.generic&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对GPU CUDA的</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@topk_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_my_new_op(topi.cuda.topk),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>为了满足Strategy 函数对于函数签名的要求（see <code>FTVMCompute</code> and <code>FTVMSchedule</code> in <code>include/tvm/relay/op_attr_types.h</code>），这里对topk的compute和schedule做了一层封装。由于算子属性不同，通常需要算子开发者自己写这部分的封装函数。</p><p>上面的例子比较简单，对于一个设备平台只有一个实现，但对一些其他的复杂算子来说，需要针对不同的算法来写相应的schedule，以卷积算子为例，可以直接写滑窗来计算，也可以使用winograd算法计算。这种情况下有多个implementation：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">strategy.add_implementation(</span><br><span class="line">    wrap_compute_conv2d(topi.cuda.conv2d_nchw),</span><br><span class="line">    wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw),</span><br><span class="line">    name=<span class="string">&quot;conv2d_nchw.cuda&quot;</span>,</span><br><span class="line">    plevel=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> winograd_condition:</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_conv2d(topi.cuda.conv2d_nchw_winograd),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw_winograd),</span><br><span class="line">        name=<span class="string">&quot;conv2d_nchw_winograd.cuda&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><p>可以看到这两个是优先级不同，在满足winograd算法的情况下，会优先选择winograd算法。这样也可以新增条件，新增implentation。<br>同样也可以对不同shape设置不同的优先级策略。下面的例子就是在<code>m &gt; 16</code>时，有额外的计算策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dense_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">  m = inputs[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">  strategy = _op.OpStrategy()</span><br><span class="line">  strategy.add_implementation(</span><br><span class="line">    wrap_compute_dense(dense_compute1),</span><br><span class="line">    wrap_topi_schedule(dense_schedule1),</span><br><span class="line">    name=<span class="string">&quot;dense_common&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tvm.te.SpecializedCondition(m &gt; <span class="number">16</span>):</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_dense(dense_compute2),</span><br><span class="line">        wrap_topi_schedule(dense_schedule2),</span><br><span class="line">        name=<span class="string">&quot;dense_for_large_m&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br><span class="line">  <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="将算子-Strategy-绑定到算子"><a href="#将算子-Strategy-绑定到算子" class="headerlink" title="将算子 Strategy 绑定到算子"></a>将算子 Strategy 绑定到算子</h2><p>定义了算子strategy函数以后，需要跟算子绑定在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_strategy(<span class="string">&quot;topk&quot;</span>, strategy.topk_strategy)</span><br></pre></td></tr></table></figure><p>然而，对于一个算子来说，写它的strategy函数是比较困难的，对简单算子来说，这里提供了两种方案。<br>第一个:算子是单射的、广播、reduce操作时候，可以通过 <code>register_injective_schedule</code>, <code>register_broadcast_schedule</code>、 <code>register_reduce_schedule</code>，这就避免自己手写schedule了。不过这种方式对于任意后端设备都是通用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_broadcast_schedule(<span class="string">&quot;add&quot;</span>)</span><br></pre></td></tr></table></figure><p>第二种：对于没有明确pattern的算子，可以用<code>register_schedule</code>实现对任意后端的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用兜底的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@generic_func</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    <span class="keyword">with</span> target:</span><br><span class="line">        <span class="keyword">return</span> topi.generic.schedule_pool(outs, attrs.layout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果特定target的，需要在对应的文件下增加</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@schedule_pool.register(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool_cpu</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">register_schedule(<span class="string">&quot;nn.max_pool2d&quot;</span>, strategy.schedule_pool)</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-选择"><a href="#Operator-Strategy-选择" class="headerlink" title="Operator Strategy 选择"></a>Operator Strategy 选择</h2><p>一个算子有多个Strategy的时候，选择策略是什么呢？</p><p>对于静态shape：首先会根据搜索时候的tune log选择最佳实现，如果tune log中没有或者已有auto TVM模板中有特定的实现，则会根据优先级选择对应的实现。如果多个实现具有相同优先级，选哪个就不确定了。</p><p>动态shape场景，则会选择高优先级的情况。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Relay Operator Strategy是建立Relay IR与TOPI算子库的桥梁，通过Relay Operator Strategy，每个Relay IR至少与一个compute和一个schedule注册关联起来。至少一个原因在于，一个算子在不同后端设备上有不同的</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>tvm-多线程代码生成和运行</title>
    <link href="https://wanger-sjtu.github.io/tvm-cpu-multi-thread/"/>
    <id>https://wanger-sjtu.github.io/tvm-cpu-multi-thread/</id>
    <published>2023-08-09T07:48:52.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<h3 id="调用链"><a href="#调用链" class="headerlink" title="调用链"></a>调用链</h3><p>tvm搜索算子在需要多线程运行的算子，是在codegen阶段时插入<code>TVMBackendParallelLaunch</code>的调用。<br><code>TVMBackendParallelLaunch</code> 是tvm的线程池并行化入口，具体如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief The callback function to execute a parallel lambda</span></span><br><span class="line"><span class="comment"> * \param task_id the task id of the function. //这里实际就是线程池线程编码，对应第几个线程</span></span><br><span class="line"><span class="comment"> * \param penv The parallel environment backs the execution. // num_task, sync</span></span><br><span class="line"><span class="comment"> * \param cdata The supporting closure data.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*FTVMParallelLambda)</span><span class="params">(<span class="type">int</span> task_id, TVMParallelGroupEnv* penv, <span class="type">void</span>* cdata)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Backend function for running parallel jobs.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \param flambda The parallel function to be launched.</span></span><br><span class="line"><span class="comment"> * \param cdata The closure data. // 可以认为时循环的变量 codegen时生成</span></span><br><span class="line"><span class="comment"> * \param num_task Number of tasks to launch, can be 0, means launch</span></span><br><span class="line"><span class="comment"> *           with all available threads. // codegen 时写入的是0，运行时根据配置写入</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \return 0 when no error is thrown, -1 when failure happens</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMBackendParallelLaunch</span><span class="params">(FTVMParallelLambda flambda, <span class="type">void</span>* cdata, <span class="type">int</span> num_task)</span></span>;</span><br></pre></td></tr></table></figure><p><code>flambda</code>的调用在单线程和多线程下略有区别。</p><p>单线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (num_workers == <span class="number">1</span>) &#123;</span><br><span class="line">    std::atomic&lt;<span class="type">int32_t</span>&gt; sync_counter&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    TVMParallelGroupEnv env;</span><br><span class="line">    env.num_task = <span class="number">1</span>;</span><br><span class="line">    env.sync_handle = &amp;sync_counter;</span><br><span class="line">    (*flambda)(<span class="number">0</span>, &amp;env, cdata);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>多线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// launcher-&gt;Init(flambda, cdata, num_task, need_sync != 0);</span></span><br><span class="line"><span class="keyword">this</span>-&gt;cdata = cdata;</span><br><span class="line"><span class="keyword">this</span>-&gt;flambda = flambda;</span><br><span class="line"><span class="keyword">this</span>-&gt;env.num_task = num_task;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (queue-&gt;<span class="built_in">Pop</span>(&amp;task, spin_count)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(task.launcher != <span class="literal">nullptr</span>);</span><br><span class="line">    TVMParallelGroupEnv* penv = &amp;(task.launcher-&gt;env);</span><br><span class="line">    <span class="type">void</span>* cdata = task.launcher-&gt;cdata;</span><br><span class="line">    <span class="keyword">if</span> ((*task.launcher-&gt;flambda)(task.task_id, penv, cdata) == <span class="number">0</span>) &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobFinish</span>();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobError</span>(task.task_id);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>可以看到 待并行函数中 <code>TVMParallelGroupEnv* penv</code> 包含了实际的运行时线程，运行时可以根据这个确定每个线程的工作区间和步长。<br><code>cdata</code>则是线程运行时需要变量信息，闭包变量。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>对要并行的函数，实际上是按照<code>lambda</code>表达式的方式生成的。<code>FTVMParallelLambda</code> 的输入参数前两个是运行时确定的，第三个是捕获的外部变量。</p><h2 id="codegen-过程"><a href="#codegen-过程" class="headerlink" title="codegen 过程"></a>codegen 过程</h2><p>下面验证一下上述的猜测。</p><p>codegen过程中，实际上是在遍历<code>tir Stmt</code>的AST，因为生成的循环都是基于For的，调用过程也比较简单了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span>  <span class="comment">// -&gt; </span></span></span><br><span class="line"><span class="function"><span class="title">CreateParallelLaunch</span><span class="params">(For(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span></span></span><br><span class="line"><span class="params"><span class="function">                        op-&gt;thread_binding, op-&gt;annotations),</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="number">0</span>, std::string(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.c_str())</span></span>;   <span class="comment">// -&gt;</span></span><br><span class="line">CodeGenCPU::<span class="built_in">VisitStmt_</span>(<span class="type">const</span> ForNode* op);</span><br></pre></td></tr></table></figure><p>当遍历到For节点时， 根据属性判断是否并行加速。这里只分析加速场景。此时<code>parallel_env_.penv == nullptr</code> 创建多线程调用函数，进入<code>CreateParallelLaunch</code>函数。<br>然后 再生成 For的遍历逻辑。<code>this-&gt;VisitStmt(body);</code> 这里的<code>body</code>其实还是<code>For</code> ，这时候就进入 </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br></pre></td></tr></table></figure><p>前文的猜测也在这里得到验证。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">  <span class="keyword">if</span> (op-&gt;kind == ForKind::kSerial || op-&gt;kind == ForKind::kUnrolled) &#123;</span><br><span class="line">    CodeGenLLVM::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (op-&gt;kind == ForKind::kParallel) &#123;</span><br><span class="line">    <span class="keyword">if</span> (parallel_env_.penv == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="built_in">CreateParallelLaunch</span>(<span class="built_in">For</span>(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span><br><span class="line">                               op-&gt;thread_binding, op-&gt;annotations),</span><br><span class="line">                           <span class="number">0</span>, std::<span class="built_in">string</span>(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.<span class="built_in">c_str</span>());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.task_id.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.num_task.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.penv != <span class="literal">nullptr</span>);</span><br><span class="line">      DataType t = op-&gt;extent.<span class="built_in">dtype</span>();</span><br><span class="line">      PrimExpr num_task = <span class="built_in">cast</span>(t, parallel_env_.num_task);</span><br><span class="line">      PrimExpr task_id = <span class="built_in">cast</span>(t, parallel_env_.task_id);</span><br><span class="line">      <span class="built_in">ICHECK</span>(!parallel_env_.in_parallel_loop)</span><br><span class="line">          &lt;&lt; <span class="string">&quot;Nested parallel loop is not supported by threadpool, try fuse them instead&quot;</span>;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">if</span> (parallel_env_.stride_pattern) &#123;</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(task_id), <span class="built_in">MakeValue</span>(op-&gt;extent), <span class="built_in">MakeValue</span>(num_task),</span><br><span class="line">                        op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        PrimExpr step = (op-&gt;extent + num_task - <span class="built_in">make_const</span>(t, <span class="number">1</span>)) / num_task;</span><br><span class="line">        PrimExpr begin = <span class="built_in">min</span>(task_id * step, op-&gt;extent);</span><br><span class="line">        PrimExpr end = <span class="built_in">min</span>((task_id + <span class="built_in">make_const</span>(t, <span class="number">1</span>)) * step, op-&gt;extent);</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(begin), <span class="built_in">MakeValue</span>(end),</span><br><span class="line">                        llvm::ConstantInt::<span class="built_in">getSigned</span>(<span class="built_in">GetLLVMType</span>(end), <span class="number">1</span>), op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">false</span>;</span><br><span class="line">      ++parallel_env_.parallel_loop_count;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;cannot handle for type &quot;</span> &lt;&lt; op-&gt;kind;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    const Stmt&amp; body  For 循环的statement</span></span><br><span class="line"><span class="comment">    int num_task, 这里设置的是0，根据运行时参数确定使用线程</span></span><br><span class="line"><span class="comment">    std::string name</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::CreateParallelLaunch</span><span class="params">(<span class="type">const</span> Stmt&amp; body, <span class="type">int</span> num_task, std::string name)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// closure data</span></span><br><span class="line">  llvm::Function* f =</span><br><span class="line">      llvm::Function::<span class="built_in">Create</span>(ftype_tvm_parallel_lambda_, llvm::Function::PrivateLinkage,</span><br><span class="line">                             <span class="string">&quot;__tvm_parallel_lambda&quot;</span>, module_.<span class="built_in">get</span>());</span><br><span class="line">  <span class="built_in">SetTargetAttributes</span>(f);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate and setup the closure, call the closure. //For 循环内部变量。这里需要声明一下</span></span><br><span class="line">  Array&lt;Var&gt; vfields = tir::<span class="built_in">UndefinedVars</span>(body, &#123;&#125;);</span><br><span class="line">  <span class="type">uint64_t</span> nbytes;</span><br><span class="line">  TypedPointer cdata = <span class="built_in">PackClosureData</span>(vfields, &amp;nbytes, <span class="string">&quot;closure_&quot;</span> + name); <span class="comment">// 可以认为时循环的变量</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> TVM_LLVM_VERSION &gt;= 90</span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = llvm::<span class="built_in">FunctionCallee</span>(ftype_tvm_parallel_launch_, <span class="built_in">RuntimeTVMParallelLaunch</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = <span class="built_in">RuntimeTVMParallelLaunch</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  llvm::BasicBlock* par_launch_end = <span class="built_in">CheckCallSuccess</span>(builder_-&gt;<span class="built_in">CreateCall</span>(</span><br><span class="line">      launch_callee,</span><br><span class="line">      &#123;f, builder_-&gt;<span class="built_in">CreatePointerCast</span>(cdata.addr, t_void_p_), <span class="built_in">ConstInt32</span>(num_task)&#125;));</span><br><span class="line">  <span class="comment">// Setup the closure function.</span></span><br><span class="line">  <span class="keyword">auto</span>* lambda_entry =</span><br><span class="line">      llvm::BasicBlock::<span class="built_in">Create</span>(*llvm_target_-&gt;<span class="built_in">GetContext</span>(), <span class="string">&quot;parallel_closure_entry&quot;</span>, f);</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(lambda_entry);</span><br><span class="line">  <span class="keyword">auto</span> it = f-&gt;<span class="built_in">arg_begin</span>();</span><br><span class="line">  llvm::Value* task_id = &amp;(*it++);</span><br><span class="line">  task_id-&gt;<span class="built_in">setName</span>(<span class="string">&quot;task_id&quot;</span>);</span><br><span class="line">  llvm::Value* penv = &amp;(*it++);</span><br><span class="line">  cdata.addr = builder_-&gt;<span class="built_in">CreatePointerCast</span>(&amp;(*it++), cdata.addr-&gt;<span class="built_in">getType</span>());</span><br><span class="line">  <span class="comment">// setup new variable map, swap it with current var context.</span></span><br><span class="line">  std::unordered_map&lt;<span class="type">const</span> VarNode*, llvm::Value*&gt; new_vmap;</span><br><span class="line">  <span class="built_in">UnpackClosureData</span>(cdata, vfields, &amp;new_vmap);</span><br><span class="line">  <span class="comment">// setup parallel env</span></span><br><span class="line">  ParallelEnv par_env;</span><br><span class="line">  par_env.task_id = <span class="built_in">Var</span>(<span class="string">&quot;task_id&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  par_env.num_task = <span class="built_in">Var</span>(<span class="string">&quot;num_task&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  new_vmap[par_env.task_id.<span class="built_in">get</span>()] = task_id;</span><br><span class="line">  new_vmap[par_env.num_task.<span class="built_in">get</span>()] = builder_-&gt;<span class="built_in">CreateLoad</span>(</span><br><span class="line">      t_int32_,</span><br><span class="line">      builder_-&gt;<span class="built_in">CreateInBoundsGEP</span>(t_tvm_parallel_group_env_, penv, &#123;<span class="built_in">ConstInt32</span>(<span class="number">0</span>), <span class="built_in">ConstInt32</span>(<span class="number">1</span>)&#125;),</span><br><span class="line">      <span class="string">&quot;num_task&quot;</span>);</span><br><span class="line">  par_env.penv = penv;</span><br><span class="line">  <span class="keyword">auto</span> new_analyzer = std::<span class="built_in">make_unique</span>&lt;arith::Analyzer&gt;();</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  <span class="keyword">this</span>-&gt;<span class="built_in">VisitStmt</span>(body);</span><br><span class="line">  builder_-&gt;<span class="built_in">CreateRet</span>(<span class="built_in">ConstInt32</span>(<span class="number">0</span>));</span><br><span class="line">  <span class="comment">// swap the var map back, now we are back on track.</span></span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  <span class="built_in">ICHECK_NE</span>(par_env.parallel_loop_count, <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Cannot find parallel loop within parallel launch&quot;</span>;</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(par_launch_end);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;调用链&quot;&gt;&lt;a href=&quot;#调用链&quot; class=&quot;headerlink&quot; title=&quot;调用链&quot;&gt;&lt;/a&gt;调用链&lt;/h3&gt;&lt;p&gt;tvm搜索算子在需要多线程运行的算子，是在codegen阶段时插入&lt;code&gt;TVMBackendParallelLaunch&lt;/c</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
    <category term="CPP" scheme="https://wanger-sjtu.github.io/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>C++&#39;s most vexing parse</title>
    <link href="https://wanger-sjtu.github.io/most-vexing-parse/"/>
    <id>https://wanger-sjtu.github.io/most-vexing-parse/</id>
    <published>2023-08-09T07:48:06.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>C++’s most vexing parse 是 Scott Meyers 在其名著《Effective STL》中创造的一个术语。Scott 用这个术语来形容 C++ 标准对于 declaration 语句的消歧义（ambiguity resolution）约定与常人的认知相悖。</p><p><strong>最令人烦恼的解析</strong> （<strong>most vexing parse</strong>）是C++中的一种反直觉的二义性解析形式。 在一些场景下，编译器无法区分某语句是初始化时某对象的参数，还是声明一个函数时指定参数类型。在这些情况下，编译器将该行解释为函数声明。</p><p>形如 <code>Type()</code> 或 <code>Type(name)</code> 的表达在某些情况下具有歧义（syntax ambiguity）。</p><h3 id="C风格强制类型转换"><a href="#C风格强制类型转换" class="headerlink" title="C风格强制类型转换"></a>C风格强制类型转换</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">double</span> my_dbl)</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span>(my_dbl))</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的第 2 行是有歧义的。一种可能的解释是声明一个变量<code>i</code>，初始值通过转换<code>my_dbl</code> 到一个<code>int</code>而来。但是，<code>C</code> 允许在函数参数声明周围使用多余的括号；因此，声明的i实际上等同于以下代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A function named i takes an integer and returns an integer.</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span> my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><h3 id="未命名的临时对象"><a href="#未命名的临时对象" class="headerlink" title="未命名的临时对象"></a>未命名的临时对象</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Timer</span> &#123;&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">TimeKeeper</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">TimeKeeper</span><span class="params">(Timer t)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">get_time</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br><span class="line">  <span class="keyword">return</span> time_keeper.<span class="built_in">get_time</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br></pre></td></tr></table></figure><p>是有歧义的，它可以被解释为：</p><ol><li>一个变量：定义为类<code>TimeKeeper</code>的变量<code>time_keeper</code>，用类<code>Timer</code>的匿名实例初始化。</li><li>一个函数声明：声明了一个函数<code>time_keeper</code>，返回一个<code>TimeKeeper</code>，有一个（未命名的）参数。参数的类型是一个（指向）不接受输入并返回<code>Timer</code>对象的函数（的指针）。</li></ol><p>[C ++标准]采取第二种解释，这与上面的第9行不一致。例如，<code>Clang++</code>警告第9行存在最令人烦恼的解析，并报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">clang++ time_keeper.cc</span></span><br><span class="line">**timekeeper.cc:9:25: warning: parentheses were disambiguated as a function declaration**</span><br><span class="line">      **[-Wvexing-parse]**</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                        **^~~~~~~~~**</span><br><span class="line">**timekeeper.cc:9:26: note:** add a pair of parentheses to declare a variable</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                         ^</span><br><span class="line">                         (      )</span><br><span class="line">**timekeeper.cc:10:21: error: member reference base type &#x27;TimeKeeper (Timer (*)())&#x27; is not a**</span><br><span class="line">      **structure or union**</span><br><span class="line">  return time_keeper.get_time();</span><br><span class="line">         **~~~~~~~~~~~^~~~~~~~~**</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>这些有歧义的声明往往不会被解析为程序员所期望的语句。C++ 中的函数类型通常隐藏在<code>typedef</code>之后，并且通常具有显式引用或指针限定符。要强制扭转解析的结果，<strong>常见做法是换一种不同的对象创建或转换语法</strong>。</p><p>在类型转换的示例中，有两种替代语法：“C 风格强制类型转换”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// declares a variable of type int</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">((<span class="type">int</span>)my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><p>或一个static_cast转换：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="keyword">static_cast</span>&lt;<span class="type">int</span>&gt;(my_dbl))</span></span>;</span><br></pre></td></tr></table></figure><p>在变量声明的示例中，首选方法（自 C++11 起）是统一（大括号）初始化。 这也允许完全省略类型名称：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Any of the following work:</span></span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer&#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;<span class="built_in">Timer</span>()&#125;;</span><br><span class="line">TimeKeeper time_keeper&#123;Timer&#123;&#125;&#125;;</span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(     &#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;     &#123;&#125;&#125;;</span><br></pre></td></tr></table></figure><p>在 C++11 之前，强制获得预期解释的常用手段是使用额外的括号或拷贝初始化：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">( <span class="comment">/*Avoid MVP*/</span> (Timer()))</span></span>; <span class="comment">// 增加一个括号</span></span><br><span class="line">TimeKeeper time_keeper = <span class="built_in">TimeKeeper</span>(<span class="built_in">Timer</span>());  <span class="comment">// c++ 17 拷贝运算可以被优化</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;C++’s most vexing parse 是 Scott Meyers 在其名著《Effective STL》中创造的一个术语。Scott 用这个术语来形容 C++ 标准对于 declaration 语句的消歧义（ambiguity resolution）约定与常人的</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="CPP" scheme="https://wanger-sjtu.github.io/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>【TVM模型编译】1. onnx2relay</title>
    <link href="https://wanger-sjtu.github.io/tvm-onnx-to-relay/"/>
    <id>https://wanger-sjtu.github.io/tvm-onnx-to-relay/</id>
    <published>2023-08-08T07:53:17.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p><a href="../tvm-onnx">上一篇</a>介绍了onnx模型在tvm中优化的总体流程。</p><p>在这一篇中，介绍onnx模型到relay模型的转换流程，主要涉及了以下几个方面：</p><ul><li>onnx算子到relay算子转换</li><li>relay算子实现</li></ul><p>这一篇介绍onnx算子到relay算子转换过程</p><h2 id="onnx算子到relay算子转换"><a href="#onnx算子到relay算子转换" class="headerlink" title="onnx算子到relay算子转换"></a>onnx算子到relay算子转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br></pre></td></tr></table></figure><p>这部分实现是在<code>python/tvm/relay/frontend/onnx.py</code>中。实现转换过程的核心在于<code>GraphProto</code>这个类。这个类中实现了读取onnx模型各个节点、输入输出，映射onnx算子到relay IR的过程。对外接口为<code>from_onnx</code>这个函数。其伪代码可以大致表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">from_onnx</span>(<span class="params">self, graph, opset, get_output_expr=<span class="literal">False</span></span>):</span><br><span class="line">    inputs, params = read_model_inputs(graph) <span class="comment"># 模型参数</span></span><br><span class="line">    nodes = read_model_node(graph) <span class="comment"># 模型节点、算子信息</span></span><br><span class="line">    convert_map = _get_convert_map(opset) <span class="comment"># 模型转换map</span></span><br><span class="line">    check_op_support(nodes, convert_map)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">        op = self._convert_operator(op_name, inputs, attr, opset)</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>从这里可以知道ONNX前端的每个算子转化与<code>_get_convert_map</code>有关。<br><code>_convert_operator</code>完成了算子转换过程。具体的<code>convert_map</code>包含了所有支持算子的转换函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_convert_operator</span>(<span class="params">self, op_name, inputs, attrs, opset</span>):</span><br><span class="line">    convert_map = _get_convert_map(opset)</span><br><span class="line">    <span class="keyword">if</span> op_name <span class="keyword">in</span> _identity_list: <span class="comment"># 对onnx这里是空的</span></span><br><span class="line">        sym = get_relay_op(op_name)(*inputs, **attrs)</span><br><span class="line">    <span class="keyword">elif</span> op_name <span class="keyword">in</span> convert_map:</span><br><span class="line">        sym = convert_map[op_name](inputs, attrs, self._params)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Operator &#123;&#125; not implemented.&quot;</span>.<span class="built_in">format</span>(op_name))</span><br><span class="line">    <span class="keyword">return</span> sym</span><br></pre></td></tr></table></figure><p>以卷积算子为例，介绍具体的转换过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Conv&quot;</span>: Conv.get_converter(opset)</span><br></pre></td></tr></table></figure><p>Conv算子的实际转换操作来自于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(<span class="title class_ inherited__">OnnxOpConverter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator converter for Conv.&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_impl_v1</span>(<span class="params">cls, inputs, attr, params</span>):</span><br><span class="line">        <span class="comment"># Use shape of input to determine convolution type.</span></span><br><span class="line">        data = inputs[<span class="number">0</span>]</span><br><span class="line">        input_shape = infer_shape(data)</span><br><span class="line">        ndim = <span class="built_in">len</span>(input_shape)</span><br><span class="line">        <span class="comment"># auto_pad ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># construct op from attrs</span></span><br><span class="line">        out = AttrCvt(</span><br><span class="line">            op_name=dimension_picker(<span class="string">&quot;conv&quot;</span>),</span><br><span class="line">            transforms=&#123;</span><br><span class="line">                <span class="string">&quot;kernel_shape&quot;</span>: <span class="string">&quot;kernel_size&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dilations&quot;</span>: (<span class="string">&quot;dilation&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="string">&quot;pads&quot;</span>: (<span class="string">&quot;padding&quot;</span>, <span class="number">0</span>),</span><br><span class="line">                <span class="string">&quot;group&quot;</span>: (<span class="string">&quot;groups&quot;</span>, <span class="number">1</span>),</span><br><span class="line">            &#125;,</span><br><span class="line">            custom_check=dimension_constraint(),</span><br><span class="line">        )([data, inputs[<span class="number">1</span>]], attr, params)</span><br><span class="line"></span><br><span class="line">        use_bias = <span class="built_in">len</span>(inputs) == <span class="number">3</span></span><br><span class="line">        <span class="keyword">if</span> use_bias:</span><br><span class="line">            out = _op.nn.bias_add(out, inputs[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>这里通过<code>AttrCvt</code>类中构建相应的<code>relay</code>算子,<code>python/tvm/relay/frontend/common.py</code></p><p><code>AttrCvt</code>类包括两部分，<code>__init__</code> 和 <code>__call__</code>，前者根据收集初始化参数，后者完成Relay IR算子构建。</p><p><code>__call__</code>中的实现主要完成了算子属性读取、转换。根据转换后输入构建Relay IR</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> get_relay_op(op_name)(*inputs, **new_attrs)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;../tvm-onnx&quot;&gt;上一篇&lt;/a&gt;介绍了onnx模型在tvm中优化的总体流程。&lt;/p&gt;
&lt;p&gt;在这一篇中，介绍onnx模型到relay模型的转换流程，主要涉及了以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;onnx算子到relay算子转换&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>【TVM模型编译】0.onnx模型优化流程.md</title>
    <link href="https://wanger-sjtu.github.io/tvm-onnx/"/>
    <id>https://wanger-sjtu.github.io/tvm-onnx/</id>
    <published>2023-08-07T07:52:43.000Z</published>
    <updated>2023-12-13T14:22:23.010Z</updated>
    
    <content type="html"><![CDATA[<p>本文以及后续文章，着重于介绍tvm的完整编译流程。<br>后续文章将会按照以上流程，介绍tvm源码。其中涉及一些编程技巧、以及tvm概念，不在此部分进行进一步讲解，另有文章进行介绍。</p><p>首先介绍一下，从onnx模型转为tvm模型的基本步骤。大致可以分为以下几步：</p><ol><li>onnx模型转到relay IR</li><li>基于Relay IR优化</li><li>导出优化模型</li><li>加载运行模型</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">onnx_model = onnx.load(model_path)</span><br><span class="line">target = <span class="string">&quot;llvm&quot;</span></span><br><span class="line">input_name = <span class="string">&quot;1&quot;</span></span><br><span class="line">shape_dict = &#123;input_name: x.shape&#125;</span><br><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br><span class="line"><span class="comment"># model build</span></span><br><span class="line"><span class="keyword">with</span> tvm.transform.PassContext(opt_level=<span class="number">3</span>):</span><br><span class="line">    lib = relay.build(mod, target=target, params=params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the library at local temporary directory.</span></span><br><span class="line">fcompile = ndk.create_shared <span class="keyword">if</span> <span class="keyword">not</span> local_demo <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">lib.export_library(<span class="string">&quot;net.so&quot;</span>, fcompile)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cpp load compiled so</span></span><br><span class="line">tvm::runtime::Module mod_factory = tvm::runtime::Module::<span class="built_in">LoadFromFile</span>(<span class="string">&quot;lib/net.so&quot;</span>);</span><br><span class="line">  <span class="comment">// create the graph executor module</span></span><br><span class="line">tvm::runtime::Module gmod = mod_factory.<span class="built_in">GetFunction</span>(<span class="string">&quot;default&quot;</span>)(dev);</span><br><span class="line">tvm::runtime::PackedFunc set_input = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;set_input&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc get_output = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;get_output&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc run = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;run&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the C++ API</span></span><br><span class="line">tvm::runtime::NDArray x = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line">tvm::runtime::NDArray y = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">    <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(x-&gt;data)[i * <span class="number">2</span> + j] = i * <span class="number">2</span> + j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// set the right input</span></span><br><span class="line"><span class="built_in">set_input</span>(<span class="string">&quot;1&quot;</span>, x);</span><br><span class="line"><span class="comment">// run the code</span></span><br><span class="line"><span class="built_in">run</span>();</span><br><span class="line"><span class="comment">// get the output</span></span><br><span class="line"><span class="built_in">get_output</span>(<span class="number">0</span>, y);</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文以及后续文章，着重于介绍tvm的完整编译流程。&lt;br&gt;后续文章将会按照以上流程，介绍tvm源码。其中涉及一些编程技巧、以及tvm概念，不在此部分进行进一步讲解，另有文章进行介绍。&lt;/p&gt;
&lt;p&gt;首先介绍一下，从onnx模型转为tvm模型的基本步骤。大致可以分为以下几步：</summary>
      
    
    
    
    <category term="技术" scheme="https://wanger-sjtu.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="TVM" scheme="https://wanger-sjtu.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>利用 Github Actions 自动部署 Hexo 博客</title>
    <link href="https://wanger-sjtu.github.io/auto-deploy/"/>
    <id>https://wanger-sjtu.github.io/auto-deploy/</id>
    <published>2023-08-03T12:55:35.000Z</published>
    <updated>2023-12-13T14:22:23.002Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Github Actions 可以很方便实现 CI&#x2F;CD 工作流，类似 Travis 的用法，来帮我们完成一些工作，比如实现自动化测试、打包、部署等操作。当我们运行 Jobs 时，它会创建一个容器 (runner)，容器支持：Ubuntu、Windows 和 MacOS 等系统，在容器中我们可以安装软件，利用安装的软件帮我们处理一些数据，然后把处理好的数据推送到某个地方。</p><p>本文将介绍利用 Github Actions 实现自动部署 hexo 到 Github Pages，在之前我们需要写完文章执行 <code>hexo generate --deploy</code> 来部署，当你文章比较多的时候，可能还需要等待很久，而且还可能会遇到本地安装的 Node.js 版本与 Hexo 不兼容的问题，目前我就是因为电脑的 Node.js 版本升到 v14 版本导致与 Hexo 不兼容部署不了，才来捣腾 Github Actions 功能的。利用 Github Actions 你将会没有这些烦恼。</p><h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><h2 id="创建所需仓库"><a href="#创建所需仓库" class="headerlink" title="创建所需仓库"></a>创建所需仓库</h2><ol><li>创建 your.github.io 仓库用来存放博客和静态博客页面，这两个在不同分支。</li></ol><h2 id="生成部署密钥"><a href="#生成部署密钥" class="headerlink" title="生成部署密钥"></a>生成部署密钥</h2><p>一路按回车直到生成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-keygen -t rsa -b 4096 -C <span class="string">&quot;<span class="subst">$(git config user.email)</span>&quot;</span> -f gh-pages -N <span class="string">&quot;&quot;</span></span></span><br></pre></td></tr></table></figure><p>当前目录下会有 gh-pages 和 gh-pages.pub 两个文件。</p><h2 id="配置部署密钥"><a href="#配置部署密钥" class="headerlink" title="配置部署密钥"></a>配置部署密钥</h2><p>复制  <code>gh-pages</code> 文件内容，在仓库 <code>Settings -&gt; secrets and variables -&gt; new repository secret</code> 页面上添加。</p><ol><li>在 <code>Name</code> 输入框填写 <code>ACTIONS_DEPLOY_KEY</code>。</li><li>在 <code>secret</code>输入框填写  <code>gh-pages</code> 文件内容。</li></ol><img src="/auto-deploy/1693718105040.png" class="" width="1693718105040"><p>复制  <code>gh-pages.pub</code> 文件内容，在 仓库 Settings -&gt; Deploy keys -&gt; Add deploy key 页面上添加。</p><ol><li>在 Title 输入框填写 HEXO_DEPLOY_PUB。</li><li>在 Key 输入框填写  gh-pages.pub 文件内容。</li><li>勾选 Allow write access 选项。</li></ol><img src="/auto-deploy/1693718293575.png" class="" width="1693718293575"><h1 id="编写-Github-Actions"><a href="#编写-Github-Actions" class="headerlink" title="编写 Github Actions"></a>编写 Github Actions</h1><h2 id="Workflow-模版"><a href="#Workflow-模版" class="headerlink" title="Workflow 模版"></a>Workflow 模版</h2><p>在 blog 仓库根目录下创建 .github&#x2F;workflows&#x2F;deploy.yml 文件，目录结构如下。</p><figure class="highlight plaintext"><figcaption><span>(repository)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">└── .github</span><br><span class="line">    └── workflows</span><br><span class="line">        └── deploy.yml</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在 deploy.yml 文件中粘贴以下内容。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Pages</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 触发器、分支</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">gh-pages</span>  <span class="comment"># default branch</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="comment"># 子任务</span></span><br><span class="line">  <span class="attr">pages:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> <span class="comment"># 定运行所需要的虚拟机环境</span></span><br><span class="line">    <span class="attr">permissions:</span></span><br><span class="line">      <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">submodules:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">fetch-depth:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment"># 每个name表示一个步骤:step</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Use</span> <span class="string">Node.js</span> <span class="number">18.</span><span class="string">x</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/setup-node@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">node-version:</span> <span class="string">&#x27;18.17.1&#x27;</span> <span class="comment"># 自己正在使用的node版本即可</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Global</span> <span class="string">Config</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="attr">ACTIONS_DEPLOY_KEY:</span> <span class="string">$&#123;&#123;secrets.ACTIONS_DEPLOY_KEY&#125;&#125;</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          sudo timedatectl set-timezone &quot;Asia/Shanghai&quot;</span></span><br><span class="line"><span class="string">          mkdir -p ~/.ssh/</span></span><br><span class="line"><span class="string">          echo &quot;$ACTIONS_DEPLOY_KEY&quot; &gt; ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          chmod 600 ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          git config --global user.email &quot;xx&quot;</span></span><br><span class="line"><span class="string">          git config --global user.name &quot;XXX&quot;</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># - run: node -v # 查看node版本号</span></span><br><span class="line">      <span class="comment"># 缓存依赖项: https://docs.github.com/cn/actions/using-workflows/caching-dependencies-to-speed-up-workflows</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Cache</span> <span class="string">NPM</span> <span class="string">dependencies</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/cache@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="comment"># npm cache files are stored in `~/.npm` on Linux/macOS</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">~/.npm</span></span><br><span class="line">          <span class="comment"># path: node_modules</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.OS</span> <span class="string">&#125;&#125;-npm-cache</span></span><br><span class="line">          <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.OS &#125;&#125;-npm-cache</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># 查看路径 : /home/runner/work/blog/blog</span></span><br><span class="line">      <span class="comment"># - name: Look Path</span></span><br><span class="line">      <span class="comment">#   run: pwd</span></span><br><span class="line">      <span class="comment"># 查看文件</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="comment"># 第一次或者依赖发生变化的时候执行 Install Dependencies，其它构建的时候不需要这一步</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Install</span> <span class="string">Dependencies</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Clean</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">clean</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">deploy</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Get</span> <span class="string">the</span> <span class="string">output</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          echo &quot;$&#123;&#123; steps.deploy.outputs.notify &#125;&#125;&quot;</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><h1 id="hexo配置文件"><a href="#hexo配置文件" class="headerlink" title="hexo配置文件"></a>hexo配置文件</h1><p>blog 根目录下，名为 _config.yml，配置一下deploy的分支信息。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span> </span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span> </span><br><span class="line">  <span class="attr">repository:</span> <span class="string">xxx</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">deploy</span></span><br></pre></td></tr></table></figure><h1 id="执行任务"><a href="#执行任务" class="headerlink" title="执行任务"></a>执行任务</h1><p>写一篇文章，push 到 仓库的 master 分支，在仓库 Actions 页面查看当前 task。</p><p>当任务完成后查看您的博客 <a href="https://your.github.io/">https://your.github.io</a>，如果不出意外的话已经可以看到新添加的文章了。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>偷懒是人类发展的动力，人都有偷懒的想法，目的就是为了让自己能够活得更好，经过几千年的不断发展，现在人偷懒的方式无疑更加的先进。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;Github Actions 可以很方便实现 CI&amp;#x2F;CD 工作流，类似 Travis 的用法，来帮我们完成一些工作，比如实现自动化</summary>
      
    
    
    
    
    <category term="CI" scheme="https://wanger-sjtu.github.io/tags/CI/"/>
    
  </entry>
  
</feed>
